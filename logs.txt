* 
* ==> Audit <==
* |---------|-----------------------|----------|------|---------|---------------------|---------------------|
| Command |         Args          | Profile  | User | Version |     Start Time      |      End Time       |
|---------|-----------------------|----------|------|---------|---------------------|---------------------|
| start   |                       | minikube | lee  | v1.32.0 | 29 Mar 24 14:17 KST | 29 Mar 24 14:23 KST |
| service | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:23 KST |                     |
|---------|-----------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/03/29 14:17:22
Running on machine: DESKTOP-1T299N9
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0329 14:17:22.797845  180155 out.go:296] Setting OutFile to fd 1 ...
I0329 14:17:22.799400  180155 out.go:348] isatty.IsTerminal(1) = true
I0329 14:17:22.799411  180155 out.go:309] Setting ErrFile to fd 2...
I0329 14:17:22.799424  180155 out.go:348] isatty.IsTerminal(2) = true
I0329 14:17:22.801402  180155 root.go:338] Updating PATH: /home/lee/.minikube/bin
W0329 14:17:22.801885  180155 root.go:314] Error reading config file at /home/lee/.minikube/config/config.json: open /home/lee/.minikube/config/config.json: no such file or directory
I0329 14:17:22.812456  180155 out.go:303] Setting JSON to false
I0329 14:17:22.823148  180155 start.go:128] hostinfo: {"hostname":"DESKTOP-1T299N9","uptime":49310,"bootTime":1711640133,"procs":41,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"089e2fbe-8854-471b-a5fd-c5dea93fbac9"}
I0329 14:17:22.823262  180155 start.go:138] virtualization:  guest
I0329 14:17:22.828697  180155 out.go:177] 😄  minikube v1.32.0 on Ubuntu 22.04 (amd64)
W0329 14:17:22.835693  180155 preload.go:295] Failed to list preload files: open /home/lee/.minikube/cache/preloaded-tarball: no such file or directory
I0329 14:17:22.835820  180155 notify.go:220] Checking for updates...
I0329 14:17:22.836123  180155 driver.go:378] Setting default libvirt URI to qemu:///system
I0329 14:17:22.836177  180155 global.go:111] Querying for installed drivers using PATH=/home/lee/.minikube/bin:/home/lee/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Users/LJM/AppData/Local/anaconda3/condabin:/mnt/c/Windows/system32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files/Git/cmd:/mnt/c/Users/LJM/AppData/Local/anaconda3/Scripts:/mnt/c/Users/LJM/AppData/Local/anaconda3:/mnt/c/Users/LJM/AppData/Local/anaconda3/Scripts:/mnt/c/Users/LJM/AppData/Local/anaconda3/Library:/mnt/c/Users/LJM/AppData/Local/anaconda3/Library/bin:/mnt/c/Users/LJM/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/LJM/AppData/Local/Programs/Microsoft VS Code/bin:/snap/bin
I0329 14:17:24.179572  180155 docker.go:122] docker version: linux-25.0.4:Docker Engine - Community
I0329 14:17:24.180161  180155 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0329 14:17:29.950261  180155 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (5.76989494s)
I0329 14:17:29.952012  180155 info.go:266] docker info: {ID:e3db2e71-b1ea-4230-a673-d2dbcdaa6727 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:true NGoroutines:44 SystemTime:2024-03-29 14:17:28.667087357 +0900 KST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4035866624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-1T299N9 Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0329 14:17:29.953048  180155 docker.go:295] overlay module found
I0329 14:17:29.953081  180155 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0329 14:17:29.982627  180155 global.go:122] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0329 14:17:30.051079  180155 global.go:122] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0329 14:17:30.051117  180155 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0329 14:17:30.101386  180155 global.go:122] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I0329 14:17:30.156399  180155 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0329 14:17:30.268691  180155 global.go:122] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0329 14:17:30.331953  180155 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0329 14:17:30.332031  180155 driver.go:313] not recommending "none" due to default: false
I0329 14:17:30.332049  180155 driver.go:313] not recommending "ssh" due to default: false
I0329 14:17:30.332095  180155 driver.go:348] Picked: docker
I0329 14:17:30.332115  180155 driver.go:349] Alternatives: [none ssh]
I0329 14:17:30.332129  180155 driver.go:350] Rejects: [podman kvm2 qemu2 virtualbox vmware]
I0329 14:17:30.337119  180155 out.go:177] ✨  Automatically selected the docker driver. Other choices: none, ssh
I0329 14:17:30.342232  180155 start.go:298] selected driver: docker
I0329 14:17:30.342248  180155 start.go:902] validating driver "docker" against <nil>
I0329 14:17:30.342273  180155 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0329 14:17:30.342482  180155 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0329 14:17:30.568729  180155 info.go:266] docker info: {ID:e3db2e71-b1ea-4230-a673-d2dbcdaa6727 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:23 OomKillDisable:true NGoroutines:44 SystemTime:2024-03-29 14:17:30.533222413 +0900 KST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4035866624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-1T299N9 Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0329 14:17:30.569456  180155 start_flags.go:309] no existing cluster config was found, will generate one from the flags 
I0329 14:17:30.571143  180155 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=3848MB, container=3848MB
I0329 14:17:30.572223  180155 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I0329 14:17:30.575126  180155 out.go:177] 📌  Using Docker driver with root privileges
I0329 14:17:30.578315  180155 cni.go:84] Creating CNI manager for ""
I0329 14:17:30.578373  180155 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 14:17:30.578406  180155 start_flags.go:318] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0329 14:17:30.578454  180155 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0329 14:17:30.584352  180155 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0329 14:17:30.587470  180155 cache.go:121] Beginning downloading kic base image for docker with docker
I0329 14:17:30.590238  180155 out.go:177] 🚜  Pulling base image ...
I0329 14:17:30.593047  180155 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0329 14:17:30.593247  180155 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0329 14:17:30.652077  180155 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I0329 14:17:30.652532  180155 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I0329 14:17:30.653794  180155 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I0329 14:17:30.755720  180155 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0329 14:17:30.755901  180155 cache.go:56] Caching tarball of preloaded images
I0329 14:17:30.757178  180155 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0329 14:17:30.768302  180155 out.go:177] 💾  Downloading Kubernetes v1.28.3 preload ...
I0329 14:17:30.791710  180155 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I0329 14:17:31.095920  180155 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4?checksum=md5:82104bbf889ff8b69d5c141ce86c05ac -> /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0329 14:18:44.614306  180155 preload.go:249] saving checksum for preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I0329 14:18:44.615031  180155 preload.go:256] verifying checksum of /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I0329 14:18:50.257196  180155 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I0329 14:18:50.257228  180155 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 from local cache
I0329 14:18:57.270140  180155 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0329 14:18:57.271039  180155 profile.go:148] Saving config to /home/lee/.minikube/profiles/minikube/config.json ...
I0329 14:18:57.271079  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/config.json: {Name:mk685ebbdc1ce5db2c36dd699f74249c2d262227 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:20:38.901270  180155 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 from cached tarball
I0329 14:20:38.904847  180155 cache.go:194] Successfully downloaded all kic artifacts
I0329 14:20:38.922618  180155 start.go:365] acquiring machines lock for minikube: {Name:mk89aad7abaa74b5ceac4179cf674336d941d2b1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0329 14:20:38.924650  180155 start.go:369] acquired machines lock for "minikube" in 573.8µs
I0329 14:20:38.928045  180155 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0329 14:20:38.928631  180155 start.go:125] createHost starting for "" (driver="docker")
I0329 14:20:38.972669  180155 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0329 14:20:39.013529  180155 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0329 14:20:39.013724  180155 client.go:168] LocalClient.Create starting
I0329 14:20:39.019740  180155 main.go:141] libmachine: Creating CA: /home/lee/.minikube/certs/ca.pem
I0329 14:20:39.193313  180155 main.go:141] libmachine: Creating client certificate: /home/lee/.minikube/certs/cert.pem
I0329 14:20:39.548876  180155 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0329 14:20:39.599155  180155 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0329 14:20:39.599282  180155 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0329 14:20:39.599316  180155 cli_runner.go:164] Run: docker network inspect minikube
W0329 14:20:39.639109  180155 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0329 14:20:39.639135  180155 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0329 14:20:39.639184  180155 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0329 14:20:39.639411  180155 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0329 14:20:39.689012  180155 network.go:209] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0020434a0}
I0329 14:20:39.690186  180155 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0329 14:20:39.691440  180155 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0329 14:20:40.037234  180155 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0329 14:20:40.042262  180155 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0329 14:20:40.042500  180155 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0329 14:20:40.100797  180155 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0329 14:20:40.162355  180155 oci.go:103] Successfully created a docker volume minikube
I0329 14:20:40.162513  180155 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib
I0329 14:20:43.884295  180155 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib: (3.721712891s)
I0329 14:20:43.884322  180155 oci.go:107] Successfully prepared a docker volume minikube
I0329 14:20:43.884407  180155 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0329 14:20:43.884456  180155 kic.go:194] Starting extracting preloaded images to volume ...
I0329 14:20:43.884559  180155 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir
I0329 14:21:05.847622  180155 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir: (21.962833675s)
I0329 14:21:05.847703  180155 kic.go:203] duration metric: took 21.963235 seconds to extract preloaded images to volume
W0329 14:21:05.852699  180155 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0329 14:21:05.852997  180155 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0329 14:21:07.658255  180155 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.805120932s)
I0329 14:21:07.658550  180155 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0
I0329 14:21:08.967361  180155 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0: (1.308647632s)
I0329 14:21:08.970011  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0329 14:21:09.033895  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 14:21:09.076966  180155 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0329 14:21:09.206059  180155 oci.go:144] the created container "minikube" has a running status.
I0329 14:21:09.206149  180155 kic.go:225] Creating ssh key for kic: /home/lee/.minikube/machines/minikube/id_rsa...
I0329 14:21:09.333345  180155 kic_runner.go:191] docker (temp): /home/lee/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0329 14:21:09.400466  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 14:21:09.439356  180155 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0329 14:21:09.439374  180155 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0329 14:21:09.555868  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 14:21:09.602723  180155 machine.go:88] provisioning docker machine ...
I0329 14:21:09.602801  180155 ubuntu.go:169] provisioning hostname "minikube"
I0329 14:21:09.602896  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:09.643074  180155 main.go:141] libmachine: Using SSH client type: native
I0329 14:21:09.644054  180155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 14:21:09.644085  180155 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0329 14:21:09.647525  180155 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:51998->127.0.0.1:32772: read: connection reset by peer
I0329 14:21:13.002714  180155 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0329 14:21:13.002889  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:13.056425  180155 main.go:141] libmachine: Using SSH client type: native
I0329 14:21:13.057132  180155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 14:21:13.057165  180155 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0329 14:21:13.242730  180155 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0329 14:21:13.242791  180155 ubuntu.go:175] set auth options {CertDir:/home/lee/.minikube CaCertPath:/home/lee/.minikube/certs/ca.pem CaPrivateKeyPath:/home/lee/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/lee/.minikube/machines/server.pem ServerKeyPath:/home/lee/.minikube/machines/server-key.pem ClientKeyPath:/home/lee/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/lee/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/lee/.minikube}
I0329 14:21:13.242864  180155 ubuntu.go:177] setting up certificates
I0329 14:21:13.242885  180155 provision.go:83] configureAuth start
I0329 14:21:13.243980  180155 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 14:21:13.337195  180155 provision.go:138] copyHostCerts
I0329 14:21:13.337380  180155 exec_runner.go:151] cp: /home/lee/.minikube/certs/cert.pem --> /home/lee/.minikube/cert.pem (1111 bytes)
I0329 14:21:13.338800  180155 exec_runner.go:151] cp: /home/lee/.minikube/certs/key.pem --> /home/lee/.minikube/key.pem (1679 bytes)
I0329 14:21:13.339865  180155 exec_runner.go:151] cp: /home/lee/.minikube/certs/ca.pem --> /home/lee/.minikube/ca.pem (1070 bytes)
I0329 14:21:13.340656  180155 provision.go:112] generating server cert: /home/lee/.minikube/machines/server.pem ca-key=/home/lee/.minikube/certs/ca.pem private-key=/home/lee/.minikube/certs/ca-key.pem org=lee.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0329 14:21:13.557025  180155 provision.go:172] copyRemoteCerts
I0329 14:21:13.557136  180155 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0329 14:21:13.557185  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:13.582490  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:21:13.743618  180155 ssh_runner.go:362] scp /home/lee/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0329 14:21:13.892087  180155 ssh_runner.go:362] scp /home/lee/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0329 14:21:14.043340  180155 ssh_runner.go:362] scp /home/lee/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0329 14:21:14.171896  180155 provision.go:86] duration metric: configureAuth took 928.821041ms
I0329 14:21:14.171954  180155 ubuntu.go:193] setting minikube options for container-runtime
I0329 14:21:14.174173  180155 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0329 14:21:14.174351  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:14.264405  180155 main.go:141] libmachine: Using SSH client type: native
I0329 14:21:14.266634  180155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 14:21:14.266697  180155 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0329 14:21:14.515162  180155 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0329 14:21:14.515198  180155 ubuntu.go:71] root file system type: overlay
I0329 14:21:14.525404  180155 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0329 14:21:14.525636  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:14.617202  180155 main.go:141] libmachine: Using SSH client type: native
I0329 14:21:14.619397  180155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 14:21:14.619992  180155 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0329 14:21:15.006741  180155 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0329 14:21:15.007066  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:15.099244  180155 main.go:141] libmachine: Using SSH client type: native
I0329 14:21:15.101534  180155 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 14:21:15.101621  180155 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0329 14:21:20.179778  180155 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:22.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-03-29 05:21:14.981029531 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0329 14:21:20.179836  180155 machine.go:91] provisioned docker machine in 10.577092443s
I0329 14:21:20.179860  180155 client.go:171] LocalClient.Create took 41.166116221s
I0329 14:21:20.180000  180155 start.go:167] duration metric: libmachine.API.Create for "minikube" took 41.166474321s
I0329 14:21:20.180041  180155 start.go:300] post-start starting for "minikube" (driver="docker")
I0329 14:21:20.180089  180155 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0329 14:21:20.180356  180155 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0329 14:21:20.181494  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:20.270477  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:21:20.474315  180155 ssh_runner.go:195] Run: cat /etc/os-release
I0329 14:21:20.493365  180155 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0329 14:21:20.493443  180155 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0329 14:21:20.493472  180155 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0329 14:21:20.493488  180155 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0329 14:21:20.493518  180155 filesync.go:126] Scanning /home/lee/.minikube/addons for local assets ...
I0329 14:21:20.494650  180155 filesync.go:126] Scanning /home/lee/.minikube/files for local assets ...
I0329 14:21:20.495635  180155 start.go:303] post-start completed in 315.567921ms
I0329 14:21:20.496753  180155 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 14:21:20.592035  180155 profile.go:148] Saving config to /home/lee/.minikube/profiles/minikube/config.json ...
I0329 14:21:20.593875  180155 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0329 14:21:20.593996  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:20.685016  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:21:20.889641  180155 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0329 14:21:20.919933  180155 start.go:128] duration metric: createHost completed in 41.991230779s
I0329 14:21:20.920012  180155 start.go:83] releasing machines lock for "minikube", held for 41.99532188s
I0329 14:21:20.920369  180155 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 14:21:21.036491  180155 ssh_runner.go:195] Run: cat /version.json
I0329 14:21:21.036753  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:21.042459  180155 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0329 14:21:21.042850  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:21:21.163766  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:21:21.167647  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:21:21.905572  180155 ssh_runner.go:195] Run: systemctl --version
I0329 14:21:21.948685  180155 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0329 14:21:21.993877  180155 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0329 14:21:22.250446  180155 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0329 14:21:22.250873  180155 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0329 14:21:22.383531  180155 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0329 14:21:22.383586  180155 start.go:472] detecting cgroup driver to use...
I0329 14:21:22.383747  180155 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0329 14:21:22.384229  180155 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0329 14:21:22.455247  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0329 14:21:22.501273  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0329 14:21:22.542233  180155 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0329 14:21:22.542442  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0329 14:21:22.586354  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0329 14:21:22.624860  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0329 14:21:22.665453  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0329 14:21:22.705432  180155 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0329 14:21:22.745476  180155 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0329 14:21:22.790897  180155 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0329 14:21:22.830623  180155 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0329 14:21:22.867391  180155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 14:21:23.265055  180155 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0329 14:21:23.702035  180155 start.go:472] detecting cgroup driver to use...
I0329 14:21:23.702133  180155 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0329 14:21:23.702319  180155 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0329 14:21:23.794305  180155 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0329 14:21:23.794465  180155 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0329 14:21:23.876461  180155 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0329 14:21:24.033130  180155 ssh_runner.go:195] Run: which cri-dockerd
I0329 14:21:24.055303  180155 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0329 14:21:24.144897  180155 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0329 14:21:24.336446  180155 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0329 14:21:24.879193  180155 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0329 14:21:25.574037  180155 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0329 14:21:25.580648  180155 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0329 14:21:25.675207  180155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 14:21:26.346869  180155 ssh_runner.go:195] Run: sudo systemctl restart docker
I0329 14:21:28.832948  180155 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.486013817s)
I0329 14:21:28.833410  180155 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0329 14:21:29.260187  180155 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0329 14:21:29.613264  180155 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0329 14:21:30.063874  180155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 14:21:30.714624  180155 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0329 14:21:30.797063  180155 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 14:21:31.323668  180155 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0329 14:21:32.287930  180155 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0329 14:21:32.288203  180155 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0329 14:21:32.316597  180155 start.go:540] Will wait 60s for crictl version
I0329 14:21:32.316803  180155 ssh_runner.go:195] Run: which crictl
I0329 14:21:32.339654  180155 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0329 14:21:33.137200  180155 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0329 14:21:33.137475  180155 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0329 14:21:33.539094  180155 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0329 14:21:33.696838  180155 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0329 14:21:33.702864  180155 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0329 14:21:33.815256  180155 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0329 14:21:33.834184  180155 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0329 14:21:33.896224  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0329 14:21:33.987624  180155 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0329 14:21:33.987832  180155 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0329 14:21:34.090896  180155 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0329 14:21:34.092938  180155 docker.go:601] Images already preloaded, skipping extraction
I0329 14:21:34.093430  180155 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0329 14:21:34.207710  180155 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0329 14:21:34.207764  180155 cache_images.go:84] Images are preloaded, skipping loading
I0329 14:21:34.211225  180155 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0329 14:21:34.498055  180155 cni.go:84] Creating CNI manager for ""
I0329 14:21:34.498197  180155 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 14:21:34.500568  180155 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0329 14:21:34.500674  180155 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0329 14:21:34.501338  180155 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0329 14:21:34.501524  180155 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0329 14:21:34.501683  180155 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0329 14:21:34.558733  180155 binaries.go:44] Found k8s binaries, skipping transfer
I0329 14:21:34.559090  180155 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0329 14:21:34.607053  180155 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0329 14:21:34.703246  180155 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0329 14:21:34.804473  180155 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0329 14:21:34.899234  180155 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0329 14:21:34.919765  180155 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0329 14:21:34.978134  180155 certs.go:56] Setting up /home/lee/.minikube/profiles/minikube for IP: 192.168.49.2
I0329 14:21:34.978225  180155 certs.go:190] acquiring lock for shared ca certs: {Name:mk1cf981783681b0bb5a6ac43e6863efc3a4fc0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:34.978861  180155 certs.go:204] generating minikubeCA CA: /home/lee/.minikube/ca.key
I0329 14:21:35.230541  180155 crypto.go:156] Writing cert to /home/lee/.minikube/ca.crt ...
I0329 14:21:35.230565  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/ca.crt: {Name:mkfb1009fb726d86db4e05135067447d08b57a65 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:35.230874  180155 crypto.go:164] Writing key to /home/lee/.minikube/ca.key ...
I0329 14:21:35.230884  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/ca.key: {Name:mk8731cc33602595792d4f96900f76ff36ebf599 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:35.231075  180155 certs.go:204] generating proxyClientCA CA: /home/lee/.minikube/proxy-client-ca.key
I0329 14:21:35.632032  180155 crypto.go:156] Writing cert to /home/lee/.minikube/proxy-client-ca.crt ...
I0329 14:21:35.632051  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/proxy-client-ca.crt: {Name:mkad06c80d759edf02f582b5dae1f49eb61801c9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:35.632346  180155 crypto.go:164] Writing key to /home/lee/.minikube/proxy-client-ca.key ...
I0329 14:21:35.632356  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/proxy-client-ca.key: {Name:mk170a090984dfce28bfc7765606291c3d1627e9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:35.632632  180155 certs.go:319] generating minikube-user signed cert: /home/lee/.minikube/profiles/minikube/client.key
I0329 14:21:35.632661  180155 crypto.go:68] Generating cert /home/lee/.minikube/profiles/minikube/client.crt with IP's: []
I0329 14:21:35.863537  180155 crypto.go:156] Writing cert to /home/lee/.minikube/profiles/minikube/client.crt ...
I0329 14:21:35.863551  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/client.crt: {Name:mkab7792d2e5eb677e3c834e2d13b84e6b2670ff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:35.863811  180155 crypto.go:164] Writing key to /home/lee/.minikube/profiles/minikube/client.key ...
I0329 14:21:35.863818  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/client.key: {Name:mk169007e6c0b9574674437b86bef19c3f194a87 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:35.863960  180155 certs.go:319] generating minikube signed cert: /home/lee/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0329 14:21:35.863974  180155 crypto.go:68] Generating cert /home/lee/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0329 14:21:36.201964  180155 crypto.go:156] Writing cert to /home/lee/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0329 14:21:36.201984  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk12d1f5f37aa4a1ce92275eb2053b9b24b6d5e8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:36.202274  180155 crypto.go:164] Writing key to /home/lee/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0329 14:21:36.202285  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk99f38d4d5a0d28974bc6556cb7e747a89f2d95 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:36.202545  180155 certs.go:337] copying /home/lee/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/lee/.minikube/profiles/minikube/apiserver.crt
I0329 14:21:36.202716  180155 certs.go:341] copying /home/lee/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/lee/.minikube/profiles/minikube/apiserver.key
I0329 14:21:36.202835  180155 certs.go:319] generating aggregator signed cert: /home/lee/.minikube/profiles/minikube/proxy-client.key
I0329 14:21:36.202859  180155 crypto.go:68] Generating cert /home/lee/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0329 14:21:37.017781  180155 crypto.go:156] Writing cert to /home/lee/.minikube/profiles/minikube/proxy-client.crt ...
I0329 14:21:37.017796  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/proxy-client.crt: {Name:mk94d061a9288e5982ee0e0c89acce69df347ddf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:37.018034  180155 crypto.go:164] Writing key to /home/lee/.minikube/profiles/minikube/proxy-client.key ...
I0329 14:21:37.018041  180155 lock.go:35] WriteFile acquiring /home/lee/.minikube/profiles/minikube/proxy-client.key: {Name:mk5c4fd0f40eb3c086351a0a7b6f013aa3786c4f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:21:37.020210  180155 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/ca-key.pem (1679 bytes)
I0329 14:21:37.020319  180155 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/ca.pem (1070 bytes)
I0329 14:21:37.020356  180155 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/cert.pem (1111 bytes)
I0329 14:21:37.020395  180155 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/key.pem (1679 bytes)
I0329 14:21:37.048141  180155 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0329 14:21:37.101367  180155 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0329 14:21:37.149263  180155 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0329 14:21:37.200488  180155 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0329 14:21:37.250791  180155 ssh_runner.go:362] scp /home/lee/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0329 14:21:37.294936  180155 ssh_runner.go:362] scp /home/lee/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0329 14:21:37.337721  180155 ssh_runner.go:362] scp /home/lee/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0329 14:21:37.383087  180155 ssh_runner.go:362] scp /home/lee/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0329 14:21:37.428453  180155 ssh_runner.go:362] scp /home/lee/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0329 14:21:37.474481  180155 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0329 14:21:37.507339  180155 ssh_runner.go:195] Run: openssl version
I0329 14:21:37.527810  180155 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0329 14:21:37.550910  180155 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0329 14:21:37.558295  180155 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar 29 05:21 /usr/share/ca-certificates/minikubeCA.pem
I0329 14:21:37.558398  180155 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0329 14:21:37.570828  180155 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0329 14:21:37.589561  180155 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0329 14:21:37.597942  180155 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0329 14:21:37.598002  180155 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0329 14:21:37.598145  180155 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0329 14:21:37.640237  180155 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0329 14:21:37.667006  180155 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0329 14:21:37.690752  180155 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I0329 14:21:37.691025  180155 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0329 14:21:37.715353  180155 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0329 14:21:37.716956  180155 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0329 14:21:38.310993  180155 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I0329 14:21:38.311336  180155 kubeadm.go:322] [preflight] Running pre-flight checks
I0329 14:21:39.731779  180155 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0329 14:21:39.732484  180155 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0329 14:21:39.733338  180155 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0329 14:21:41.300684  180155 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0329 14:21:41.314356  180155 out.go:204]     ▪ Generating certificates and keys ...
I0329 14:21:41.316042  180155 kubeadm.go:322] [certs] Using existing ca certificate authority
I0329 14:21:41.316882  180155 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0329 14:21:41.681779  180155 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0329 14:21:42.227122  180155 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0329 14:21:42.990025  180155 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0329 14:21:43.164042  180155 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0329 14:21:43.703255  180155 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0329 14:21:43.703785  180155 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0329 14:21:44.093602  180155 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0329 14:21:44.093936  180155 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0329 14:21:44.323271  180155 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0329 14:21:44.601292  180155 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0329 14:21:44.962306  180155 kubeadm.go:322] [certs] Generating "sa" key and public key
I0329 14:21:44.962581  180155 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0329 14:21:45.197480  180155 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0329 14:21:45.365057  180155 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0329 14:21:45.811713  180155 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0329 14:21:46.751882  180155 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0329 14:21:46.753385  180155 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0329 14:21:46.761295  180155 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0329 14:21:46.764622  180155 out.go:204]     ▪ Booting up control plane ...
I0329 14:21:46.765047  180155 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0329 14:21:46.765362  180155 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0329 14:21:46.765592  180155 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0329 14:21:46.787728  180155 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0329 14:21:46.789427  180155 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0329 14:21:46.789570  180155 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0329 14:21:47.154093  180155 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0329 14:22:27.154728  180155 kubeadm.go:322] [kubelet-check] Initial timeout of 40s passed.
I0329 14:22:28.670879  180155 kubeadm.go:322] [apiclient] All control plane components are healthy after 41.516397 seconds
I0329 14:22:28.671590  180155 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0329 14:22:28.745723  180155 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0329 14:22:29.338381  180155 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0329 14:22:29.339165  180155 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0329 14:22:29.907305  180155 kubeadm.go:322] [bootstrap-token] Using token: wwwhyh.g0sxbzfd1zpclpsn
I0329 14:22:29.916643  180155 out.go:204]     ▪ Configuring RBAC rules ...
I0329 14:22:29.917643  180155 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0329 14:22:29.953929  180155 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0329 14:22:29.978161  180155 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0329 14:22:29.985028  180155 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0329 14:22:29.994763  180155 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0329 14:22:30.001840  180155 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0329 14:22:30.031742  180155 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0329 14:22:31.688571  180155 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0329 14:22:32.034629  180155 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0329 14:22:32.045867  180155 kubeadm.go:322] 
I0329 14:22:32.046392  180155 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0329 14:22:32.046431  180155 kubeadm.go:322] 
I0329 14:22:32.052559  180155 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0329 14:22:32.052619  180155 kubeadm.go:322] 
I0329 14:22:32.052945  180155 kubeadm.go:322]   mkdir -p $HOME/.kube
I0329 14:22:32.053547  180155 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0329 14:22:32.053993  180155 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0329 14:22:32.054027  180155 kubeadm.go:322] 
I0329 14:22:32.054481  180155 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0329 14:22:32.054517  180155 kubeadm.go:322] 
I0329 14:22:32.054931  180155 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0329 14:22:32.054980  180155 kubeadm.go:322] 
I0329 14:22:32.055429  180155 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0329 14:22:32.056064  180155 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0329 14:22:32.056654  180155 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0329 14:22:32.056689  180155 kubeadm.go:322] 
I0329 14:22:32.057694  180155 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0329 14:22:32.058172  180155 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0329 14:22:32.058199  180155 kubeadm.go:322] 
I0329 14:22:32.058780  180155 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token wwwhyh.g0sxbzfd1zpclpsn \
I0329 14:22:32.059409  180155 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:3454b9633ebec846bc9e9fd1ded20cc9a96229812a67213a4cae6cac1947acd5 \
I0329 14:22:32.059943  180155 kubeadm.go:322] 	--control-plane 
I0329 14:22:32.059986  180155 kubeadm.go:322] 
I0329 14:22:32.060615  180155 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0329 14:22:32.060652  180155 kubeadm.go:322] 
I0329 14:22:32.061271  180155 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token wwwhyh.g0sxbzfd1zpclpsn \
I0329 14:22:32.061930  180155 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:3454b9633ebec846bc9e9fd1ded20cc9a96229812a67213a4cae6cac1947acd5 
I0329 14:22:32.101146  180155 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0329 14:22:32.101575  180155 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0329 14:22:32.101624  180155 cni.go:84] Creating CNI manager for ""
I0329 14:22:32.101669  180155 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 14:22:32.107437  180155 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0329 14:22:32.118857  180155 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0329 14:22:32.362394  180155 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0329 14:22:32.860634  180155 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0329 14:22:32.862130  180155 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0329 14:22:32.862433  180155 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2024_03_29T14_22_32_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0329 14:22:37.671004  180155 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2024_03_29T14_22_32_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig: (4.808519954s)
I0329 14:22:37.671063  180155 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (4.810398454s)
I0329 14:22:37.671094  180155 ops.go:34] apiserver oom_adj: -16
I0329 14:22:37.671204  180155 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (4.809031954s)
I0329 14:22:37.671228  180155 kubeadm.go:1081] duration metric: took 4.809262454s to wait for elevateKubeSystemPrivileges.
I0329 14:22:37.671242  180155 kubeadm.go:406] StartCluster complete in 1m0.073248402s
I0329 14:22:37.671267  180155 settings.go:142] acquiring lock: {Name:mkac65ce0183564c0160b580651098887c156546 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:22:37.672457  180155 settings.go:150] Updating kubeconfig:  /home/lee/.kube/config
I0329 14:22:37.674439  180155 lock.go:35] WriteFile acquiring /home/lee/.kube/config: {Name:mke78f7e985c39ffd6431c856c291714e0dfcbf0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 14:22:37.674972  180155 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0329 14:22:37.677340  180155 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0329 14:22:37.680741  180155 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0329 14:22:37.680877  180155 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0329 14:22:37.680901  180155 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0329 14:22:37.681057  180155 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0329 14:22:37.682951  180155 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0329 14:22:37.684483  180155 host.go:66] Checking if "minikube" exists ...
I0329 14:22:37.687578  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 14:22:37.688800  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 14:22:37.762491  180155 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0329 14:22:37.777649  180155 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0329 14:22:37.777663  180155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0329 14:22:37.777906  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:22:37.778564  180155 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0329 14:22:37.778616  180155 host.go:66] Checking if "minikube" exists ...
I0329 14:22:37.780168  180155 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 14:22:37.858322  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:22:37.870449  180155 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0329 14:22:37.870470  180155 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0329 14:22:37.870618  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 14:22:37.928538  180155 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 14:22:37.956144  180155 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0329 14:22:37.956208  180155 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0329 14:22:37.960291  180155 out.go:177] 🔎  Verifying Kubernetes components...
I0329 14:22:37.974293  180155 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0329 14:22:39.172708  180155 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.198310929s)
I0329 14:22:39.173086  180155 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0329 14:22:39.275957  180155 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.60088534s)
I0329 14:22:39.278771  180155 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0329 14:22:39.278878  180155 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0329 14:22:39.326502  180155 api_server.go:52] waiting for apiserver process to appear ...
I0329 14:22:39.326574  180155 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 14:22:39.551213  180155 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0329 14:23:06.872101  180155 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (27.593202282s)
I0329 14:23:06.872398  180155 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (27.593445882s)
I0329 14:23:06.872467  180155 start.go:926] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0329 14:23:06.872602  180155 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (27.545978481s)
I0329 14:23:06.872655  180155 api_server.go:72] duration metric: took 28.916377915s to wait for apiserver process to appear ...
I0329 14:23:06.872683  180155 api_server.go:88] waiting for apiserver healthz status ...
I0329 14:23:06.872758  180155 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 14:23:06.874894  180155 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (27.323589975s)
I0329 14:23:07.064053  180155 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0329 14:23:07.093720  180155 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0329 14:23:07.112236  180155 addons.go:502] enable addons completed in 29.431465438s: enabled=[storage-provisioner default-storageclass]
I0329 14:23:07.135768  180155 api_server.go:141] control plane version: v1.28.3
I0329 14:23:07.135854  180155 api_server.go:131] duration metric: took 263.136918ms to wait for apiserver health ...
I0329 14:23:07.135977  180155 system_pods.go:43] waiting for kube-system pods to appear ...
I0329 14:23:07.337626  180155 system_pods.go:59] 7 kube-system pods found
I0329 14:23:07.337859  180155 system_pods.go:61] "coredns-5dd5756b68-r7x2l" [cefa8631-fb98-4413-93a4-bfc2b264055e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0329 14:23:07.337908  180155 system_pods.go:61] "etcd-minikube" [dd41b80d-3e65-4a5f-a17b-00a713a9505d] Running
I0329 14:23:07.337979  180155 system_pods.go:61] "kube-apiserver-minikube" [a06337f4-100b-44df-ad92-6f3f7aabb4f0] Running
I0329 14:23:07.338028  180155 system_pods.go:61] "kube-controller-manager-minikube" [c159ab35-d19a-4b52-919d-e406e6664940] Running
I0329 14:23:07.338062  180155 system_pods.go:61] "kube-proxy-2cp4n" [ff9bf080-33af-4fc2-a1b6-a8d898db14ef] Running
I0329 14:23:07.338095  180155 system_pods.go:61] "kube-scheduler-minikube" [5be45852-8295-4133-a8bb-4cec3e936781] Running
I0329 14:23:07.338162  180155 system_pods.go:61] "storage-provisioner" [24bf02fc-5c5a-41bf-aa78-0309d0b93889] Pending
I0329 14:23:07.338224  180155 system_pods.go:74] duration metric: took 202.190213ms to wait for pod list to return data ...
I0329 14:23:07.338383  180155 kubeadm.go:581] duration metric: took 29.382004746s to wait for : map[apiserver:true system_pods:true] ...
I0329 14:23:07.338453  180155 node_conditions.go:102] verifying NodePressure condition ...
I0329 14:23:07.383267  180155 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0329 14:23:07.383293  180155 node_conditions.go:123] node cpu capacity is 4
I0329 14:23:07.386116  180155 node_conditions.go:105] duration metric: took 47.642303ms to run NodePressure ...
I0329 14:23:07.386150  180155 start.go:228] waiting for startup goroutines ...
I0329 14:23:07.386160  180155 start.go:233] waiting for cluster config update ...
I0329 14:23:07.386176  180155 start.go:242] writing updated cluster config ...
I0329 14:23:07.388583  180155 ssh_runner.go:195] Run: rm -f paused
I0329 14:23:08.300780  180155 start.go:600] kubectl: 1.29.3, cluster: 1.28.3 (minor skew: 1)
I0329 14:23:08.304803  180155 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.385857609Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.385868609Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.385918309Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.386025709Z" level=info msg="Daemon has completed initialization"
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.529181499Z" level=info msg="API listen on /var/run/docker.sock"
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.529259599Z" level=info msg="API listen on [::]:2376"
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.532954299Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Mar 29 05:21:26 minikube dockerd[945]: time="2024-03-29T05:21:26.534890898Z" level=info msg="Daemon shutdown complete"
Mar 29 05:21:26 minikube systemd[1]: docker.service: Deactivated successfully.
Mar 29 05:21:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Mar 29 05:21:26 minikube systemd[1]: Starting Docker Application Container Engine...
Mar 29 05:21:26 minikube dockerd[1142]: time="2024-03-29T05:21:26.651564990Z" level=info msg="Starting up"
Mar 29 05:21:26 minikube dockerd[1142]: time="2024-03-29T05:21:26.680973188Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Mar 29 05:21:26 minikube dockerd[1142]: time="2024-03-29T05:21:26.712366485Z" level=info msg="Loading containers: start."
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.133433181Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.440267558Z" level=info msg="Loading containers: done."
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.513836852Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.514014152Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.514053552Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.514075852Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.514142852Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.514279152Z" level=info msg="Daemon has completed initialization"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.822746430Z" level=info msg="API listen on /var/run/docker.sock"
Mar 29 05:21:28 minikube dockerd[1142]: time="2024-03-29T05:21:28.822953130Z" level=info msg="API listen on [::]:2376"
Mar 29 05:21:28 minikube systemd[1]: Started Docker Application Container Engine.
Mar 29 05:21:31 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Start docker client with request timeout 0s"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Hairpin mode is set to hairpin-veth"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Loaded network plugin cni"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Docker cri networking managed by network plugin cni"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Docker Info: &{ID:fb9696b6-0100-447a-8a87-c26b394bf022 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:36 SystemTime:2024-03-29T05:21:32.221917379Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc00026af50 NCPU:4 MemTotal:4035866624 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Setting cgroupDriver cgroupfs"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Mar 29 05:21:32 minikube cri-dockerd[1354]: time="2024-03-29T05:21:32Z" level=info msg="Start cri-dockerd grpc backend"
Mar 29 05:21:32 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Mar 29 05:21:51 minikube cri-dockerd[1354]: time="2024-03-29T05:21:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/027818adbb12a1cf8771e4aab751a32f44712e6abcc29f4d86f1a68b46d2dd51/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 05:21:51 minikube cri-dockerd[1354]: time="2024-03-29T05:21:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b9cbf0f58a88fb36962a658cd3f5add00dba2095a4e66763f4c519e6e80875d7/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 05:21:51 minikube cri-dockerd[1354]: time="2024-03-29T05:21:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2e3536aadb305a6c0fd38f6ea66e2a95e9e988d918147e811fbc3c04c8a948d0/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 05:21:52 minikube cri-dockerd[1354]: time="2024-03-29T05:21:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c0241f57df6546a0d9605a41128f8761b39add7f41c0b15455e6c24b411f20ef/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 05:22:54 minikube cri-dockerd[1354]: time="2024-03-29T05:22:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db5b4e75ec0928a375088e2c6ee933a0d2f4c36ee196afcae81c47a7b494b02c/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 05:22:56 minikube cri-dockerd[1354]: time="2024-03-29T05:22:56Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 29 05:22:59 minikube cri-dockerd[1354]: time="2024-03-29T05:22:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ec6b6fa3bf9bf4e084ac39df6d36cc290452ce69d3ff1863962cb363851f5a0d/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 05:23:07 minikube cri-dockerd[1354]: time="2024-03-29T05:23:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/facd4ee5057965a52214bb92193e30ace1b0462276819483b776de7e1ee43c09/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:11:36 minikube cri-dockerd[1354]: time="2024-03-29T07:11:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d4d2323dacaf7e966a3396b2a29e380e9fdca6bbecf46657b31c0747ce617291/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 07:11:40 minikube dockerd[1142]: time="2024-03-29T07:11:40.866785107Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:11:40 minikube dockerd[1142]: time="2024-03-29T07:11:40.869730808Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 29 07:11:59 minikube dockerd[1142]: time="2024-03-29T07:11:59.908531343Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:11:59 minikube dockerd[1142]: time="2024-03-29T07:11:59.909026043Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 29 07:12:33 minikube dockerd[1142]: time="2024-03-29T07:12:33.292361292Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:12:33 minikube dockerd[1142]: time="2024-03-29T07:12:33.292518592Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 29 07:13:19 minikube dockerd[1142]: time="2024-03-29T07:13:19.576281788Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:13:19 minikube dockerd[1142]: time="2024-03-29T07:13:19.576807188Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 29 07:14:43 minikube dockerd[1142]: time="2024-03-29T07:14:43.898882099Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:14:43 minikube dockerd[1142]: time="2024-03-29T07:14:43.899266699Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 29 07:17:36 minikube dockerd[1142]: time="2024-03-29T07:17:36.599442848Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:17:36 minikube dockerd[1142]: time="2024-03-29T07:17:36.599708648Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 29 07:22:49 minikube dockerd[1142]: time="2024-03-29T07:22:49.734058425Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 29 07:22:49 minikube dockerd[1142]: time="2024-03-29T07:22:49.734624925Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
1f8f6873d7e4f       6e38f40d628db       2 hours ago         Running             storage-provisioner       0                   facd4ee505796       storage-provisioner
88882201cd883       ead0a4a53df89       2 hours ago         Running             coredns                   0                   ec6b6fa3bf9bf       coredns-5dd5756b68-r7x2l
dcbe0483c3fbc       bfc896cf80fba       2 hours ago         Running             kube-proxy                0                   db5b4e75ec092       kube-proxy-2cp4n
e9cfcebd9af69       5374347291230       2 hours ago         Running             kube-apiserver            0                   b9cbf0f58a88f       kube-apiserver-minikube
ea9b9aea2cfbd       73deb9a3f7025       2 hours ago         Running             etcd                      0                   c0241f57df654       etcd-minikube
b984a30c81076       6d1b4fd1b182d       2 hours ago         Running             kube-scheduler            0                   2e3536aadb305       kube-scheduler-minikube
ab42615bd9c52       10baa1ca17068       2 hours ago         Running             kube-controller-manager   0                   027818adbb12a       kube-controller-manager-minikube

* 
* ==> coredns [88882201cd88] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:32987 - 16690 "HINFO IN 6256261035404887090.4925723574841502497. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.034221096s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_29T14_22_32_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 29 Mar 2024 05:22:21 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 29 Mar 2024 07:23:30 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 29 Mar 2024 07:20:46 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 29 Mar 2024 07:20:46 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 29 Mar 2024 07:20:46 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 29 Mar 2024 07:20:46 +0000   Fri, 29 Mar 2024 05:22:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941276Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941276Ki
  pods:               110
System Info:
  Machine ID:                 e790007a52ab45f88d0859d1935684b8
  System UUID:                e790007a52ab45f88d0859d1935684b8
  Boot ID:                    f67e2ab8-e954-494c-8c47-1f9b00e8dbae
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     fastapi-deployment-588df47757-r8mvl    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12m
  kube-system                 coredns-5dd5756b68-r7x2l               100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     120m
  kube-system                 etcd-minikube                          100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         120m
  kube-system                 kube-apiserver-minikube                250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         120m
  kube-system                 kube-controller-manager-minikube       200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         120m
  kube-system                 kube-proxy-2cp4n                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         120m
  kube-system                 kube-scheduler-minikube                100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         120m
  kube-system                 storage-provisioner                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         120m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Mar28 15:35] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.000000] PCI: Fatal: No config space access function found
[  +0.020887] PCI: System does not support PCI
[  +0.039109] kvm: no hardware support
[  +0.000005] kvm: no hardware support
[  +0.822368] FS-Cache: Duplicate cookie detected
[  +0.000580] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.001113] FS-Cache: O-cookie d=00000000d2239c5b{9P.session} n=00000000a8ae233a
[  +0.002107] FS-Cache: O-key=[10] '34323934393337333835'
[  +0.002764] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000930] FS-Cache: N-cookie d=00000000d2239c5b{9P.session} n=000000003b585744
[  +0.003254] FS-Cache: N-key=[10] '34323934393337333835'
[  +0.007823] FS-Cache: Duplicate cookie detected
[  +0.000653] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000643] FS-Cache: O-cookie d=00000000d2239c5b{9P.session} n=00000000017dd655
[  +0.000712] FS-Cache: O-key=[10] '34323934393337333837'
[  +0.000579] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000670] FS-Cache: N-cookie d=00000000d2239c5b{9P.session} n=000000007723fc8b
[  +0.000938] FS-Cache: N-key=[10] '34323934393337333837'
[  +1.266886] Failed to connect to bus: No such file or directory
[  +0.208744] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.049081] Failed to connect to bus: No such file or directory
[Mar29 05:23] hrtimer: interrupt took 10824198 ns

* 
* ==> etcd [ea9b9aea2cfb] <==
* {"level":"info","ts":"2024-03-29T06:53:57.703727Z","caller":"traceutil/trace.go:171","msg":"trace[851582110] transaction","detail":"{read_only:false; response_revision:4732; number_of_response:1; }","duration":"100.794011ms","start":"2024-03-29T06:53:57.6029Z","end":"2024-03-29T06:53:57.703694Z","steps":["trace[851582110] 'process raft request'  (duration: 100.358511ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T06:56:09.822666Z","caller":"traceutil/trace.go:171","msg":"trace[371390200] transaction","detail":"{read_only:false; response_revision:4838; number_of_response:1; }","duration":"239.436827ms","start":"2024-03-29T06:56:09.583202Z","end":"2024-03-29T06:56:09.822639Z","steps":["trace[371390200] 'process raft request'  (duration: 238.860327ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T06:56:09.822683Z","caller":"traceutil/trace.go:171","msg":"trace[1726800579] transaction","detail":"{read_only:false; response_revision:4837; number_of_response:1; }","duration":"388.599544ms","start":"2024-03-29T06:56:09.434055Z","end":"2024-03-29T06:56:09.822654Z","steps":["trace[1726800579] 'process raft request'  (duration: 296.127133ms)","trace[1726800579] 'compare'  (duration: 91.30691ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T06:56:09.823052Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T06:56:09.434028Z","time spent":"388.917244ms","remote":"127.0.0.1:56916","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:4836 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2024-03-29T06:56:14.030049Z","caller":"traceutil/trace.go:171","msg":"trace[441018266] transaction","detail":"{read_only:false; response_revision:4841; number_of_response:1; }","duration":"146.819215ms","start":"2024-03-29T06:56:13.883198Z","end":"2024-03-29T06:56:14.030017Z","steps":["trace[441018266] 'process raft request'  (duration: 146.514415ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T06:56:18.220333Z","caller":"traceutil/trace.go:171","msg":"trace[317725015] transaction","detail":"{read_only:false; response_revision:4844; number_of_response:1; }","duration":"101.213011ms","start":"2024-03-29T06:56:18.119027Z","end":"2024-03-29T06:56:18.220241Z","steps":["trace[317725015] 'process raft request'  (duration: 95.41051ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T06:56:28.907619Z","caller":"traceutil/trace.go:171","msg":"trace[475584346] transaction","detail":"{read_only:false; response_revision:4852; number_of_response:1; }","duration":"121.106379ms","start":"2024-03-29T06:56:28.786477Z","end":"2024-03-29T06:56:28.907584Z","steps":["trace[475584346] 'process raft request'  (duration: 120.854279ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T06:56:58.328887Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4638}
{"level":"info","ts":"2024-03-29T06:56:58.342548Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4638,"took":"9.9713ms","hash":797317833}
{"level":"info","ts":"2024-03-29T06:56:58.343691Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":797317833,"revision":4638,"compact-revision":4402}
{"level":"info","ts":"2024-03-29T07:00:01.566781Z","caller":"traceutil/trace.go:171","msg":"trace[974362084] transaction","detail":"{read_only:false; response_revision:5020; number_of_response:1; }","duration":"121.526197ms","start":"2024-03-29T07:00:01.445193Z","end":"2024-03-29T07:00:01.56672Z","steps":["trace[974362084] 'process raft request'  (duration: 121.074397ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:01:21.28284Z","caller":"traceutil/trace.go:171","msg":"trace[1142981468] transaction","detail":"{read_only:false; response_revision:5083; number_of_response:1; }","duration":"184.670005ms","start":"2024-03-29T07:01:21.098113Z","end":"2024-03-29T07:01:21.282783Z","steps":["trace[1142981468] 'process raft request'  (duration: 184.373205ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:01:58.409311Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4876}
{"level":"info","ts":"2024-03-29T07:01:58.41162Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":4876,"took":"1.779601ms","hash":3400915279}
{"level":"info","ts":"2024-03-29T07:01:58.411771Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3400915279,"revision":4876,"compact-revision":4638}
{"level":"warn","ts":"2024-03-29T07:02:09.770648Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.321616ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:02:09.770837Z","caller":"traceutil/trace.go:171","msg":"trace[1134550809] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5121; }","duration":"114.533416ms","start":"2024-03-29T07:02:09.656278Z","end":"2024-03-29T07:02:09.770811Z","steps":["trace[1134550809] 'range keys from in-memory index tree'  (duration: 114.119616ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:02:39.182082Z","caller":"traceutil/trace.go:171","msg":"trace[1802455170] transaction","detail":"{read_only:false; response_revision:5143; number_of_response:1; }","duration":"120.679921ms","start":"2024-03-29T07:02:39.061374Z","end":"2024-03-29T07:02:39.182054Z","steps":["trace[1802455170] 'process raft request'  (duration: 120.423521ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:04:04.810454Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"140.282204ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:04:04.810616Z","caller":"traceutil/trace.go:171","msg":"trace[1836488116] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5210; }","duration":"140.451804ms","start":"2024-03-29T07:04:04.670138Z","end":"2024-03-29T07:04:04.810589Z","steps":["trace[1836488116] 'range keys from in-memory index tree'  (duration: 140.050904ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:05:58.365031Z","caller":"traceutil/trace.go:171","msg":"trace[1010011714] transaction","detail":"{read_only:false; response_revision:5301; number_of_response:1; }","duration":"130.166306ms","start":"2024-03-29T07:05:58.234471Z","end":"2024-03-29T07:05:58.364638Z","steps":["trace[1010011714] 'process raft request'  (duration: 129.810506ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:06:44.085952Z","caller":"traceutil/trace.go:171","msg":"trace[567912677] transaction","detail":"{read_only:false; response_revision:5336; number_of_response:1; }","duration":"177.211945ms","start":"2024-03-29T07:06:43.908715Z","end":"2024-03-29T07:06:44.085927Z","steps":["trace[567912677] 'process raft request'  (duration: 177.072746ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:06:50.140203Z","caller":"traceutil/trace.go:171","msg":"trace[1575144698] linearizableReadLoop","detail":"{readStateIndex:6621; appliedIndex:6620; }","duration":"465.393554ms","start":"2024-03-29T07:06:49.674781Z","end":"2024-03-29T07:06:50.140175Z","steps":["trace[1575144698] 'read index received'  (duration: 464.950454ms)","trace[1575144698] 'applied index is now lower than readState.Index'  (duration: 440.9µs)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:06:50.140247Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T07:06:49.538773Z","time spent":"601.467923ms","remote":"127.0.0.1:48956","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-03-29T07:06:50.140385Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"465.581953ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:06:50.140453Z","caller":"traceutil/trace.go:171","msg":"trace[245590324] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5340; }","duration":"465.866753ms","start":"2024-03-29T07:06:49.674565Z","end":"2024-03-29T07:06:50.140432Z","steps":["trace[245590324] 'agreement among raft nodes before linearized reading'  (duration: 465.739553ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:06:50.140601Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T07:06:49.674425Z","time spent":"466.152153ms","remote":"127.0.0.1:48934","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-03-29T07:06:50.140774Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"364.462351ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:06:50.140833Z","caller":"traceutil/trace.go:171","msg":"trace[663462193] range","detail":"{range_begin:/registry/statefulsets/; range_end:/registry/statefulsets0; response_count:0; response_revision:5340; }","duration":"364.543551ms","start":"2024-03-29T07:06:49.776272Z","end":"2024-03-29T07:06:50.140815Z","steps":["trace[663462193] 'agreement among raft nodes before linearized reading'  (duration: 364.431951ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:06:50.1409Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T07:06:49.776224Z","time spent":"364.653851ms","remote":"127.0.0.1:57202","response type":"/etcdserverpb.KV/Range","request count":0,"request size":52,"response count":0,"response size":29,"request content":"key:\"/registry/statefulsets/\" range_end:\"/registry/statefulsets0\" count_only:true "}
{"level":"warn","ts":"2024-03-29T07:06:50.141276Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"364.32895ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:06:50.141355Z","caller":"traceutil/trace.go:171","msg":"trace[1550402015] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:5340; }","duration":"364.41095ms","start":"2024-03-29T07:06:49.776914Z","end":"2024-03-29T07:06:50.141325Z","steps":["trace[1550402015] 'agreement among raft nodes before linearized reading'  (duration: 364.292451ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:06:50.141434Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T07:06:49.776881Z","time spent":"364.52885ms","remote":"127.0.0.1:56958","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":29,"request content":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true "}
{"level":"info","ts":"2024-03-29T07:06:58.499938Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5111}
{"level":"info","ts":"2024-03-29T07:06:58.503521Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5111,"took":"2.888598ms","hash":1518114362}
{"level":"info","ts":"2024-03-29T07:06:58.503681Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1518114362,"revision":5111,"compact-revision":4876}
{"level":"info","ts":"2024-03-29T07:07:11.015417Z","caller":"traceutil/trace.go:171","msg":"trace[391162018] transaction","detail":"{read_only:false; response_revision:5359; number_of_response:1; }","duration":"176.30553ms","start":"2024-03-29T07:07:10.839085Z","end":"2024-03-29T07:07:11.015391Z","steps":["trace[391162018] 'process raft request'  (duration: 176.08493ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:10:19.878821Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.338915ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2024-03-29T07:10:19.879132Z","caller":"traceutil/trace.go:171","msg":"trace[910973646] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:5504; }","duration":"144.809315ms","start":"2024-03-29T07:10:19.734242Z","end":"2024-03-29T07:10:19.879051Z","steps":["trace[910973646] 'count revisions from in-memory index tree'  (duration: 143.914815ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:11:33.956187Z","caller":"traceutil/trace.go:171","msg":"trace[1029941469] transaction","detail":"{read_only:false; response_revision:5574; number_of_response:1; }","duration":"101.101ms","start":"2024-03-29T07:11:33.854973Z","end":"2024-03-29T07:11:33.956074Z","steps":["trace[1029941469] 'process raft request'  (duration: 28.8198ms)","trace[1029941469] 'compare'  (duration: 63.8756ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T07:11:34.833542Z","caller":"traceutil/trace.go:171","msg":"trace[2111539975] transaction","detail":"{read_only:false; response_revision:5585; number_of_response:1; }","duration":"152.499917ms","start":"2024-03-29T07:11:34.680958Z","end":"2024-03-29T07:11:34.833458Z","steps":["trace[2111539975] 'process raft request'  (duration: 151.002917ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:11:52.598152Z","caller":"traceutil/trace.go:171","msg":"trace[1678347464] transaction","detail":"{read_only:false; response_revision:5608; number_of_response:1; }","duration":"157.4609ms","start":"2024-03-29T07:11:52.440497Z","end":"2024-03-29T07:11:52.597958Z","steps":["trace[1678347464] 'process raft request'  (duration: 156.7927ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:11:58.570058Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5349}
{"level":"info","ts":"2024-03-29T07:11:58.574403Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5349,"took":"3.6317ms","hash":2213397720}
{"level":"info","ts":"2024-03-29T07:11:58.57456Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2213397720,"revision":5349,"compact-revision":5111}
{"level":"info","ts":"2024-03-29T07:12:49.831778Z","caller":"traceutil/trace.go:171","msg":"trace[810757466] transaction","detail":"{read_only:false; response_revision:5668; number_of_response:1; }","duration":"151.889597ms","start":"2024-03-29T07:12:49.679815Z","end":"2024-03-29T07:12:49.831704Z","steps":["trace[810757466] 'process raft request'  (duration: 151.459197ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:16:53.926169Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"232.714483ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:16:53.926653Z","caller":"traceutil/trace.go:171","msg":"trace[2079493062] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:5875; }","duration":"233.264683ms","start":"2024-03-29T07:16:53.693317Z","end":"2024-03-29T07:16:53.926582Z","steps":["trace[2079493062] 'range keys from in-memory index tree'  (duration: 232.222283ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:16:58.601957Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5615}
{"level":"info","ts":"2024-03-29T07:16:58.610975Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5615,"took":"7.2743ms","hash":1943132941}
{"level":"info","ts":"2024-03-29T07:16:58.611368Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1943132941,"revision":5615,"compact-revision":5349}
{"level":"info","ts":"2024-03-29T07:20:32.508603Z","caller":"traceutil/trace.go:171","msg":"trace[1192644421] linearizableReadLoop","detail":"{readStateIndex:7500; appliedIndex:7499; }","duration":"237.103878ms","start":"2024-03-29T07:20:32.271205Z","end":"2024-03-29T07:20:32.508309Z","steps":["trace[1192644421] 'read index received'  (duration: 236.339678ms)","trace[1192644421] 'applied index is now lower than readState.Index'  (duration: 758.6µs)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:20:32.509396Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"238.168378ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/persistentvolumeclaims/\" range_end:\"/registry/persistentvolumeclaims0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:20:32.510095Z","caller":"traceutil/trace.go:171","msg":"trace[1348156641] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:6048; }","duration":"238.884178ms","start":"2024-03-29T07:20:32.271122Z","end":"2024-03-29T07:20:32.510006Z","steps":["trace[1348156641] 'agreement among raft nodes before linearized reading'  (duration: 237.777378ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:20:32.519799Z","caller":"traceutil/trace.go:171","msg":"trace[1572565604] transaction","detail":"{read_only:false; response_revision:6048; number_of_response:1; }","duration":"273.239175ms","start":"2024-03-29T07:20:32.246214Z","end":"2024-03-29T07:20:32.519454Z","steps":["trace[1572565604] 'process raft request'  (duration: 261.637076ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:20:34.773269Z","caller":"traceutil/trace.go:171","msg":"trace[1865998266] transaction","detail":"{read_only:false; response_revision:6049; number_of_response:1; }","duration":"119.787389ms","start":"2024-03-29T07:20:34.653405Z","end":"2024-03-29T07:20:34.773193Z","steps":["trace[1865998266] 'process raft request'  (duration: 119.342389ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:20:56.631263Z","caller":"traceutil/trace.go:171","msg":"trace[1468550512] transaction","detail":"{read_only:false; response_revision:6067; number_of_response:1; }","duration":"165.846101ms","start":"2024-03-29T07:20:56.465338Z","end":"2024-03-29T07:20:56.631184Z","steps":["trace[1468550512] 'process raft request'  (duration: 165.331201ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:21:58.70998Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":5878}
{"level":"info","ts":"2024-03-29T07:21:58.711882Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":5878,"took":"1.2032ms","hash":3265623964}
{"level":"info","ts":"2024-03-29T07:21:58.711978Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3265623964,"revision":5878,"compact-revision":5615}

* 
* ==> kernel <==
*  07:23:36 up 15:48,  0 users,  load average: 2.34, 1.93, 1.81
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [e9cfcebd9af6] <==
* Trace[2108241967]: [611.500489ms] [611.500489ms] END
I0329 05:22:46.731610       1 trace.go:236] Trace[321391855]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:7f6cdb88-1f5c-40f2-b1db-99cdda135825,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:ttl-controller,verb:PATCH (29-Mar-2024 05:22:46.165) (total time: 565ms):
Trace[321391855]: ---"About to check admission control" 195ms (05:22:46.361)
Trace[321391855]: ---"Object stored in database" 273ms (05:22:46.635)
Trace[321391855]: ---"Writing http response done" 96ms (05:22:46.731)
Trace[321391855]: [565.56979ms] [565.56979ms] END
I0329 05:22:47.035014       1 trace.go:236] Trace[542486300]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c715b7a5-ff60-4f19-93e1-24d99d895861,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/endpointslice-controller/token,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/kube-controller-manager,verb:POST (29-Mar-2024 05:22:46.453) (total time: 581ms):
Trace[542486300]: ---"Write to database call succeeded" len:81 580ms (05:22:47.034)
Trace[542486300]: [581.57949ms] [581.57949ms] END
I0329 05:22:47.051399       1 trace.go:236] Trace[1987764628]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:fcccc278-d7f9-4e76-a4f5-a867bb68ab3c,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/certificate-controller/token,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/kube-controller-manager,verb:POST (29-Mar-2024 05:22:46.466) (total time: 583ms):
Trace[1987764628]: ---"Write to database call succeeded" len:81 579ms (05:22:47.048)
Trace[1987764628]: [583.27669ms] [583.27669ms] END
I0329 05:22:47.083285       1 trace.go:236] Trace[176871976]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a2cc8a83-3405-4078-863d-ea1913181da6,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/etcd-minikube/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (29-Mar-2024 05:22:46.553) (total time: 529ms):
Trace[176871976]: ---"About to check admission control" 116ms (05:22:46.670)
Trace[176871976]: ---"Object stored in database" 390ms (05:22:47.061)
Trace[176871976]: ---"Writing http response done" 21ms (05:22:47.083)
Trace[176871976]: [529.301591ms] [529.301591ms] END
I0329 05:22:47.275185       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0329 05:22:47.376905       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0329 05:22:47.783737       1 trace.go:236] Trace[2030006566]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:748f71e2-93b2-41c7-880a-2e4f0ff15667,client:192.168.49.2,protocol:HTTP/2.0,resource:controllerrevisions,scope:resource,url:/apis/apps/v1/namespaces/kube-system/controllerrevisions,user-agent:kube-controller-manager/v1.28.3 (linux/amd64) kubernetes/a8a1abc/system:serviceaccount:kube-system:daemon-set-controller,verb:POST (29-Mar-2024 05:22:47.256) (total time: 527ms):
Trace[2030006566]: ["Create etcd3" audit-id:748f71e2-93b2-41c7-880a-2e4f0ff15667,key:/controllerrevisions/kube-system/kube-proxy-dffc744c9,type:*apps.ControllerRevision,resource:controllerrevisions.apps 500ms (05:22:47.282)
Trace[2030006566]:  ---"Txn call succeeded" 500ms (05:22:47.783)]
Trace[2030006566]: [527.182791ms] [527.182791ms] END
I0329 05:22:49.361525       1 trace.go:236] Trace[1451535004]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (29-Mar-2024 05:22:48.576) (total time: 785ms):
Trace[1451535004]: ---"initial value restored" 201ms (05:22:48.777)
Trace[1451535004]: ---"Transaction prepared" 375ms (05:22:49.152)
Trace[1451535004]: ---"Txn call completed" 208ms (05:22:49.361)
Trace[1451535004]: [785.155986ms] [785.155986ms] END
I0329 05:22:49.476798       1 trace.go:236] Trace[349033034]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:39faaf6a-bb22-4e37-9e6e-c3ac06e56570,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (29-Mar-2024 05:22:48.749) (total time: 726ms):
Trace[349033034]: ---"Write to database call succeeded" len:148 725ms (05:22:49.475)
Trace[349033034]: [726.628387ms] [726.628387ms] END
I0329 05:22:49.852715       1 trace.go:236] Trace[1307418065]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9265c9a8-e197-42e5-a3ff-318640c668d0,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/kube-proxy-2cp4n/status,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (29-Mar-2024 05:22:49.282) (total time: 570ms):
Trace[1307418065]: ---"limitedReadBody succeeded" len:1024 93ms (05:22:49.375)
Trace[1307418065]: ---"About to check admission control" 298ms (05:22:49.741)
Trace[1307418065]: ---"Object stored in database" 91ms (05:22:49.833)
Trace[1307418065]: ---"Writing http response done" 19ms (05:22:49.852)
Trace[1307418065]: [570.33819ms] [570.33819ms] END
I0329 05:36:13.438823       1 trace.go:236] Trace[267769208]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e1eaf0ac-d4d9-4b38-9148-dd2b23a24264,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (29-Mar-2024 05:36:12.114) (total time: 1324ms):
Trace[267769208]: ["GuaranteedUpdate etcd3" audit-id:e1eaf0ac-d4d9-4b38-9148-dd2b23a24264,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 1324ms (05:36:12.114)
Trace[267769208]:  ---"Txn call completed" 1322ms (05:36:13.438)]
Trace[267769208]: [1.324263142s] [1.324263142s] END
I0329 06:44:26.675489       1 trace.go:236] Trace[28713589]: "Update" accept:application/json, */*,audit-id:566e2362-02d7-4ad5-9a10-b49c12dd6d66,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (29-Mar-2024 06:44:25.814) (total time: 860ms):
Trace[28713589]: ["GuaranteedUpdate etcd3" audit-id:566e2362-02d7-4ad5-9a10-b49c12dd6d66,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 860ms (06:44:25.815)
Trace[28713589]:  ---"Txn call completed" 858ms (06:44:26.675)]
Trace[28713589]: [860.92765ms] [860.92765ms] END
I0329 06:44:35.646834       1 trace.go:236] Trace[287761689]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2e0e27be-b8cb-447a-bcb2-55e70afcd349,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (29-Mar-2024 06:44:33.929) (total time: 1717ms):
Trace[287761689]: ["GuaranteedUpdate etcd3" audit-id:2e0e27be-b8cb-447a-bcb2-55e70afcd349,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1716ms (06:44:33.930)
Trace[287761689]:  ---"Txn call completed" 1710ms (06:44:35.642)]
Trace[287761689]: [1.717011074s] [1.717011074s] END
I0329 06:44:35.674388       1 trace.go:236] Trace[1710096071]: "Get" accept:application/json, */*,audit-id:4ef0d136-2bce-4252-bc1f-b26851adfb5c,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (29-Mar-2024 06:44:34.770) (total time: 903ms):
Trace[1710096071]: ---"About to write a response" 903ms (06:44:35.674)
Trace[1710096071]: [903.558639ms] [903.558639ms] END
I0329 06:48:36.804008       1 trace.go:236] Trace[813799949]: "Update" accept:application/json, */*,audit-id:c8313ca3-7c0e-4c71-bcc9-79c02af25ce5,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (29-Mar-2024 06:48:35.866) (total time: 937ms):
Trace[813799949]: ["GuaranteedUpdate etcd3" audit-id:c8313ca3-7c0e-4c71-bcc9-79c02af25ce5,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 937ms (06:48:35.866)
Trace[813799949]:  ---"Txn call completed" 935ms (06:48:36.803)]
Trace[813799949]: [937.547584ms] [937.547584ms] END
I0329 07:06:50.171349       1 trace.go:236] Trace[237760877]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (29-Mar-2024 07:06:49.535) (total time: 635ms):
Trace[237760877]: ---"Transaction prepared" 603ms (07:06:50.141)
Trace[237760877]: [635.922191ms] [635.922191ms] END
I0329 07:11:33.754635       1 alloc.go:330] "allocated clusterIPs" service="default/fastapi-service" clusterIPs={"IPv4":"10.102.198.14"}

* 
* ==> kube-controller-manager [ab42615bd9c5] <==
* I0329 05:22:45.855209       1 shared_informer.go:318] Caches are synced for PV protection
I0329 05:22:45.871553       1 shared_informer.go:318] Caches are synced for ReplicationController
I0329 05:22:45.891273       1 shared_informer.go:318] Caches are synced for job
I0329 05:22:45.891345       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0329 05:22:45.891381       1 shared_informer.go:318] Caches are synced for attach detach
I0329 05:22:45.944156       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0329 05:22:45.891415       1 shared_informer.go:318] Caches are synced for daemon sets
I0329 05:22:45.892681       1 shared_informer.go:318] Caches are synced for HPA
I0329 05:22:45.892763       1 shared_informer.go:318] Caches are synced for endpoint
I0329 05:22:45.892816       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0329 05:22:45.892936       1 shared_informer.go:318] Caches are synced for disruption
I0329 05:22:45.893022       1 shared_informer.go:318] Caches are synced for persistent volume
I0329 05:22:45.930531       1 shared_informer.go:318] Caches are synced for GC
I0329 05:22:45.930622       1 shared_informer.go:318] Caches are synced for ephemeral
I0329 05:22:45.940274       1 shared_informer.go:318] Caches are synced for resource quota
I0329 05:22:45.958769       1 shared_informer.go:318] Caches are synced for PVC protection
I0329 05:22:45.958977       1 shared_informer.go:318] Caches are synced for taint
I0329 05:22:45.959457       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0329 05:22:45.960837       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0329 05:22:45.961183       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0329 05:22:45.961469       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0329 05:22:45.961995       1 taint_manager.go:211] "Sending events to api server"
I0329 05:22:45.962176       1 shared_informer.go:318] Caches are synced for resource quota
I0329 05:22:46.034971       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0329 05:22:46.274373       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0329 05:22:46.741637       1 shared_informer.go:318] Caches are synced for garbage collector
I0329 05:22:46.760852       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0329 05:22:46.775265       1 shared_informer.go:318] Caches are synced for garbage collector
I0329 05:22:47.653806       1 event.go:307] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-5dd5756b68 to 1"
I0329 05:22:47.991456       1 event.go:307] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-2cp4n"
I0329 05:22:48.431860       1 event.go:307] "Event occurred" object="kube-system/coredns-5dd5756b68" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-5dd5756b68-r7x2l"
I0329 05:22:48.879060       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="1.13614228s"
I0329 05:22:49.471260       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="591.913689ms"
I0329 05:22:49.471985       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="445µs"
I0329 05:22:50.263423       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="964µs"
I0329 05:23:06.998491       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="2.304301ms"
I0329 05:23:40.555672       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="95.229571ms"
I0329 05:23:40.572645       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="3.101899ms"
I0329 07:11:33.798066       1 event.go:307] "Event occurred" object="default/fastapi-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set fastapi-deployment-588df47757 to 1"
I0329 07:11:34.252933       1 event.go:307] "Event occurred" object="default/fastapi-deployment-588df47757" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fastapi-deployment-588df47757-r8mvl"
I0329 07:11:34.440337       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="640.785729ms"
I0329 07:11:34.563819       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="123.006513ms"
I0329 07:11:34.564240       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="213.1µs"
I0329 07:11:34.564653       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="224.7µs"
I0329 07:11:34.751585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="202µs"
I0329 07:11:41.803658       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="151µs"
I0329 07:11:57.014402       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="92.6µs"
I0329 07:12:15.149292       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="196.3µs"
I0329 07:12:30.003943       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="60µs"
I0329 07:12:49.100788       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="1.7423ms"
I0329 07:13:01.355397       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="401.1µs"
I0329 07:13:32.080935       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="2.6729ms"
I0329 07:13:47.168949       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="396.8µs"
I0329 07:14:57.045987       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="190.8µs"
I0329 07:15:08.187065       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="701.499µs"
I0329 07:17:49.165046       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="237µs"
I0329 07:18:04.073491       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="468.3µs"
I0329 07:22:42.743532       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-xzd68" approvedExpiration="1h0m0s"
I0329 07:23:04.135260       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="294.3µs"
I0329 07:23:15.044817       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fastapi-deployment-588df47757" duration="547.1µs"

* 
* ==> kube-proxy [dcbe0483c3fb] <==
* I0329 05:23:06.297955       1 server_others.go:69] "Using iptables proxy"
I0329 05:23:07.173084       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0329 05:23:07.529858       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0329 05:23:07.538716       1 server_others.go:152] "Using iptables Proxier"
I0329 05:23:07.538868       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0329 05:23:07.538911       1 server_others.go:438] "Defaulting to no-op detect-local"
I0329 05:23:07.541918       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0329 05:23:07.543137       1 server.go:846] "Version info" version="v1.28.3"
I0329 05:23:07.543186       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 05:23:07.555762       1 config.go:188] "Starting service config controller"
I0329 05:23:07.557908       1 shared_informer.go:311] Waiting for caches to sync for service config
I0329 05:23:07.558059       1 config.go:97] "Starting endpoint slice config controller"
I0329 05:23:07.558089       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0329 05:23:07.563327       1 config.go:315] "Starting node config controller"
I0329 05:23:07.563369       1 shared_informer.go:311] Waiting for caches to sync for node config
I0329 05:23:07.658806       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0329 05:23:07.658984       1 shared_informer.go:318] Caches are synced for service config
I0329 05:23:07.664160       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [b984a30c8107] <==
* E0329 05:22:24.268325       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0329 05:22:24.282033       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0329 05:22:24.282251       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0329 05:22:24.351798       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0329 05:22:24.353315       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0329 05:22:24.390366       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0329 05:22:24.390552       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0329 05:22:24.433943       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0329 05:22:24.434172       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0329 05:22:24.454928       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:24.455139       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:24.640089       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0329 05:22:24.642412       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0329 05:22:24.642559       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0329 05:22:24.642462       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0329 05:22:24.651660       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0329 05:22:24.651879       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0329 05:22:24.754416       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:24.755391       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:24.771140       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0329 05:22:24.771356       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0329 05:22:24.931867       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0329 05:22:24.932164       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0329 05:22:24.954621       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:24.956327       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:25.051072       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0329 05:22:25.054510       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0329 05:22:25.074872       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:25.078039       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:25.955150       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0329 05:22:25.955513       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0329 05:22:26.296229       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0329 05:22:26.298232       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0329 05:22:26.348559       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0329 05:22:26.350284       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0329 05:22:26.444462       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0329 05:22:26.444888       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0329 05:22:26.534397       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0329 05:22:26.534574       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0329 05:22:26.590039       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:26.590341       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:26.682110       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0329 05:22:26.682440       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0329 05:22:26.842801       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:26.843524       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:26.977281       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0329 05:22:26.986215       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0329 05:22:27.016325       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0329 05:22:27.016890       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0329 05:22:27.175621       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0329 05:22:27.175869       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0329 05:22:27.212421       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0329 05:22:27.212542       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0329 05:22:27.341938       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:27.343484       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:27.367052       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0329 05:22:27.367425       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0329 05:22:27.627848       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0329 05:22:27.628588       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
I0329 05:22:31.638616       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Mar 29 07:13:19 minikube kubelet[2613]: E0329 07:13:19.627730    2613 kuberuntime_manager.go:1256] container &Container{Name:fastapi-container,Image:my-fastapi-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r26d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fastapi-deployment-588df47757-r8mvl_default(82b934ea-3a26-45fa-92aa-71bb782d419d): ErrImagePull: Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Mar 29 07:13:19 minikube kubelet[2613]: E0329 07:13:19.628086    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ErrImagePull: \"Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:13:32 minikube kubelet[2613]: E0329 07:13:31.982855    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:13:46 minikube kubelet[2613]: E0329 07:13:46.971561    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:13:58 minikube kubelet[2613]: E0329 07:13:58.958698    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:14:13 minikube kubelet[2613]: E0329 07:14:13.968740    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:14:27 minikube kubelet[2613]: E0329 07:14:27.968675    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:14:43 minikube kubelet[2613]: E0329 07:14:43.920992    2613 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-fastapi-app:latest"
Mar 29 07:14:43 minikube kubelet[2613]: E0329 07:14:43.921445    2613 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-fastapi-app:latest"
Mar 29 07:14:43 minikube kubelet[2613]: E0329 07:14:43.922586    2613 kuberuntime_manager.go:1256] container &Container{Name:fastapi-container,Image:my-fastapi-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r26d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fastapi-deployment-588df47757-r8mvl_default(82b934ea-3a26-45fa-92aa-71bb782d419d): ErrImagePull: Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Mar 29 07:14:43 minikube kubelet[2613]: E0329 07:14:43.923121    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ErrImagePull: \"Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:14:56 minikube kubelet[2613]: E0329 07:14:56.995557    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:15:07 minikube kubelet[2613]: E0329 07:15:07.984483    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:15:19 minikube kubelet[2613]: E0329 07:15:19.968652    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:15:31 minikube kubelet[2613]: E0329 07:15:31.987010    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:15:45 minikube kubelet[2613]: E0329 07:15:45.981892    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:15:56 minikube kubelet[2613]: E0329 07:15:56.961520    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:16:12 minikube kubelet[2613]: E0329 07:16:12.031199    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:16:26 minikube kubelet[2613]: E0329 07:16:26.973691    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:16:41 minikube kubelet[2613]: E0329 07:16:41.965752    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:16:54 minikube kubelet[2613]: E0329 07:16:54.965588    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:17:08 minikube kubelet[2613]: E0329 07:17:08.960103    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:17:22 minikube kubelet[2613]: E0329 07:17:22.968007    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:17:35 minikube kubelet[2613]: W0329 07:17:35.534745    2613 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 29 07:17:36 minikube kubelet[2613]: E0329 07:17:36.615128    2613 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-fastapi-app:latest"
Mar 29 07:17:36 minikube kubelet[2613]: E0329 07:17:36.615348    2613 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-fastapi-app:latest"
Mar 29 07:17:36 minikube kubelet[2613]: E0329 07:17:36.615811    2613 kuberuntime_manager.go:1256] container &Container{Name:fastapi-container,Image:my-fastapi-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r26d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fastapi-deployment-588df47757-r8mvl_default(82b934ea-3a26-45fa-92aa-71bb782d419d): ErrImagePull: Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Mar 29 07:17:36 minikube kubelet[2613]: E0329 07:17:36.616126    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ErrImagePull: \"Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:17:49 minikube kubelet[2613]: E0329 07:17:49.032178    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:18:03 minikube kubelet[2613]: E0329 07:18:03.980906    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:18:14 minikube kubelet[2613]: E0329 07:18:14.978811    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:18:28 minikube kubelet[2613]: E0329 07:18:28.954664    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:18:39 minikube kubelet[2613]: E0329 07:18:39.964817    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:18:54 minikube kubelet[2613]: E0329 07:18:54.979137    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:19:08 minikube kubelet[2613]: E0329 07:19:08.963840    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:19:22 minikube kubelet[2613]: E0329 07:19:22.954698    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:19:36 minikube kubelet[2613]: E0329 07:19:36.954522    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:19:48 minikube kubelet[2613]: E0329 07:19:48.977699    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:20:02 minikube kubelet[2613]: E0329 07:20:02.984563    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:20:14 minikube kubelet[2613]: E0329 07:20:14.002432    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:20:26 minikube kubelet[2613]: E0329 07:20:26.969286    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:20:35 minikube kubelet[2613]: E0329 07:20:35.036522    2613 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.08s"
Mar 29 07:20:38 minikube kubelet[2613]: E0329 07:20:38.958803    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:20:53 minikube kubelet[2613]: E0329 07:20:53.031708    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:21:08 minikube kubelet[2613]: E0329 07:21:08.069822    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:21:20 minikube kubelet[2613]: E0329 07:21:20.967351    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:21:34 minikube kubelet[2613]: E0329 07:21:34.963474    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:21:49 minikube kubelet[2613]: E0329 07:21:49.961853    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:22:03 minikube kubelet[2613]: E0329 07:22:03.072222    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:22:16 minikube kubelet[2613]: E0329 07:22:16.955949    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:22:31 minikube kubelet[2613]: E0329 07:22:31.001959    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:22:35 minikube kubelet[2613]: W0329 07:22:35.558970    2613 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 29 07:22:49 minikube kubelet[2613]: E0329 07:22:49.778842    2613 remote_image.go:180] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-fastapi-app:latest"
Mar 29 07:22:49 minikube kubelet[2613]: E0329 07:22:49.780241    2613 kuberuntime_image.go:53] "Failed to pull image" err="Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="my-fastapi-app:latest"
Mar 29 07:22:49 minikube kubelet[2613]: E0329 07:22:49.781058    2613 kuberuntime_manager.go:1256] container &Container{Name:fastapi-container,Image:my-fastapi-app,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7r26d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fastapi-deployment-588df47757-r8mvl_default(82b934ea-3a26-45fa-92aa-71bb782d419d): ErrImagePull: Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Mar 29 07:22:49 minikube kubelet[2613]: E0329 07:22:49.788428    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ErrImagePull: \"Error response from daemon: pull access denied for my-fastapi-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:23:03 minikube kubelet[2613]: E0329 07:23:03.994261    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:23:14 minikube kubelet[2613]: E0329 07:23:14.976947    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:23:25 minikube kubelet[2613]: E0329 07:23:25.970518    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"
Mar 29 07:23:36 minikube kubelet[2613]: E0329 07:23:36.957152    2613 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fastapi-container\" with ImagePullBackOff: \"Back-off pulling image \\\"my-fastapi-app\\\"\"" pod="default/fastapi-deployment-588df47757-r8mvl" podUID="82b934ea-3a26-45fa-92aa-71bb782d419d"

* 
* ==> storage-provisioner [1f8f6873d7e4] <==
* I0329 05:23:08.689213       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0329 05:23:08.743219       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0329 05:23:08.744217       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0329 05:23:08.769373       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0329 05:23:08.771628       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e3f83578-b340-4432-b14f-74833c2215da", APIVersion:"v1", ResourceVersion:"416", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_fa1123c4-912f-4a13-82e7-afa54fb645a8 became leader
I0329 05:23:08.772114       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_fa1123c4-912f-4a13-82e7-afa54fb645a8!
I0329 05:23:08.878964       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_fa1123c4-912f-4a13-82e7-afa54fb645a8!

