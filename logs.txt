* 
* ==> Audit <==
* |------------|-----------------------|----------|------|---------|---------------------|---------------------|
|  Command   |         Args          | Profile  | User | Version |     Start Time      |      End Time       |
|------------|-----------------------|----------|------|---------|---------------------|---------------------|
| start      |                       | minikube | lee  | v1.32.0 | 29 Mar 24 14:17 KST | 29 Mar 24 14:23 KST |
| service    | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:23 KST |                     |
| service    | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:34 KST |                     |
| service    | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:34 KST |                     |
| service    | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:39 KST |                     |
| start      | --driver=docker       | minikube | lee  | v1.32.0 | 29 Mar 24 16:47 KST | 29 Mar 24 16:50 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 29 Mar 24 17:14 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 29 Mar 24 17:21 KST | 29 Mar 24 17:26 KST |
| tunnel     |                       | minikube | lee  | v1.32.0 | 01 Apr 24 10:51 KST |                     |
| start      |                       | minikube | lee  | v1.32.0 | 01 Apr 24 10:52 KST | 01 Apr 24 10:52 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:11 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:11 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:12 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:13 KST |                     |
| start      | fast-api              | minikube | lee  | v1.32.0 | 01 Apr 24 15:14 KST |                     |
| start      | --driver=docker       | minikube | lee  | v1.32.0 | 01 Apr 24 15:14 KST | 01 Apr 24 15:17 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:18 KST |                     |
| service    | list                  | minikube | lee  | v1.32.0 | 01 Apr 24 15:20 KST | 01 Apr 24 15:20 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:25 KST |                     |
| service    | fast-api              | minikube | lee  | v1.32.0 | 01 Apr 24 15:28 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:28 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:35 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:35 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:36 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:37 KST | 01 Apr 24 15:38 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 15:42 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 16:31 KST | 01 Apr 24 16:35 KST |
| docker-env |                       | minikube | lee  | v1.32.0 | 01 Apr 24 16:49 KST | 01 Apr 24 16:49 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 16:52 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 16:53 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 16:54 KST |                     |
| docker-env | minikube docker-env   | minikube | lee  | v1.32.0 | 01 Apr 24 17:13 KST | 01 Apr 24 17:13 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:14 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:15 KST |                     |
| docker-env |                       | minikube | lee  | v1.32.0 | 01 Apr 24 17:17 KST | 01 Apr 24 17:17 KST |
| docker-env | minikube docker-env   | minikube | lee  | v1.32.0 | 01 Apr 24 17:17 KST | 01 Apr 24 17:17 KST |
| docker-env |                       | minikube | lee  | v1.32.0 | 01 Apr 24 17:20 KST | 01 Apr 24 17:20 KST |
| docker-env | minikube docker-env   | minikube | lee  | v1.32.0 | 01 Apr 24 17:20 KST | 01 Apr 24 17:20 KST |
| image      | ls --format table     | minikube | lee  | v1.32.0 | 01 Apr 24 17:21 KST | 01 Apr 24 17:21 KST |
| image      | ls --format table     | minikube | lee  | v1.32.0 | 01 Apr 24 17:34 KST | 01 Apr 24 17:34 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:36 KST |                     |
| image      | ls --format table     | minikube | lee  | v1.32.0 | 01 Apr 24 17:38 KST | 01 Apr 24 17:38 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:38 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:39 KST |                     |
| image      | ls --format table     | minikube | lee  | v1.32.0 | 01 Apr 24 17:40 KST | 01 Apr 24 17:40 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:41 KST | 01 Apr 24 17:41 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:42 KST |                     |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:42 KST |                     |
| image      | ls --format table     | minikube | lee  | v1.32.0 | 01 Apr 24 17:55 KST | 01 Apr 24 17:55 KST |
| service    | fast-api-service      | minikube | lee  | v1.32.0 | 01 Apr 24 17:56 KST |                     |
|------------|-----------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/04/01 15:14:35
Running on machine: DESKTOP-1T299N9
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0401 15:14:35.111800  229559 out.go:296] Setting OutFile to fd 1 ...
I0401 15:14:35.113181  229559 out.go:348] isatty.IsTerminal(1) = true
I0401 15:14:35.113200  229559 out.go:309] Setting ErrFile to fd 2...
I0401 15:14:35.113229  229559 out.go:348] isatty.IsTerminal(2) = true
I0401 15:14:35.114265  229559 root.go:338] Updating PATH: /home/lee/.minikube/bin
W0401 15:14:35.114937  229559 root.go:314] Error reading config file at /home/lee/.minikube/config/config.json: open /home/lee/.minikube/config/config.json: no such file or directory
I0401 15:14:35.116602  229559 out.go:303] Setting JSON to false
I0401 15:14:35.121358  229559 start.go:128] hostinfo: {"hostname":"DESKTOP-1T299N9","uptime":13587,"bootTime":1711938488,"procs":51,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"089e2fbe-8854-471b-a5fd-c5dea93fbac9"}
I0401 15:14:35.121685  229559 start.go:138] virtualization:  guest
I0401 15:14:35.138579  229559 out.go:177] 😄  minikube v1.32.0 on Ubuntu 22.04 (amd64)
I0401 15:14:35.154669  229559 notify.go:220] Checking for updates...
I0401 15:14:35.160894  229559 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0401 15:14:35.172351  229559 driver.go:378] Setting default libvirt URI to qemu:///system
I0401 15:14:35.380146  229559 docker.go:122] docker version: linux-25.0.4:Docker Engine - Community
I0401 15:14:35.380451  229559 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0401 15:14:35.619704  229559 info.go:266] docker info: {ID:e3db2e71-b1ea-4230-a673-d2dbcdaa6727 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:31 OomKillDisable:true NGoroutines:55 SystemTime:2024-04-01 15:14:35.551930471 +0900 KST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4035866624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-1T299N9 Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0401 15:14:35.619969  229559 docker.go:295] overlay module found
I0401 15:14:35.629430  229559 out.go:177] ✨  Using the docker driver based on existing profile
I0401 15:14:35.651563  229559 start.go:298] selected driver: docker
I0401 15:14:35.651601  229559 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0401 15:14:35.651765  229559 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0401 15:14:35.651982  229559 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0401 15:14:35.861792  229559 info.go:266] docker info: {ID:e3db2e71-b1ea-4230-a673-d2dbcdaa6727 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:31 OomKillDisable:true NGoroutines:55 SystemTime:2024-04-01 15:14:35.829248038 +0900 KST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4035866624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-1T299N9 Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0401 15:14:35.863072  229559 cni.go:84] Creating CNI manager for ""
I0401 15:14:35.863102  229559 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0401 15:14:35.863117  229559 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0401 15:14:35.868812  229559 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I0401 15:14:35.879490  229559 cache.go:121] Beginning downloading kic base image for docker with docker
I0401 15:14:35.890026  229559 out.go:177] 🚜  Pulling base image ...
I0401 15:14:35.901649  229559 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0401 15:14:35.901755  229559 preload.go:148] Found local preload: /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0401 15:14:35.901804  229559 cache.go:56] Caching tarball of preloaded images
I0401 15:14:35.901945  229559 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0401 15:14:35.902069  229559 preload.go:174] Found /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0401 15:14:35.902102  229559 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0401 15:14:35.902332  229559 profile.go:148] Saving config to /home/lee/.minikube/profiles/minikube/config.json ...
I0401 15:14:35.957187  229559 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0401 15:14:35.957212  229559 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0401 15:14:35.957233  229559 cache.go:194] Successfully downloaded all kic artifacts
I0401 15:14:35.957276  229559 start.go:365] acquiring machines lock for minikube: {Name:mk89aad7abaa74b5ceac4179cf674336d941d2b1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0401 15:14:35.957372  229559 start.go:369] acquired machines lock for "minikube" in 64.9µs
I0401 15:14:35.957408  229559 start.go:96] Skipping create...Using existing machine configuration
I0401 15:14:35.957416  229559 fix.go:54] fixHost starting: 
I0401 15:14:35.957963  229559 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0401 15:14:36.006329  229559 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0401 15:14:36.006393  229559 fix.go:128] unexpected machine state, will restart: <nil>
I0401 15:14:36.012145  229559 out.go:177] 🏃  Updating the running docker "minikube" container ...
I0401 15:14:36.024616  229559 machine.go:88] provisioning docker machine ...
I0401 15:14:36.024651  229559 ubuntu.go:169] provisioning hostname "minikube"
I0401 15:14:36.024746  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:36.083402  229559 main.go:141] libmachine: Using SSH client type: native
I0401 15:14:36.084296  229559 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0401 15:14:36.084342  229559 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0401 15:14:37.269287  229559 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0401 15:14:37.269679  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:37.445929  229559 main.go:141] libmachine: Using SSH client type: native
I0401 15:14:37.447052  229559 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0401 15:14:37.447121  229559 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0401 15:14:39.438620  229559 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0401 15:14:39.439316  229559 ubuntu.go:175] set auth options {CertDir:/home/lee/.minikube CaCertPath:/home/lee/.minikube/certs/ca.pem CaPrivateKeyPath:/home/lee/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/lee/.minikube/machines/server.pem ServerKeyPath:/home/lee/.minikube/machines/server-key.pem ClientKeyPath:/home/lee/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/lee/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/lee/.minikube}
I0401 15:14:39.439425  229559 ubuntu.go:177] setting up certificates
I0401 15:14:39.439476  229559 provision.go:83] configureAuth start
I0401 15:14:39.439868  229559 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0401 15:14:39.583317  229559 provision.go:138] copyHostCerts
I0401 15:14:39.583528  229559 exec_runner.go:144] found /home/lee/.minikube/ca.pem, removing ...
I0401 15:14:39.583548  229559 exec_runner.go:203] rm: /home/lee/.minikube/ca.pem
I0401 15:14:39.583738  229559 exec_runner.go:151] cp: /home/lee/.minikube/certs/ca.pem --> /home/lee/.minikube/ca.pem (1070 bytes)
I0401 15:14:39.584577  229559 exec_runner.go:144] found /home/lee/.minikube/cert.pem, removing ...
I0401 15:14:39.584597  229559 exec_runner.go:203] rm: /home/lee/.minikube/cert.pem
I0401 15:14:39.584725  229559 exec_runner.go:151] cp: /home/lee/.minikube/certs/cert.pem --> /home/lee/.minikube/cert.pem (1111 bytes)
I0401 15:14:39.585141  229559 exec_runner.go:144] found /home/lee/.minikube/key.pem, removing ...
I0401 15:14:39.585159  229559 exec_runner.go:203] rm: /home/lee/.minikube/key.pem
I0401 15:14:39.585687  229559 exec_runner.go:151] cp: /home/lee/.minikube/certs/key.pem --> /home/lee/.minikube/key.pem (1679 bytes)
I0401 15:14:39.586028  229559 provision.go:112] generating server cert: /home/lee/.minikube/machines/server.pem ca-key=/home/lee/.minikube/certs/ca.pem private-key=/home/lee/.minikube/certs/ca-key.pem org=lee.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0401 15:14:40.287401  229559 provision.go:172] copyRemoteCerts
I0401 15:14:40.287546  229559 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0401 15:14:40.287694  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:40.328311  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:14:40.859862  229559 ssh_runner.go:362] scp /home/lee/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0401 15:14:41.764208  229559 ssh_runner.go:362] scp /home/lee/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0401 15:14:43.137485  229559 ssh_runner.go:362] scp /home/lee/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0401 15:14:45.253831  229559 provision.go:86] duration metric: configureAuth took 5.814291445s
I0401 15:14:45.254101  229559 ubuntu.go:193] setting minikube options for container-runtime
I0401 15:14:45.255393  229559 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0401 15:14:45.255788  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:45.452343  229559 main.go:141] libmachine: Using SSH client type: native
I0401 15:14:45.459215  229559 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0401 15:14:45.459333  229559 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0401 15:14:47.027815  229559 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0401 15:14:47.027903  229559 ubuntu.go:71] root file system type: overlay
I0401 15:14:47.028615  229559 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0401 15:14:47.028961  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:47.194336  229559 main.go:141] libmachine: Using SSH client type: native
I0401 15:14:47.195226  229559 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0401 15:14:47.195364  229559 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0401 15:14:51.135476  229559 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0401 15:14:51.135955  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:51.298465  229559 main.go:141] libmachine: Using SSH client type: native
I0401 15:14:51.299788  229559 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0401 15:14:51.299833  229559 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0401 15:14:55.677844  229559 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0401 15:14:55.677907  229559 machine.go:91] provisioned docker machine in 19.653258015s
I0401 15:14:55.677947  229559 start.go:300] post-start starting for "minikube" (driver="docker")
I0401 15:14:55.677999  229559 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0401 15:14:55.678589  229559 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0401 15:14:55.678852  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:55.799711  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:14:58.161415  229559 ssh_runner.go:235] Completed: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs: (2.482665194s)
I0401 15:14:58.161874  229559 ssh_runner.go:195] Run: cat /etc/os-release
I0401 15:14:58.421737  229559 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0401 15:14:58.421948  229559 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0401 15:14:58.422028  229559 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0401 15:14:58.422066  229559 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0401 15:14:58.422117  229559 filesync.go:126] Scanning /home/lee/.minikube/addons for local assets ...
I0401 15:14:58.422421  229559 filesync.go:126] Scanning /home/lee/.minikube/files for local assets ...
I0401 15:14:58.422588  229559 start.go:303] post-start completed in 2.744610783s
I0401 15:14:58.422863  229559 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0401 15:14:58.423109  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:14:58.593623  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:15:00.448244  229559 ssh_runner.go:235] Completed: sh -c "df -h /var | awk 'NR==2{print $5}'": (2.025283219s)
I0401 15:15:00.448568  229559 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0401 15:15:00.761826  229559 fix.go:56] fixHost completed within 24.804355122s
I0401 15:15:00.761898  229559 start.go:83] releasing machines lock for "minikube", held for 24.804494123s
I0401 15:15:00.762328  229559 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0401 15:15:00.899629  229559 ssh_runner.go:195] Run: cat /version.json
I0401 15:15:00.899805  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:15:00.900350  229559 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0401 15:15:00.900534  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:15:00.970906  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:15:00.988663  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:15:04.562763  229559 ssh_runner.go:235] Completed: cat /version.json: (3.663047326s)
I0401 15:15:04.562979  229559 ssh_runner.go:195] Run: systemctl --version
I0401 15:15:04.563284  229559 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (3.662891126s)
W0401 15:15:04.563323  229559 start.go:840] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2000 milliseconds with 0 bytes received
W0401 15:15:04.563405  229559 out.go:239] ❗  This container is having trouble accessing https://registry.k8s.io
W0401 15:15:04.563443  229559 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0401 15:15:04.725638  229559 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0401 15:15:04.935940  229559 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0401 15:15:05.555024  229559 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0401 15:15:05.555166  229559 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0401 15:15:05.937149  229559 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0401 15:15:05.937270  229559 start.go:472] detecting cgroup driver to use...
I0401 15:15:05.937328  229559 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0401 15:15:05.937527  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0401 15:15:06.541375  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0401 15:15:06.841901  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0401 15:15:06.958331  229559 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0401 15:15:06.958446  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0401 15:15:07.337866  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0401 15:15:07.623282  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0401 15:15:07.766564  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0401 15:15:07.923675  229559 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0401 15:15:08.238071  229559 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0401 15:15:08.338241  229559 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0401 15:15:08.439232  229559 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0401 15:15:08.540250  229559 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0401 15:15:09.626699  229559 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.086414187s)
I0401 15:15:09.626782  229559 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0401 15:15:22.616307  229559 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (12.989485668s)
I0401 15:15:22.616335  229559 start.go:472] detecting cgroup driver to use...
I0401 15:15:22.616384  229559 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0401 15:15:22.621155  229559 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0401 15:15:22.671273  229559 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0401 15:15:22.671388  229559 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0401 15:15:22.755925  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0401 15:15:23.056555  229559 ssh_runner.go:195] Run: which cri-dockerd
I0401 15:15:23.135871  229559 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0401 15:15:23.195120  229559 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0401 15:15:23.291419  229559 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0401 15:15:23.799953  229559 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0401 15:15:24.059444  229559 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0401 15:15:24.059595  229559 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0401 15:15:24.102053  229559 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0401 15:15:24.507581  229559 ssh_runner.go:195] Run: sudo systemctl restart docker
I0401 15:15:28.024951  229559 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.517296961s)
I0401 15:15:28.025154  229559 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0401 15:15:30.066271  229559 ssh_runner.go:235] Completed: sudo systemctl enable cri-docker.socket: (2.040995726s)
I0401 15:15:30.066643  229559 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0401 15:15:31.302312  229559 ssh_runner.go:235] Completed: sudo systemctl unmask cri-docker.socket: (1.235598766s)
I0401 15:15:31.302449  229559 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0401 15:15:32.932958  229559 ssh_runner.go:235] Completed: sudo systemctl enable cri-docker.socket: (1.630279856s)
I0401 15:15:32.933248  229559 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0401 15:15:35.231814  229559 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (2.298461137s)
I0401 15:15:35.232118  229559 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0401 15:15:35.844726  229559 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0401 15:15:37.770509  229559 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.925696048s)
I0401 15:15:37.770640  229559 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0401 15:15:38.387422  229559 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0401 15:15:38.387526  229559 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0401 15:15:38.398466  229559 start.go:540] Will wait 60s for crictl version
I0401 15:15:38.398574  229559 ssh_runner.go:195] Run: which crictl
I0401 15:15:38.408233  229559 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0401 15:15:38.529444  229559 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0401 15:15:38.529543  229559 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0401 15:15:38.585185  229559 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0401 15:15:38.656171  229559 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0401 15:15:38.658652  229559 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0401 15:15:38.701585  229559 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0401 15:15:38.712630  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0401 15:15:38.752422  229559 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0401 15:15:38.752548  229559 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0401 15:15:38.801357  229559 docker.go:671] Got preloaded images: -- stdout --
jaruda/fast-api:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0401 15:15:38.801738  229559 docker.go:601] Images already preloaded, skipping extraction
I0401 15:15:38.802132  229559 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0401 15:15:38.849277  229559 docker.go:671] Got preloaded images: -- stdout --
jaruda/fast-api:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0401 15:15:38.849456  229559 cache_images.go:84] Images are preloaded, skipping loading
I0401 15:15:38.850209  229559 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0401 15:15:43.168151  229559 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (4.317882726s)
I0401 15:15:43.168971  229559 cni.go:84] Creating CNI manager for ""
I0401 15:15:43.169010  229559 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0401 15:15:43.169437  229559 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0401 15:15:43.170076  229559 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0401 15:15:43.170363  229559 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0401 15:15:43.170481  229559 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0401 15:15:43.170915  229559 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0401 15:15:43.323278  229559 binaries.go:44] Found k8s binaries, skipping transfer
I0401 15:15:43.323396  229559 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0401 15:15:43.521818  229559 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0401 15:15:43.751304  229559 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0401 15:15:44.044214  229559 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0401 15:15:44.433092  229559 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0401 15:15:44.523514  229559 certs.go:56] Setting up /home/lee/.minikube/profiles/minikube for IP: 192.168.49.2
I0401 15:15:44.523638  229559 certs.go:190] acquiring lock for shared ca certs: {Name:mk1cf981783681b0bb5a6ac43e6863efc3a4fc0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0401 15:15:44.524308  229559 certs.go:199] skipping minikubeCA CA generation: /home/lee/.minikube/ca.key
I0401 15:15:44.528004  229559 certs.go:199] skipping proxyClientCA CA generation: /home/lee/.minikube/proxy-client-ca.key
I0401 15:15:44.531232  229559 certs.go:315] skipping minikube-user signed cert generation: /home/lee/.minikube/profiles/minikube/client.key
I0401 15:15:44.535184  229559 certs.go:315] skipping minikube signed cert generation: /home/lee/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0401 15:15:44.537219  229559 certs.go:315] skipping aggregator signed cert generation: /home/lee/.minikube/profiles/minikube/proxy-client.key
I0401 15:15:44.538503  229559 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/ca-key.pem (1679 bytes)
I0401 15:15:44.538742  229559 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/ca.pem (1070 bytes)
I0401 15:15:44.538940  229559 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/cert.pem (1111 bytes)
I0401 15:15:44.539581  229559 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/key.pem (1679 bytes)
I0401 15:15:44.552059  229559 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0401 15:15:45.255515  229559 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0401 15:15:46.224139  229559 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0401 15:15:47.953238  229559 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0401 15:15:49.038098  229559 ssh_runner.go:362] scp /home/lee/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0401 15:15:51.274830  229559 ssh_runner.go:362] scp /home/lee/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0401 15:15:52.257350  229559 ssh_runner.go:362] scp /home/lee/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0401 15:15:54.443901  229559 ssh_runner.go:362] scp /home/lee/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0401 15:15:57.155342  229559 ssh_runner.go:362] scp /home/lee/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0401 15:15:58.622928  229559 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0401 15:15:59.959632  229559 ssh_runner.go:195] Run: openssl version
I0401 15:16:00.444730  229559 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0401 15:16:01.759752  229559 ssh_runner.go:235] Completed: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem": (1.31489779s)
I0401 15:16:01.760060  229559 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0401 15:16:02.247445  229559 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar 29 05:21 /usr/share/ca-certificates/minikubeCA.pem
I0401 15:16:02.247751  229559 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0401 15:16:02.740635  229559 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0401 15:16:03.659937  229559 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0401 15:16:04.019003  229559 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0401 15:16:04.930211  229559 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0401 15:16:05.244468  229559 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0401 15:16:05.330574  229559 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0401 15:16:05.561574  229559 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0401 15:16:05.731866  229559 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0401 15:16:05.963862  229559 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0401 15:16:05.964454  229559 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0401 15:16:06.654306  229559 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0401 15:16:06.819821  229559 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0401 15:16:06.821035  229559 kubeadm.go:636] restartCluster start
I0401 15:16:06.821761  229559 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0401 15:16:06.932555  229559 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0401 15:16:06.932684  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0401 15:16:06.985102  229559 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:32769"
I0401 15:16:06.988961  229559 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0401 15:16:07.420161  229559 api_server.go:166] Checking apiserver status ...
I0401 15:16:07.420584  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:07.719330  229559 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/175878/cgroup
I0401 15:16:07.933192  229559 api_server.go:182] apiserver freezer: "7:freezer:/docker/9b2fc22828ae7ee0611287afdf22ba2bb5d769be7dd365d2da6a8ce714349a0e/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/9ee4f6b9ca048208d80bbf94872b2845a4128aaf7850c1a2eadff7d3e1f210c8"
I0401 15:16:07.933547  229559 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/docker/9b2fc22828ae7ee0611287afdf22ba2bb5d769be7dd365d2da6a8ce714349a0e/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/9ee4f6b9ca048208d80bbf94872b2845a4128aaf7850c1a2eadff7d3e1f210c8/freezer.state
I0401 15:16:08.228777  229559 api_server.go:204] freezer state: "THAWED"
I0401 15:16:08.228866  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:13.230436  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0401 15:16:13.230846  229559 retry.go:31] will retry after 294.067297ms: state is "Stopped"
I0401 15:16:13.526011  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:16.144368  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:16.144478  229559 retry.go:31] will retry after 339.161241ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:16.484710  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:16.945385  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:16.945488  229559 retry.go:31] will retry after 401.347661ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:17.348131  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:17.529166  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:17.529270  229559 retry.go:31] will retry after 469.290833ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:18.000110  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:18.231407  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:18.231524  229559 retry.go:31] will retry after 565.867684ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:18.798513  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:18.932757  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:18.932881  229559 retry.go:31] will retry after 891.983696ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:19.826312  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:20.020226  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:20.020341  229559 retry.go:31] will retry after 940.352897ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:20.964523  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:21.077092  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:21.077202  229559 retry.go:31] will retry after 1.005513699s: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:22.083771  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:22.221206  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:22.221326  229559 kubeadm.go:611] needs reconfigure: apiserver error: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:16:22.222318  229559 kubeadm.go:1128] stopping kube-system containers ...
I0401 15:16:22.224764  229559 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0401 15:16:23.235444  229559 ssh_runner.go:235] Completed: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}: (1.010546506s)
I0401 15:16:23.235837  229559 docker.go:469] Stopping containers: [e7679c81496e 9206101402c8 1f3fa48a9c3a 9ee4f6b9ca04 69630a57be5c 871ba57330e5 2c5289d83a55 76ba9fa06c6b f6c37846ea32 ef8c2287af63 0a711f0b1ec9 da828d37c019 d4eae3abcde0 be43d79e6f83 2649effe3d24 350fd62815a0 cd42b9c0add4 21b09f332de5 42c423188178 d3bb1a2290f9 52219f1d7eb6 8aea3bcd29b1 893f3001eeee 5cffff3e86ea 1e59a71d029f 2bcd3bc2051d 68b3e836a9ff 2d99af2d5255 c705e7544641 d1f089dbf726 22988f993d68 482724ff1768 667e9b337540]
I0401 15:16:23.236258  229559 ssh_runner.go:195] Run: docker stop e7679c81496e 9206101402c8 1f3fa48a9c3a 9ee4f6b9ca04 69630a57be5c 871ba57330e5 2c5289d83a55 76ba9fa06c6b f6c37846ea32 ef8c2287af63 0a711f0b1ec9 da828d37c019 d4eae3abcde0 be43d79e6f83 2649effe3d24 350fd62815a0 cd42b9c0add4 21b09f332de5 42c423188178 d3bb1a2290f9 52219f1d7eb6 8aea3bcd29b1 893f3001eeee 5cffff3e86ea 1e59a71d029f 2bcd3bc2051d 68b3e836a9ff 2d99af2d5255 c705e7544641 d1f089dbf726 22988f993d68 482724ff1768 667e9b337540
I0401 15:16:32.686658  229559 ssh_runner.go:235] Completed: docker stop e7679c81496e 9206101402c8 1f3fa48a9c3a 9ee4f6b9ca04 69630a57be5c 871ba57330e5 2c5289d83a55 76ba9fa06c6b f6c37846ea32 ef8c2287af63 0a711f0b1ec9 da828d37c019 d4eae3abcde0 be43d79e6f83 2649effe3d24 350fd62815a0 cd42b9c0add4 21b09f332de5 42c423188178 d3bb1a2290f9 52219f1d7eb6 8aea3bcd29b1 893f3001eeee 5cffff3e86ea 1e59a71d029f 2bcd3bc2051d 68b3e836a9ff 2d99af2d5255 c705e7544641 d1f089dbf726 22988f993d68 482724ff1768 667e9b337540: (9.449161687s)
I0401 15:16:32.687254  229559 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0401 15:16:33.534649  229559 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0401 15:16:33.592314  229559 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Mar 29 05:21 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Apr  1 01:52 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar 29 05:22 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Apr  1 01:52 /etc/kubernetes/scheduler.conf

I0401 15:16:33.592469  229559 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0401 15:16:33.648519  229559 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0401 15:16:33.727645  229559 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0401 15:16:33.943042  229559 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0401 15:16:33.943483  229559 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0401 15:16:34.159703  229559 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0401 15:16:34.436435  229559 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0401 15:16:34.436734  229559 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0401 15:16:34.720051  229559 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0401 15:16:34.937546  229559 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0401 15:16:34.938559  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0401 15:16:37.217407  229559 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (2.27880827s)
I0401 15:16:37.217440  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0401 15:16:39.343298  229559 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (2.125819558s)
I0401 15:16:39.343329  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0401 15:16:39.756991  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0401 15:16:39.918055  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0401 15:16:40.192984  229559 api_server.go:52] waiting for apiserver process to appear ...
I0401 15:16:40.193114  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:40.250872  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:40.820959  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:41.321552  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:41.821054  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:42.320880  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:42.822196  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:43.320800  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:43.822455  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:44.322252  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:44.821868  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:45.321665  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:45.821048  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:46.322037  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:46.821925  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:47.321920  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:47.821910  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:16:48.348417  229559 api_server.go:72] duration metric: took 8.155399785s to wait for apiserver process to appear ...
I0401 15:16:48.348478  229559 api_server.go:88] waiting for apiserver healthz status ...
I0401 15:16:48.348563  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:48.368959  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:48588->127.0.0.1:32769: read: connection reset by peer
I0401 15:16:48.369096  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:48.376476  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:48592->127.0.0.1:32769: read: connection reset by peer
I0401 15:16:48.877770  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:48.881707  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:48606->127.0.0.1:32769: read: connection reset by peer
I0401 15:16:49.377018  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:49.381821  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:48622->127.0.0.1:32769: read: connection reset by peer
I0401 15:16:49.878099  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:49.883367  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:48628->127.0.0.1:32769: read: connection reset by peer
I0401 15:16:50.377832  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:16:55.379070  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0401 15:16:55.379198  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:00.380439  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0401 15:17:00.380524  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:05.382636  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0401 15:17:05.382754  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:10.383910  229559 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0401 15:17:10.383944  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:13.453012  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0401 15:17:13.453112  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0401 15:17:13.453176  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:13.753394  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0401 15:17:13.753477  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:17:13.877946  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:14.018975  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0401 15:17:14.019049  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:17:14.377154  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:14.575529  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0401 15:17:14.575712  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:17:14.877582  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:14.975993  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0401 15:17:14.976092  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:17:15.379020  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:15.449764  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0401 15:17:15.449856  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:17:15.877382  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:15.891041  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0401 15:17:15.891069  229559 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0401 15:17:16.378246  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:16.421543  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0401 15:17:16.534104  229559 api_server.go:141] control plane version: v1.28.3
I0401 15:17:16.534207  229559 api_server.go:131] duration metric: took 28.185692096s to wait for apiserver health ...
I0401 15:17:16.534270  229559 cni.go:84] Creating CNI manager for ""
I0401 15:17:16.534354  229559 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0401 15:17:16.542343  229559 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0401 15:17:16.557475  229559 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0401 15:17:16.580590  229559 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0401 15:17:16.626138  229559 system_pods.go:43] waiting for kube-system pods to appear ...
I0401 15:17:16.644290  229559 system_pods.go:59] 7 kube-system pods found
I0401 15:17:16.644320  229559 system_pods.go:61] "coredns-5dd5756b68-r7x2l" [cefa8631-fb98-4413-93a4-bfc2b264055e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0401 15:17:16.644328  229559 system_pods.go:61] "etcd-minikube" [dd41b80d-3e65-4a5f-a17b-00a713a9505d] Running
I0401 15:17:16.644340  229559 system_pods.go:61] "kube-apiserver-minikube" [a06337f4-100b-44df-ad92-6f3f7aabb4f0] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0401 15:17:16.644347  229559 system_pods.go:61] "kube-controller-manager-minikube" [c159ab35-d19a-4b52-919d-e406e6664940] Running
I0401 15:17:16.644353  229559 system_pods.go:61] "kube-proxy-2cp4n" [ff9bf080-33af-4fc2-a1b6-a8d898db14ef] Running
I0401 15:17:16.644358  229559 system_pods.go:61] "kube-scheduler-minikube" [5be45852-8295-4133-a8bb-4cec3e936781] Running
I0401 15:17:16.644365  229559 system_pods.go:61] "storage-provisioner" [24bf02fc-5c5a-41bf-aa78-0309d0b93889] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0401 15:17:16.644373  229559 system_pods.go:74] duration metric: took 18.219701ms to wait for pod list to return data ...
I0401 15:17:16.644382  229559 node_conditions.go:102] verifying NodePressure condition ...
I0401 15:17:16.651882  229559 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0401 15:17:16.651903  229559 node_conditions.go:123] node cpu capacity is 4
I0401 15:17:16.651915  229559 node_conditions.go:105] duration metric: took 7.5284ms to run NodePressure ...
I0401 15:17:16.651937  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0401 15:17:17.348506  229559 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0401 15:17:17.522160  229559 ops.go:34] apiserver oom_adj: -16
I0401 15:17:17.522575  229559 kubeadm.go:640] restartCluster took 1m10.701136355s
I0401 15:17:17.522635  229559 kubeadm.go:406] StartCluster complete in 1m11.558799186s
I0401 15:17:17.523287  229559 settings.go:142] acquiring lock: {Name:mkac65ce0183564c0160b580651098887c156546 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0401 15:17:17.523627  229559 settings.go:150] Updating kubeconfig:  /home/lee/.kube/config
I0401 15:17:17.527988  229559 lock.go:35] WriteFile acquiring /home/lee/.kube/config: {Name:mke78f7e985c39ffd6431c856c291714e0dfcbf0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0401 15:17:17.531669  229559 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0401 15:17:17.532366  229559 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0401 15:17:17.532505  229559 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0401 15:17:17.533150  229559 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0401 15:17:17.533237  229559 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0401 15:17:17.533414  229559 addons.go:240] addon storage-provisioner should already be in state true
I0401 15:17:17.534099  229559 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0401 15:17:17.534179  229559 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0401 15:17:17.535432  229559 host.go:66] Checking if "minikube" exists ...
I0401 15:17:17.536308  229559 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0401 15:17:17.543517  229559 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0401 15:17:17.641968  229559 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0401 15:17:17.642096  229559 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0401 15:17:17.651940  229559 out.go:177] 🔎  Verifying Kubernetes components...
I0401 15:17:17.666305  229559 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0401 15:17:17.678795  229559 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0401 15:17:17.678890  229559 addons.go:240] addon default-storageclass should already be in state true
I0401 15:17:17.678964  229559 host.go:66] Checking if "minikube" exists ...
I0401 15:17:17.680799  229559 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0401 15:17:17.692426  229559 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0401 15:17:17.698619  229559 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0401 15:17:17.698641  229559 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0401 15:17:17.698756  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:17:17.757906  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:17:17.761558  229559 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0401 15:17:17.761578  229559 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0401 15:17:17.761686  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0401 15:17:17.806237  229559 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0401 15:17:17.911401  229559 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0401 15:17:17.911401  229559 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0401 15:17:17.946781  229559 api_server.go:52] waiting for apiserver process to appear ...
I0401 15:17:17.946895  229559 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0401 15:17:17.953919  229559 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0401 15:17:17.987172  229559 api_server.go:72] duration metric: took 344.986711ms to wait for apiserver process to appear ...
I0401 15:17:17.987197  229559 api_server.go:88] waiting for apiserver healthz status ...
I0401 15:17:17.987493  229559 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0401 15:17:17.992986  229559 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0401 15:17:18.001539  229559 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0401 15:17:18.005205  229559 api_server.go:141] control plane version: v1.28.3
I0401 15:17:18.005230  229559 api_server.go:131] duration metric: took 17.8051ms to wait for apiserver health ...
I0401 15:17:18.005252  229559 system_pods.go:43] waiting for kube-system pods to appear ...
I0401 15:17:18.032216  229559 system_pods.go:59] 7 kube-system pods found
I0401 15:17:18.032257  229559 system_pods.go:61] "coredns-5dd5756b68-r7x2l" [cefa8631-fb98-4413-93a4-bfc2b264055e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0401 15:17:18.032274  229559 system_pods.go:61] "etcd-minikube" [dd41b80d-3e65-4a5f-a17b-00a713a9505d] Running
I0401 15:17:18.032295  229559 system_pods.go:61] "kube-apiserver-minikube" [a06337f4-100b-44df-ad92-6f3f7aabb4f0] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0401 15:17:18.032307  229559 system_pods.go:61] "kube-controller-manager-minikube" [c159ab35-d19a-4b52-919d-e406e6664940] Running
I0401 15:17:18.032318  229559 system_pods.go:61] "kube-proxy-2cp4n" [ff9bf080-33af-4fc2-a1b6-a8d898db14ef] Running
I0401 15:17:18.032330  229559 system_pods.go:61] "kube-scheduler-minikube" [5be45852-8295-4133-a8bb-4cec3e936781] Running
I0401 15:17:18.032343  229559 system_pods.go:61] "storage-provisioner" [24bf02fc-5c5a-41bf-aa78-0309d0b93889] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0401 15:17:18.032355  229559 system_pods.go:74] duration metric: took 27.093501ms to wait for pod list to return data ...
I0401 15:17:18.032372  229559 kubeadm.go:581] duration metric: took 390.193512ms to wait for : map[apiserver:true system_pods:true] ...
I0401 15:17:18.032394  229559 node_conditions.go:102] verifying NodePressure condition ...
I0401 15:17:18.050370  229559 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0401 15:17:18.050395  229559 node_conditions.go:123] node cpu capacity is 4
I0401 15:17:18.050410  229559 node_conditions.go:105] duration metric: took 18.007901ms to run NodePressure ...
I0401 15:17:18.050434  229559 start.go:228] waiting for startup goroutines ...
I0401 15:17:21.481450  229559 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.527497307s)
I0401 15:17:21.481607  229559 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.488579506s)
I0401 15:17:21.503247  229559 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0401 15:17:21.508967  229559 addons.go:502] enable addons completed in 3.976444321s: enabled=[storage-provisioner default-storageclass]
I0401 15:17:21.509059  229559 start.go:233] waiting for cluster config update ...
I0401 15:17:21.509077  229559 start.go:242] writing updated cluster config ...
I0401 15:17:21.509625  229559 ssh_runner.go:195] Run: rm -f paused
I0401 15:17:21.780917  229559 start.go:600] kubectl: 1.29.3, cluster: 1.28.3 (minor skew: 1)
I0401 15:17:21.784615  229559 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Apr 01 06:38:48 minikube dockerd[174559]: time="2024-04-01T06:38:48.348444321Z" level=info msg="ignoring event" container=a5adc5ed7c8dde58388fa514aff48fcc87f71e660b41cdd2177522ecc40d95f1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:38:48 minikube dockerd[174559]: time="2024-04-01T06:38:48.649403934Z" level=info msg="ignoring event" container=b054da696e9fcab05d6701bd75724a81e221013557dd0c7fc7d76a4f607bf6b7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:38:50 minikube dockerd[174559]: time="2024-04-01T06:38:50.720444719Z" level=info msg="ignoring event" container=e893cc2334da442fbed2cc29771a0a90dd41bb40fd479b52ff4eb9bdaa89fc77 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:38:50 minikube dockerd[174559]: time="2024-04-01T06:38:50.846453024Z" level=info msg="ignoring event" container=fdf86f94b26e6d2afb5d245ea436393fab6e58711b0c07e6df05374b7f843706 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:38:53 minikube cri-dockerd[175143]: time="2024-04-01T06:38:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f587669f071f16be4ae0d7d9455997fda4952c42dac85da67cf76b2dd18e63ed/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 06:38:53 minikube cri-dockerd[175143]: time="2024-04-01T06:38:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/38548b53be3da8f6a8e9548bc223c80215960d9716c5c009dd93a1b1614b7dfd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 06:38:58 minikube cri-dockerd[175143]: time="2024-04-01T06:38:58Z" level=info msg="Stop pulling image jaruda/fast-api:latest: Status: Image is up to date for jaruda/fast-api:latest"
Apr 01 06:39:02 minikube cri-dockerd[175143]: time="2024-04-01T06:39:02Z" level=info msg="Stop pulling image jaruda/fast-api:latest: Status: Image is up to date for jaruda/fast-api:latest"
Apr 01 06:39:14 minikube dockerd[174559]: time="2024-04-01T06:39:14.477894051Z" level=info msg="ignoring event" container=c15642e6099cc5772d5997039fab24458d32c2f1962791094c4ce7470cabc8c8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:39:15 minikube dockerd[174559]: time="2024-04-01T06:39:15.559685924Z" level=info msg="ignoring event" container=f587669f071f16be4ae0d7d9455997fda4952c42dac85da67cf76b2dd18e63ed module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:39:18 minikube dockerd[174559]: time="2024-04-01T06:39:18.593433627Z" level=info msg="ignoring event" container=9a3b172a3bb6eddbf6dbc9afa3e0e904deb48e1b741cdc77d7b1eb590097e8af module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 06:39:19 minikube dockerd[174559]: time="2024-04-01T06:39:19.248075415Z" level=info msg="ignoring event" container=38548b53be3da8f6a8e9548bc223c80215960d9716c5c009dd93a1b1614b7dfd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 07:30:43 minikube cri-dockerd[175143]: time="2024-04-01T07:30:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a41086200f08b0114921ea7f6a0e84194f0609e33a70ad8228590fed7f842102/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 07:30:58 minikube cri-dockerd[175143]: time="2024-04-01T07:30:58Z" level=info msg="Pulling image jaruda/fast-api2:latest: ac7741184a3e: Extracting [=============================================>     ]  27.53MB/30.27MB"
Apr 01 07:30:58 minikube cri-dockerd[175143]: time="2024-04-01T07:30:58Z" level=info msg="Stop pulling image jaruda/fast-api2:latest: Status: Downloaded newer image for jaruda/fast-api2:latest"
Apr 01 07:51:56 minikube dockerd[174559]: time="2024-04-01T07:51:56.824969652Z" level=info msg="ignoring event" container=882b157304dd1b79a7ed5cbcd3a914b4d5fd299b9d95a0df6f5faf5976331fd9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 07:51:57 minikube dockerd[174559]: time="2024-04-01T07:51:57.782133997Z" level=info msg="ignoring event" container=a41086200f08b0114921ea7f6a0e84194f0609e33a70ad8228590fed7f842102 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 07:51:57 minikube cri-dockerd[175143]: time="2024-04-01T07:51:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c351deb34a363772fa6232c100994c48908a028d1cf68e2eaab8d8a6a5683fca/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 07:52:01 minikube cri-dockerd[175143]: time="2024-04-01T07:52:01Z" level=info msg="Stop pulling image jaruda/fast-api2:latest: Status: Image is up to date for jaruda/fast-api2:latest"
Apr 01 07:52:20 minikube cri-dockerd[175143]: time="2024-04-01T07:52:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8a72ce87a2731f4bbce1ff508814e90a1649afda55749e5b9d92bae0fe45b064/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 07:52:33 minikube dockerd[174559]: time="2024-04-01T07:52:33.724503953Z" level=info msg="Container failed to exit within 30s of signal 15 - using the force" container=609c84f7dccb6defb8bb7ee3ab6859f53af7b2d54a805351437f0a56af0412e8
Apr 01 07:52:33 minikube dockerd[174559]: time="2024-04-01T07:52:33.926440659Z" level=info msg="ignoring event" container=609c84f7dccb6defb8bb7ee3ab6859f53af7b2d54a805351437f0a56af0412e8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 07:52:34 minikube dockerd[174559]: time="2024-04-01T07:52:34.363387372Z" level=info msg="ignoring event" container=c351deb34a363772fa6232c100994c48908a028d1cf68e2eaab8d8a6a5683fca module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:13:46 minikube dockerd[174559]: time="2024-04-01T08:13:46.389181324Z" level=info msg="ignoring event" container=8a72ce87a2731f4bbce1ff508814e90a1649afda55749e5b9d92bae0fe45b064 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:13:46 minikube cri-dockerd[175143]: time="2024-04-01T08:13:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/69af5dd8404043075fd85c9066eed9a43371b0c829f4854eefe5ee3e09d4852a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:13:52 minikube dockerd[174559]: time="2024-04-01T08:13:52.161855971Z" level=info msg="ignoring event" container=69af5dd8404043075fd85c9066eed9a43371b0c829f4854eefe5ee3e09d4852a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:14:10 minikube cri-dockerd[175143]: time="2024-04-01T08:14:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ff808d2e85a093a2d4d6930bc5d30a0470335c056ec71e805be1d7442905744b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:16:29 minikube dockerd[174559]: time="2024-04-01T08:16:29.047515173Z" level=info msg="ignoring event" container=ff808d2e85a093a2d4d6930bc5d30a0470335c056ec71e805be1d7442905744b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:16:29 minikube cri-dockerd[175143]: time="2024-04-01T08:16:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ed25aa3ed50aac184fb5580f92fdd6d2a0d8200831e9506706ba3ff0a1cfd19f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:16:32 minikube dockerd[174559]: time="2024-04-01T08:16:32.808788889Z" level=info msg="ignoring event" container=ed25aa3ed50aac184fb5580f92fdd6d2a0d8200831e9506706ba3ff0a1cfd19f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:19:51 minikube dockerd[174559]: time="2024-04-01T08:19:51.846903669Z" level=warning msg="no trace recorder found, skipping"
Apr 01 08:35:48 minikube cri-dockerd[175143]: time="2024-04-01T08:35:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d45fa8b1aa4df13fcff8dce600cd0a1adc19519e1ca309b989f15445aa440735/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:38:51 minikube dockerd[174559]: time="2024-04-01T08:38:51.161152684Z" level=info msg="ignoring event" container=d45fa8b1aa4df13fcff8dce600cd0a1adc19519e1ca309b989f15445aa440735 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:38:51 minikube cri-dockerd[175143]: time="2024-04-01T08:38:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2f1ecb204c46f68895bef4351aa6420019778f6c9651135924f30499f009dc29/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:38:55 minikube dockerd[174559]: time="2024-04-01T08:38:55.627195227Z" level=info msg="ignoring event" container=2f1ecb204c46f68895bef4351aa6420019778f6c9651135924f30499f009dc29 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:39:18 minikube cri-dockerd[175143]: time="2024-04-01T08:39:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/480fb78c9eea2799b7d1bc8fefb25cfed77214341a209d3d2ad619768338f7d6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:39:22 minikube dockerd[174559]: time="2024-04-01T08:39:22.210139817Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 01 08:39:22 minikube dockerd[174559]: time="2024-04-01T08:39:22.210349317Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 01 08:39:37 minikube dockerd[174559]: time="2024-04-01T08:39:37.880821807Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 01 08:39:37 minikube dockerd[174559]: time="2024-04-01T08:39:37.881081207Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 01 08:40:07 minikube dockerd[174559]: time="2024-04-01T08:40:07.639835661Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 01 08:40:07 minikube dockerd[174559]: time="2024-04-01T08:40:07.641070161Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 01 08:40:55 minikube dockerd[174559]: time="2024-04-01T08:40:55.008300802Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 01 08:40:55 minikube dockerd[174559]: time="2024-04-01T08:40:55.008682202Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 01 08:40:58 minikube dockerd[174559]: time="2024-04-01T08:40:58.050872802Z" level=info msg="ignoring event" container=480fb78c9eea2799b7d1bc8fefb25cfed77214341a209d3d2ad619768338f7d6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:40:58 minikube cri-dockerd[175143]: time="2024-04-01T08:40:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/56ce2121847a9441347031bbd5b2692d4a416d534c55f600b8c56d01fe84706e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:41:02 minikube dockerd[174559]: time="2024-04-01T08:41:02.365139385Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Apr 01 08:41:02 minikube dockerd[174559]: time="2024-04-01T08:41:02.365232585Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Apr 01 08:41:03 minikube dockerd[174559]: time="2024-04-01T08:41:03.461370959Z" level=info msg="ignoring event" container=56ce2121847a9441347031bbd5b2692d4a416d534c55f600b8c56d01fe84706e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:41:25 minikube cri-dockerd[175143]: time="2024-04-01T08:41:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fd5e18a1d6d3730df26bfb967c2d594de64be410d287915c4f3dd96569b03cc2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:42:05 minikube dockerd[174559]: time="2024-04-01T08:42:05.063419399Z" level=info msg="ignoring event" container=baab5e67a82e3076830a55b22ec0a78843f54b85e43164d1064f15ad52f3bf2c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:42:05 minikube cri-dockerd[175143]: time="2024-04-01T08:42:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3f1045980823bff7aeb94f01ff62842a6e0509463cefa9d58bc4de91e5a111a9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:42:05 minikube dockerd[174559]: time="2024-04-01T08:42:05.415085508Z" level=info msg="ignoring event" container=fd5e18a1d6d3730df26bfb967c2d594de64be410d287915c4f3dd96569b03cc2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:42:09 minikube dockerd[174559]: time="2024-04-01T08:42:09.468475696Z" level=info msg="ignoring event" container=14470f80660998b5d1b54373dd6020dac35a2ad3830dc418f0016309ce0e2edb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:42:09 minikube dockerd[174559]: time="2024-04-01T08:42:09.918301290Z" level=info msg="ignoring event" container=3f1045980823bff7aeb94f01ff62842a6e0509463cefa9d58bc4de91e5a111a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:42:51 minikube cri-dockerd[175143]: time="2024-04-01T08:42:51Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/218228d976bfe9f8268627da3b048cc5c62bad6ed43636d880bf681053116b2c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:53:58 minikube dockerd[174559]: time="2024-04-01T08:53:58.461072468Z" level=info msg="ignoring event" container=218228d976bfe9f8268627da3b048cc5c62bad6ed43636d880bf681053116b2c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:53:58 minikube cri-dockerd[175143]: time="2024-04-01T08:53:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1f5e89bd3564fc66cb19d6274fadb1bcf8154a7cfdde4c18c0f55282249a88b6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Apr 01 08:54:02 minikube dockerd[174559]: time="2024-04-01T08:54:02.567195705Z" level=info msg="ignoring event" container=1f5e89bd3564fc66cb19d6274fadb1bcf8154a7cfdde4c18c0f55282249a88b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Apr 01 08:55:57 minikube cri-dockerd[175143]: time="2024-04-01T08:55:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/93bdcab309e71533046d3ab2dd97702fb575977af1a0b352308f18630f116de5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
9ffa6f580ea0e       6e38f40d628db       3 hours ago         Running             storage-provisioner       11                  0757da60ea3cf       storage-provisioner
93d74cc49832c       ead0a4a53df89       3 hours ago         Running             coredns                   6                   b9cce9e9f9e75       coredns-5dd5756b68-r7x2l
f1ee1a0b6484e       6e38f40d628db       3 hours ago         Exited              storage-provisioner       10                  0757da60ea3cf       storage-provisioner
63dfa62cfbd96       bfc896cf80fba       3 hours ago         Running             kube-proxy                6                   7ecab98cf96c2       kube-proxy-2cp4n
6f4e6cd32d172       5374347291230       3 hours ago         Running             kube-apiserver            6                   4c26f0fe4c179       kube-apiserver-minikube
dae2c55155e1e       6d1b4fd1b182d       3 hours ago         Running             kube-scheduler            6                   ed233e13d0302       kube-scheduler-minikube
ce5de01a2961c       10baa1ca17068       3 hours ago         Running             kube-controller-manager   6                   dfb004587c648       kube-controller-manager-minikube
dab70cb5e3b73       73deb9a3f7025       3 hours ago         Running             etcd                      6                   0582eff1b4326       etcd-minikube
e7679c81496e7       ead0a4a53df89       3 hours ago         Exited              coredns                   5                   871ba57330e50       coredns-5dd5756b68-r7x2l
9206101402c8f       73deb9a3f7025       3 hours ago         Exited              etcd                      5                   2c5289d83a557       etcd-minikube
1f3fa48a9c3a2       10baa1ca17068       3 hours ago         Exited              kube-controller-manager   5                   76ba9fa06c6bf       kube-controller-manager-minikube
9ee4f6b9ca048       5374347291230       3 hours ago         Exited              kube-apiserver            5                   ef8c2287af63b       kube-apiserver-minikube
69630a57be5c6       6d1b4fd1b182d       3 hours ago         Exited              kube-scheduler            5                   da828d37c0192       kube-scheduler-minikube
0a711f0b1ec9e       bfc896cf80fba       3 hours ago         Exited              kube-proxy                5                   2649effe3d24b       kube-proxy-2cp4n

* 
* ==> coredns [93d74cc49832] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:37205 - 8876 "HINFO IN 1083555194305296467.7465515288272660051. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.049378095s

* 
* ==> coredns [e7679c81496e] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:34707 - 16920 "HINFO IN 8038231472128302156.6460758985050802966. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.160495306s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_29T14_22_32_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 29 Mar 2024 05:22:21 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 01 Apr 2024 08:56:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 01 Apr 2024 08:55:24 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 01 Apr 2024 08:55:24 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 01 Apr 2024 08:55:24 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 01 Apr 2024 08:55:24 +0000   Fri, 29 Mar 2024 05:22:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941276Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941276Ki
  pods:               110
System Info:
  Machine ID:                 2fcd08a5855c4797bd8a6c3dfc648a2b
  System UUID:                2fcd08a5855c4797bd8a6c3dfc648a2b
  Boot ID:                    912e6e5b-e010-450f-9ac4-f4f7e10da9ba
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     fast-api-deployment-bdc468646-s9q2x    500m (12%!)(MISSING)    500m (12%!)(MISSING)  256Mi (6%!)(MISSING)       256Mi (6%!)(MISSING)     23s
  kube-system                 coredns-5dd5756b68-r7x2l               100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     3d3h
  kube-system                 etcd-minikube                          100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         3d3h
  kube-system                 kube-apiserver-minikube                250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d3h
  kube-system                 kube-controller-manager-minikube       200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d3h
  kube-system                 kube-proxy-2cp4n                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d3h
  kube-system                 kube-scheduler-minikube                100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d3h
  kube-system                 storage-provisioner                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d3h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1250m (31%!)(MISSING)  500m (12%!)(MISSING)
  memory             426Mi (11%!)(MISSING)  426Mi (11%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Apr 1 02:28] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.000000] PCI: Fatal: No config space access function found
[  +0.030584] PCI: System does not support PCI
[  +0.030106] kvm: no hardware support
[  +0.000004] kvm: no hardware support
[  +0.849940] FS-Cache: Duplicate cookie detected
[  +0.001238] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.002332] FS-Cache: O-cookie d=0000000041705acc{9P.session} n=000000004bde9075
[  +0.002104] FS-Cache: O-key=[10] '34323934393337333837'
[  +0.000644] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000821] FS-Cache: N-cookie d=0000000041705acc{9P.session} n=00000000eb2176ab
[  +0.004222] FS-Cache: N-key=[10] '34323934393337333837'
[  +0.005907] FS-Cache: Duplicate cookie detected
[  +0.000649] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000599] FS-Cache: O-cookie d=0000000041705acc{9P.session} n=00000000292de3d8
[  +0.000713] FS-Cache: O-key=[10] '34323934393337333839'
[  +0.000479] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000591] FS-Cache: N-cookie d=0000000041705acc{9P.session} n=0000000040636dd6
[  +0.001023] FS-Cache: N-key=[10] '34323934393337333839'
[  +3.967247] Failed to connect to bus: No such file or directory
[  +0.230159] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.025320] Failed to connect to bus: No such file or directory
[  +0.314944] systemd-journald[41]: File /var/log/journal/089e2fbe8854471ba5fdc5dea93fbac9/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Apr 1 05:09] hrtimer: interrupt took 8318599 ns

* 
* ==> etcd [9206101402c8] <==
* {"level":"info","ts":"2024-04-01T06:15:56.22328Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":2658304,"backend-size":"2.7 MB","backend-size-in-use-bytes":1155072,"backend-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-04-01T06:16:02.064811Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":25496}
{"level":"info","ts":"2024-04-01T06:16:02.073144Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-04-01T06:16:02.073465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 6"}
{"level":"info","ts":"2024-04-01T06:16:02.073615Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 6, commit: 25496, applied: 20002, lastindex: 25496, lastterm: 6]"}
{"level":"info","ts":"2024-04-01T06:16:02.12207Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-01T06:16:02.122345Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-04-01T06:16:02.128082Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-04-01T06:16:02.140111Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-04-01T06:16:02.149736Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":20077}
{"level":"info","ts":"2024-04-01T06:16:02.323107Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":20408}
{"level":"info","ts":"2024-04-01T06:16:02.417297Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-04-01T06:16:02.462101Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-04-01T06:16:02.464452Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-01T06:16:02.4648Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-04-01T06:16:02.516796Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-04-01T06:16:02.518738Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-01T06:16:02.518916Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-01T06:16:02.518985Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-04-01T06:16:02.826305Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-01T06:16:02.826693Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-01T06:16:02.826967Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-04-01T06:16:02.834421Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-04-01T06:16:02.834623Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-04-01T06:16:03.131805Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 6"}
{"level":"info","ts":"2024-04-01T06:16:03.134152Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 6"}
{"level":"info","ts":"2024-04-01T06:16:03.134462Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 6"}
{"level":"info","ts":"2024-04-01T06:16:03.134589Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 7"}
{"level":"info","ts":"2024-04-01T06:16:03.134672Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-04-01T06:16:03.157378Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 7"}
{"level":"info","ts":"2024-04-01T06:16:03.157619Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-04-01T06:16:03.457057Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-04-01T06:16:03.457295Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-01T06:16:03.518253Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-04-01T06:16:03.525309Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-04-01T06:16:03.539817Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-04-01T06:16:03.539982Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-04-01T06:16:03.542083Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-04-01T06:16:19.63131Z","caller":"traceutil/trace.go:171","msg":"trace[1759170394] linearizableReadLoop","detail":"{readStateIndex:25518; appliedIndex:25515; }","duration":"104.96541ms","start":"2024-04-01T06:16:19.526262Z","end":"2024-04-01T06:16:19.631227Z","steps":["trace[1759170394] 'read index received'  (duration: 10.329301ms)","trace[1759170394] 'applied index is now lower than readState.Index'  (duration: 94.631309ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-01T06:16:19.631994Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.751012ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-apiserver-minikube.17c213535e35cdd5\" ","response":"range_response_count:1 size:834"}
{"level":"info","ts":"2024-04-01T06:16:19.633001Z","caller":"traceutil/trace.go:171","msg":"trace[1965696701] range","detail":"{range_begin:/registry/events/kube-system/kube-apiserver-minikube.17c213535e35cdd5; range_end:; response_count:1; response_revision:20424; }","duration":"106.108612ms","start":"2024-04-01T06:16:19.526148Z","end":"2024-04-01T06:16:19.632255Z","steps":["trace[1965696701] 'agreement among raft nodes before linearized reading'  (duration: 105.578212ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T06:16:19.916838Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"175.637218ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:volume-scheduler\" ","response":"range_response_count:1 size:724"}
{"level":"info","ts":"2024-04-01T06:16:19.91719Z","caller":"traceutil/trace.go:171","msg":"trace[1401666116] range","detail":"{range_begin:/registry/clusterroles/system:volume-scheduler; range_end:; response_count:1; response_revision:20425; }","duration":"175.999018ms","start":"2024-04-01T06:16:19.741127Z","end":"2024-04-01T06:16:19.917126Z","steps":["trace[1401666116] 'range keys from in-memory index tree'  (duration: 171.824718ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T06:16:21.853271Z","caller":"traceutil/trace.go:171","msg":"trace[1193599729] transaction","detail":"{read_only:false; response_revision:20440; number_of_response:1; }","duration":"101.429611ms","start":"2024-04-01T06:16:21.751764Z","end":"2024-04-01T06:16:21.853193Z","steps":["trace[1193599729] 'process raft request'  (duration: 94.88981ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T06:16:25.23062Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"184.402619ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-controller-manager-minikube.17c2050b4fb012c7\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-01T06:16:25.230909Z","caller":"traceutil/trace.go:171","msg":"trace[1002835215] range","detail":"{range_begin:/registry/events/kube-system/kube-controller-manager-minikube.17c2050b4fb012c7; range_end:; response_count:0; response_revision:20460; }","duration":"184.697819ms","start":"2024-04-01T06:16:25.046134Z","end":"2024-04-01T06:16:25.230832Z","steps":["trace[1002835215] 'range keys from in-memory index tree'  (duration: 184.030119ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T06:16:25.231136Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"166.948517ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/rolebindings/kube-system/system:controller:cloud-provider\" ","response":"range_response_count:1 size:734"}
{"level":"info","ts":"2024-04-01T06:16:25.231425Z","caller":"traceutil/trace.go:171","msg":"trace[673707664] range","detail":"{range_begin:/registry/rolebindings/kube-system/system:controller:cloud-provider; range_end:; response_count:1; response_revision:20460; }","duration":"167.249017ms","start":"2024-04-01T06:16:25.064097Z","end":"2024-04-01T06:16:25.231346Z","steps":["trace[673707664] 'range keys from in-memory index tree'  (duration: 166.505417ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T06:16:25.818942Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-04-01T06:16:25.819231Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-04-01T06:16:25.819798Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-01T06:16:25.820228Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
WARNING: 2024/04/01 06:16:25 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
WARNING: 2024/04/01 06:16:25 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2024-04-01T06:16:26.14755Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-04-01T06:16:26.147782Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-04-01T06:16:26.148261Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-04-01T06:16:26.220329Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-01T06:16:26.221406Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-04-01T06:16:26.221546Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [dab70cb5e3b7] <==
* {"level":"info","ts":"2024-04-01T08:20:21.433236Z","caller":"traceutil/trace.go:171","msg":"trace[1642396267] transaction","detail":"{read_only:false; response_revision:26981; number_of_response:1; }","duration":"200.971213ms","start":"2024-04-01T08:20:21.232236Z","end":"2024-04-01T08:20:21.433208Z","steps":["trace[1642396267] 'process raft request'  (duration: 108.812807ms)","trace[1642396267] 'compare'  (duration: 92.017906ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:20:26.477649Z","caller":"traceutil/trace.go:171","msg":"trace[353036253] transaction","detail":"{read_only:false; response_revision:26985; number_of_response:1; }","duration":"128.050308ms","start":"2024-04-01T08:20:26.349505Z","end":"2024-04-01T08:20:26.477555Z","steps":["trace[353036253] 'process raft request'  (duration: 63.492204ms)","trace[353036253] 'compare'  (duration: 63.859004ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:20:31.354406Z","caller":"traceutil/trace.go:171","msg":"trace[1120377025] transaction","detail":"{read_only:false; response_revision:26989; number_of_response:1; }","duration":"126.937608ms","start":"2024-04-01T08:20:31.227373Z","end":"2024-04-01T08:20:31.35431Z","steps":["trace[1120377025] 'process raft request'  (duration: 68.773705ms)","trace[1120377025] 'compare'  (duration: 57.426303ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:20:51.301635Z","caller":"traceutil/trace.go:171","msg":"trace[27359000] transaction","detail":"{read_only:false; response_revision:27004; number_of_response:1; }","duration":"107.799311ms","start":"2024-04-01T08:20:51.193704Z","end":"2024-04-01T08:20:51.301504Z","steps":["trace[27359000] 'process raft request'  (duration: 78.977408ms)","trace[27359000] 'compare'  (duration: 28.286303ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:20:55.552102Z","caller":"traceutil/trace.go:171","msg":"trace[883704206] transaction","detail":"{read_only:false; response_revision:27008; number_of_response:1; }","duration":"162.538617ms","start":"2024-04-01T08:20:55.389516Z","end":"2024-04-01T08:20:55.552055Z","steps":["trace[883704206] 'process raft request'  (duration: 139.855315ms)","trace[883704206] 'compare'  (duration: 22.377902ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:21:11.364478Z","caller":"traceutil/trace.go:171","msg":"trace[1307453094] transaction","detail":"{read_only:false; response_revision:27019; number_of_response:1; }","duration":"128.236704ms","start":"2024-04-01T08:21:11.236154Z","end":"2024-04-01T08:21:11.364391Z","steps":["trace[1307453094] 'process raft request'  (duration: 33.941501ms)","trace[1307453094] 'compare'  (duration: 93.461703ms)"],"step_count":2}
{"level":"warn","ts":"2024-04-01T08:21:31.363955Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"103.314788ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028212287037456 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:27027 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128028212287037454 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-01T08:21:31.364387Z","caller":"traceutil/trace.go:171","msg":"trace[56704860] transaction","detail":"{read_only:false; response_revision:27035; number_of_response:1; }","duration":"138.484185ms","start":"2024-04-01T08:21:31.225827Z","end":"2024-04-01T08:21:31.364311Z","steps":["trace[56704860] 'process raft request'  (duration: 34.633897ms)","trace[56704860] 'compare'  (duration: 102.805888ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:21:42.199959Z","caller":"traceutil/trace.go:171","msg":"trace[777613478] transaction","detail":"{read_only:false; response_revision:27044; number_of_response:1; }","duration":"127.107988ms","start":"2024-04-01T08:21:42.072836Z","end":"2024-04-01T08:21:42.199944Z","steps":["trace[777613478] 'process raft request'  (duration: 126.948888ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:22:01.359463Z","caller":"traceutil/trace.go:171","msg":"trace[1138309638] transaction","detail":"{read_only:false; response_revision:27058; number_of_response:1; }","duration":"130.08078ms","start":"2024-04-01T08:22:01.229351Z","end":"2024-04-01T08:22:01.359432Z","steps":["trace[1138309638] 'process raft request'  (duration: 129.73988ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:22:10.480331Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26826}
{"level":"info","ts":"2024-04-01T08:22:10.482808Z","caller":"traceutil/trace.go:171","msg":"trace[473346240] compact","detail":"{revision:26826; response_revision:27066; }","duration":"108.26979ms","start":"2024-04-01T08:22:10.374443Z","end":"2024-04-01T08:22:10.482713Z","steps":["trace[473346240] 'process raft request'  (duration: 33.636297ms)","trace[473346240] 'check and update compact revision'  (duration: 72.145593ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:22:10.534437Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":26826,"took":"52.413995ms","hash":1047216967}
{"level":"info","ts":"2024-04-01T08:22:10.534502Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1047216967,"revision":26826,"compact-revision":26463}
{"level":"info","ts":"2024-04-01T08:22:28.68423Z","caller":"traceutil/trace.go:171","msg":"trace[673316664] transaction","detail":"{read_only:false; response_revision:27081; number_of_response:1; }","duration":"113.419857ms","start":"2024-04-01T08:22:28.570769Z","end":"2024-04-01T08:22:28.684189Z","steps":["trace[673316664] 'process raft request'  (duration: 113.300557ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:22:51.377852Z","caller":"traceutil/trace.go:171","msg":"trace[1297363736] transaction","detail":"{read_only:false; response_revision:27099; number_of_response:1; }","duration":"117.005193ms","start":"2024-04-01T08:22:51.260788Z","end":"2024-04-01T08:22:51.377793Z","steps":["trace[1297363736] 'process raft request'  (duration: 85.745395ms)","trace[1297363736] 'compare'  (duration: 30.733898ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:23:14.103169Z","caller":"traceutil/trace.go:171","msg":"trace[1940829011] transaction","detail":"{read_only:false; response_revision:27117; number_of_response:1; }","duration":"131.298891ms","start":"2024-04-01T08:23:13.971847Z","end":"2024-04-01T08:23:14.103146Z","steps":["trace[1940829011] 'process raft request'  (duration: 131.117891ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:24:05.522847Z","caller":"traceutil/trace.go:171","msg":"trace[393518761] transaction","detail":"{read_only:false; response_revision:27157; number_of_response:1; }","duration":"126.338814ms","start":"2024-04-01T08:24:05.396461Z","end":"2024-04-01T08:24:05.5228Z","steps":["trace[393518761] 'process raft request'  (duration: 126.008814ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:26:11.799225Z","caller":"traceutil/trace.go:171","msg":"trace[565486225] linearizableReadLoop","detail":"{readStateIndex:34042; appliedIndex:34041; }","duration":"111.390632ms","start":"2024-04-01T08:26:11.687784Z","end":"2024-04-01T08:26:11.799174Z","steps":["trace[565486225] 'read index received'  (duration: 111.372532ms)","trace[565486225] 'applied index is now lower than readState.Index'  (duration: 17.3µs)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:26:11.799356Z","caller":"traceutil/trace.go:171","msg":"trace[1873094913] transaction","detail":"{read_only:false; response_revision:27259; number_of_response:1; }","duration":"112.335732ms","start":"2024-04-01T08:26:11.686984Z","end":"2024-04-01T08:26:11.79932Z","steps":["trace[1873094913] 'process raft request'  (duration: 112.064632ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T08:26:11.799414Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.636432ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2024-04-01T08:26:11.799452Z","caller":"traceutil/trace.go:171","msg":"trace[1175833918] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:27259; }","duration":"111.689632ms","start":"2024-04-01T08:26:11.68775Z","end":"2024-04-01T08:26:11.79944Z","steps":["trace[1175833918] 'agreement among raft nodes before linearized reading'  (duration: 111.612132ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:27:10.494558Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27066}
{"level":"info","ts":"2024-04-01T08:27:10.503529Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27066,"took":"8.5306ms","hash":911701956}
{"level":"info","ts":"2024-04-01T08:27:10.503613Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":911701956,"revision":27066,"compact-revision":26826}
{"level":"info","ts":"2024-04-01T08:27:57.087521Z","caller":"traceutil/trace.go:171","msg":"trace[1041486869] transaction","detail":"{read_only:false; response_revision:27343; number_of_response:1; }","duration":"108.075003ms","start":"2024-04-01T08:27:56.979421Z","end":"2024-04-01T08:27:57.087496Z","steps":["trace[1041486869] 'process raft request'  (duration: 107.899103ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:29:31.568443Z","caller":"traceutil/trace.go:171","msg":"trace[230896533] transaction","detail":"{read_only:false; response_revision:27418; number_of_response:1; }","duration":"114.636694ms","start":"2024-04-01T08:29:31.453738Z","end":"2024-04-01T08:29:31.568375Z","steps":["trace[230896533] 'process raft request'  (duration: 114.157794ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:29:58.32204Z","caller":"traceutil/trace.go:171","msg":"trace[1024672118] transaction","detail":"{read_only:false; response_revision:27439; number_of_response:1; }","duration":"139.497897ms","start":"2024-04-01T08:29:58.182513Z","end":"2024-04-01T08:29:58.322011Z","steps":["trace[1024672118] 'process raft request'  (duration: 138.987697ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:30:54.945343Z","caller":"traceutil/trace.go:171","msg":"trace[824425215] transaction","detail":"{read_only:false; response_revision:27485; number_of_response:1; }","duration":"167.326691ms","start":"2024-04-01T08:30:54.777951Z","end":"2024-04-01T08:30:54.945278Z","steps":["trace[824425215] 'process raft request'  (duration: 167.193491ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:31:17.387015Z","caller":"traceutil/trace.go:171","msg":"trace[893247341] linearizableReadLoop","detail":"{readStateIndex:34349; appliedIndex:34348; }","duration":"215.042701ms","start":"2024-04-01T08:31:17.171938Z","end":"2024-04-01T08:31:17.386981Z","steps":["trace[893247341] 'read index received'  (duration: 214.848601ms)","trace[893247341] 'applied index is now lower than readState.Index'  (duration: 193.3µs)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:31:17.387041Z","caller":"traceutil/trace.go:171","msg":"trace[187016627] transaction","detail":"{read_only:false; response_revision:27502; number_of_response:1; }","duration":"221.840001ms","start":"2024-04-01T08:31:17.165186Z","end":"2024-04-01T08:31:17.387026Z","steps":["trace[187016627] 'process raft request'  (duration: 221.663601ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T08:31:17.387147Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"215.435401ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2024-04-01T08:31:17.387229Z","caller":"traceutil/trace.go:171","msg":"trace[515249186] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:27502; }","duration":"215.539101ms","start":"2024-04-01T08:31:17.171677Z","end":"2024-04-01T08:31:17.387216Z","steps":["trace[515249186] 'agreement among raft nodes before linearized reading'  (duration: 215.384901ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:31:49.900083Z","caller":"traceutil/trace.go:171","msg":"trace[682034933] transaction","detail":"{read_only:false; response_revision:27530; number_of_response:1; }","duration":"161.042095ms","start":"2024-04-01T08:31:49.739012Z","end":"2024-04-01T08:31:49.900054Z","steps":["trace[682034933] 'process raft request'  (duration: 160.831695ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T08:32:01.555292Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.725285ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028212287039871 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:27531 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128028212287039869 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2024-04-01T08:32:01.557246Z","caller":"traceutil/trace.go:171","msg":"trace[2011965707] transaction","detail":"{read_only:false; response_revision:27539; number_of_response:1; }","duration":"196.884277ms","start":"2024-04-01T08:32:01.360135Z","end":"2024-04-01T08:32:01.557019Z","steps":["trace[2011965707] 'process raft request'  (duration: 69.652892ms)","trace[2011965707] 'compare'  (duration: 119.846086ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:32:10.503336Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27305}
{"level":"info","ts":"2024-04-01T08:32:10.504404Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27305,"took":"859.3µs","hash":2317901635}
{"level":"info","ts":"2024-04-01T08:32:10.504452Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2317901635,"revision":27305,"compact-revision":27066}
{"level":"info","ts":"2024-04-01T08:33:10.51145Z","caller":"traceutil/trace.go:171","msg":"trace[719830057] transaction","detail":"{read_only:false; response_revision:27595; number_of_response:1; }","duration":"307.798714ms","start":"2024-04-01T08:33:10.203633Z","end":"2024-04-01T08:33:10.511432Z","steps":["trace[719830057] 'process raft request'  (duration: 307.650214ms)"],"step_count":1}
{"level":"warn","ts":"2024-04-01T08:33:10.51158Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-04-01T08:33:10.203609Z","time spent":"307.892214ms","remote":"127.0.0.1:50492","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":520,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:27587 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:471 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2024-04-01T08:33:21.046988Z","caller":"traceutil/trace.go:171","msg":"trace[2019474327] linearizableReadLoop","detail":"{readStateIndex:34476; appliedIndex:34475; }","duration":"149.3645ms","start":"2024-04-01T08:33:20.897577Z","end":"2024-04-01T08:33:21.046942Z","steps":["trace[2019474327] 'read index received'  (duration: 149.0252ms)","trace[2019474327] 'applied index is now lower than readState.Index'  (duration: 336.7µs)"],"step_count":2}
{"level":"warn","ts":"2024-04-01T08:33:21.04737Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"149.8897ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1111"}
{"level":"info","ts":"2024-04-01T08:33:21.047496Z","caller":"traceutil/trace.go:171","msg":"trace[1163471411] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:27603; }","duration":"150.0345ms","start":"2024-04-01T08:33:20.897422Z","end":"2024-04-01T08:33:21.047456Z","steps":["trace[1163471411] 'agreement among raft nodes before linearized reading'  (duration: 149.741ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:33:21.048467Z","caller":"traceutil/trace.go:171","msg":"trace[116689239] transaction","detail":"{read_only:false; response_revision:27603; number_of_response:1; }","duration":"219.142001ms","start":"2024-04-01T08:33:20.829281Z","end":"2024-04-01T08:33:21.048423Z","steps":["trace[116689239] 'process raft request'  (duration: 217.404801ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:37:10.515099Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27547}
{"level":"info","ts":"2024-04-01T08:37:10.516839Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27547,"took":"1.369ms","hash":3611553937}
{"level":"info","ts":"2024-04-01T08:37:10.516954Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3611553937,"revision":27547,"compact-revision":27305}
{"level":"info","ts":"2024-04-01T08:37:58.167462Z","caller":"traceutil/trace.go:171","msg":"trace[855564245] transaction","detail":"{read_only:false; response_revision:27877; number_of_response:1; }","duration":"146.551213ms","start":"2024-04-01T08:37:58.020889Z","end":"2024-04-01T08:37:58.167441Z","steps":["trace[855564245] 'process raft request'  (duration: 146.367913ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:42:10.527566Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27829}
{"level":"info","ts":"2024-04-01T08:42:10.52957Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":27829,"took":"1.543399ms","hash":2887437288}
{"level":"info","ts":"2024-04-01T08:42:10.529679Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2887437288,"revision":27829,"compact-revision":27547}
{"level":"info","ts":"2024-04-01T08:46:36.737374Z","caller":"traceutil/trace.go:171","msg":"trace[1447432191] transaction","detail":"{read_only:false; response_revision:28528; number_of_response:1; }","duration":"150.38312ms","start":"2024-04-01T08:46:36.586964Z","end":"2024-04-01T08:46:36.737347Z","steps":["trace[1447432191] 'process raft request'  (duration: 127.234117ms)","trace[1447432191] 'compare'  (duration: 22.972803ms)"],"step_count":2}
{"level":"info","ts":"2024-04-01T08:47:10.536395Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28266}
{"level":"info","ts":"2024-04-01T08:47:10.538013Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":28266,"took":"1.337ms","hash":1619578429}
{"level":"info","ts":"2024-04-01T08:47:10.538082Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1619578429,"revision":28266,"compact-revision":27829}
{"level":"info","ts":"2024-04-01T08:49:16.678831Z","caller":"traceutil/trace.go:171","msg":"trace[606729816] transaction","detail":"{read_only:false; response_revision:28657; number_of_response:1; }","duration":"156.784844ms","start":"2024-04-01T08:49:16.522007Z","end":"2024-04-01T08:49:16.678791Z","steps":["trace[606729816] 'process raft request'  (duration: 156.277145ms)"],"step_count":1}
{"level":"info","ts":"2024-04-01T08:52:10.547111Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28555}
{"level":"info","ts":"2024-04-01T08:52:10.548664Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":28555,"took":"1.1709ms","hash":1926007531}
{"level":"info","ts":"2024-04-01T08:52:10.548775Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1926007531,"revision":28555,"compact-revision":28266}

* 
* ==> kernel <==
*  08:56:19 up  6:28,  0 users,  load average: 0.86, 0.95, 1.24
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [6f4e6cd32d17] <==
* I0401 06:17:16.948885       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0401 06:17:17.130554       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0401 06:17:17.236867       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0401 06:17:28.536693       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0401 06:17:28.546455       1 controller.go:624] quota admission added evaluator for: endpoints
I0401 06:19:27.510499       1 trace.go:236] Trace[1161842344]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (01-Apr-2024 06:19:26.154) (total time: 1355ms):
Trace[1161842344]: ---"Transaction prepared" 1339ms (06:19:27.496)
Trace[1161842344]: [1.355450437s] [1.355450437s] END
I0401 06:19:27.518403       1 trace.go:236] Trace[1391334540]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:c3b0711a-a8b7-4df0-ac3d-a49734cb7bf5,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (01-Apr-2024 06:19:26.920) (total time: 597ms):
Trace[1391334540]: ["GuaranteedUpdate etcd3" audit-id:c3b0711a-a8b7-4df0-ac3d-a49734cb7bf5,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 597ms (06:19:26.921)
Trace[1391334540]:  ---"Txn call completed" 592ms (06:19:27.514)]
Trace[1391334540]: [597.465337ms] [597.465337ms] END
I0401 06:20:26.253240       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 06:20:26.382080       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 06:21:38.636102       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0401 06:21:38.686390       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.100.140.139"}
I0401 06:32:46.427830       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 06:32:46.475620       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 06:33:03.258827       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.107.193.194"}
I0401 06:37:06.530308       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 06:37:06.624078       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 06:37:21.044653       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.110.34.253"}
I0401 06:38:46.548627       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 06:38:46.579484       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 06:39:55.071876       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.104.124.168"}
I0401 06:54:26.855205       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 06:54:26.882288       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 06:54:57.491822       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.100.230.60"}
I0401 07:03:16.927848       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 07:03:17.041313       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 07:30:41.854824       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.104.162.44"}
I0401 07:51:57.360063       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 07:51:57.381335       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 07:52:18.776209       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.109.42.199"}
I0401 07:57:38.401127       1 trace.go:236] Trace[1583861348]: "Get" accept:application/json, */*,audit-id:4a817850-c559-4e28-aac5-fca072a8cc2e,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (01-Apr-2024 07:57:37.543) (total time: 857ms):
Trace[1583861348]: ---"About to write a response" 857ms (07:57:38.400)
Trace[1583861348]: [857.190886ms] [857.190886ms] END
I0401 07:57:44.808885       1 trace.go:236] Trace[2670574]: "Get" accept:application/json, */*,audit-id:0a0f5854-79d3-45bd-82d3-9d66b12f3ea1,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (01-Apr-2024 07:57:40.423) (total time: 778ms):
Trace[2670574]: ---"About to write a response" 778ms (07:57:44.808)
Trace[2670574]: [778.919487ms] [778.919487ms] END
I0401 08:13:41.242791       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:13:41.393066       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:14:09.070989       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.105.230.84"}
I0401 08:16:31.233400       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:16:31.370758       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:35:11.313613       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:35:11.334650       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:35:48.061731       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.99.124.127"}
I0401 08:38:51.289729       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:38:51.306518       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:39:17.228894       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.104.153.83"}
I0401 08:41:01.307517       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:41:01.322620       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:41:24.911597       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.106.133.148"}
I0401 08:42:01.335306       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:42:01.348990       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:42:50.917127       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.99.178.129"}
I0401 08:54:01.372537       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0401 08:54:01.385678       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0401 08:55:56.684511       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.105.225.3"}

* 
* ==> kube-apiserver [9ee4f6b9ca04] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.129696       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.147749       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.149017       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.156376       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.182258       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.211341       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 06:16:31.224364       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [1f3fa48a9c3a] <==
* I0401 06:16:02.230005       1 serving.go:348] Generated self-signed cert in-memory
I0401 06:16:07.120249       1 controllermanager.go:189] "Starting" version="v1.28.3"
I0401 06:16:07.120459       1 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0401 06:16:07.135943       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0401 06:16:07.136774       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0401 06:16:07.143295       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0401 06:16:07.145986       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"

* 
* ==> kube-controller-manager [ce5de01a2961] <==
* I0401 08:51:32.919594       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:51:47.920674       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:51:47.921015       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:52:02.921116       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:52:02.921812       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:52:17.922148       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:52:17.922937       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:52:32.922567       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:52:32.922734       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:52:47.924070       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:52:47.924454       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:53:02.924346       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:53:02.925447       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:53:17.926110       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:53:17.926777       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:53:32.926283       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:53:32.926509       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:53:47.927128       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:53:47.927311       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
I0401 08:53:58.164652       1 event.go:307] "Event occurred" object="default/fast-api-deployment-6846896678" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fast-api-deployment-6846896678-2rzlg"
I0401 08:53:58.178887       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="33.362104ms"
I0401 08:53:58.179036       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="93.7µs"
I0401 08:53:58.194759       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="14.671702ms"
I0401 08:53:58.194897       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="72µs"
I0401 08:53:58.207374       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="127.4µs"
I0401 08:53:58.587487       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="51.4µs"
I0401 08:53:59.205107       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="83.9µs"
I0401 08:53:59.225054       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="99.8µs"
I0401 08:53:59.236621       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="143.001µs"
I0401 08:53:59.243409       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="55.4µs"
I0401 08:53:59.255874       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="56.4µs"
I0401 08:54:02.212409       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-6846896678" duration="6.7µs"
E0401 08:54:02.928320       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:54:02.928544       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:54:17.929963       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:54:17.930784       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:54:32.930342       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:54:32.931871       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:54:47.931498       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:54:47.934195       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
I0401 08:55:02.931617       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:55:02.931626       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
E0401 08:55:17.932364       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:55:17.932502       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:55:32.933320       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:55:32.933911       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:55:47.933585       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:55:47.933754       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
I0401 08:55:56.657205       1 event.go:307] "Event occurred" object="default/fast-api-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set fast-api-deployment-bdc468646 to 1"
I0401 08:55:56.681197       1 event.go:307] "Event occurred" object="default/fast-api-deployment-bdc468646" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fast-api-deployment-bdc468646-s9q2x"
I0401 08:55:56.695961       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-bdc468646" duration="38.547102ms"
I0401 08:55:56.716487       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-bdc468646" duration="20.403101ms"
I0401 08:55:56.716612       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-bdc468646" duration="66.1µs"
I0401 08:55:56.716888       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-bdc468646" duration="65.9µs"
I0401 08:55:56.726390       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-bdc468646" duration="49.6µs"
I0401 08:55:58.134945       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-bdc468646" duration="51.9µs"
E0401 08:56:02.933775       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:56:02.934076       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"
E0401 08:56:17.933903       1 pv_controller.go:1562] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" PVC="default/data-volume-claim"
I0401 08:56:17.934030       1 event.go:307] "Event occurred" object="default/data-volume-claim" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Warning" reason="ProvisioningFailed" message="storageclass.storage.k8s.io \"manual\" not found"

* 
* ==> kube-proxy [0a711f0b1ec9] <==
* I0401 06:15:43.228870       1 server_others.go:69] "Using iptables proxy"
E0401 06:15:43.240761       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0401 06:15:44.436963       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0401 06:15:46.644439       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
E0401 06:16:01.828696       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": net/http: TLS handshake timeout
I0401 06:16:16.967496       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0401 06:16:20.435052       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0401 06:16:20.734833       1 server_others.go:152] "Using iptables Proxier"
I0401 06:16:20.735065       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0401 06:16:20.735139       1 server_others.go:438] "Defaulting to no-op detect-local"
I0401 06:16:20.736106       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0401 06:16:20.745194       1 server.go:846] "Version info" version="v1.28.3"
I0401 06:16:20.745298       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0401 06:16:20.768796       1 config.go:188] "Starting service config controller"
I0401 06:16:20.769783       1 shared_informer.go:311] Waiting for caches to sync for service config
I0401 06:16:20.771699       1 config.go:315] "Starting node config controller"
I0401 06:16:20.771858       1 shared_informer.go:311] Waiting for caches to sync for node config
I0401 06:16:20.774984       1 config.go:97] "Starting endpoint slice config controller"
I0401 06:16:20.863345       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0401 06:16:20.863472       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0401 06:16:20.916607       1 shared_informer.go:318] Caches are synced for service config
I0401 06:16:20.926345       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [63dfa62cfbd9] <==
* I0401 06:17:35.018927       1 server_others.go:69] "Using iptables proxy"
I0401 06:17:35.337042       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0401 06:17:35.763491       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0401 06:17:35.836186       1 server_others.go:152] "Using iptables Proxier"
I0401 06:17:35.836403       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0401 06:17:35.836496       1 server_others.go:438] "Defaulting to no-op detect-local"
I0401 06:17:35.836695       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0401 06:17:35.838451       1 server.go:846] "Version info" version="v1.28.3"
I0401 06:17:35.838555       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0401 06:17:35.846979       1 config.go:188] "Starting service config controller"
I0401 06:17:35.847063       1 shared_informer.go:311] Waiting for caches to sync for service config
I0401 06:17:35.847212       1 config.go:97] "Starting endpoint slice config controller"
I0401 06:17:35.847255       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0401 06:17:35.858089       1 config.go:315] "Starting node config controller"
I0401 06:17:35.858165       1 shared_informer.go:311] Waiting for caches to sync for node config
I0401 06:17:35.947268       1 shared_informer.go:318] Caches are synced for service config
I0401 06:17:35.947663       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0401 06:17:35.960088       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [69630a57be5c] <==
* I0401 06:16:03.075515       1 serving.go:348] Generated self-signed cert in-memory
I0401 06:16:17.534223       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0401 06:16:17.534399       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0401 06:16:17.724245       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0401 06:16:17.726911       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0401 06:16:17.734589       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0401 06:16:17.735332       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0401 06:16:17.738831       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0401 06:16:17.738910       1 shared_informer.go:311] Waiting for caches to sync for RequestHeaderAuthRequestController
I0401 06:16:17.759262       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0401 06:16:17.759700       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0401 06:16:17.928514       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0401 06:16:17.936444       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0401 06:16:17.939572       1 shared_informer.go:318] Caches are synced for RequestHeaderAuthRequestController
E0401 06:16:25.928408       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [dae2c55155e1] <==
* I0401 06:17:02.175200       1 serving.go:348] Generated self-signed cert in-memory
W0401 06:17:13.417824       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0401 06:17:13.417972       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0401 06:17:13.418053       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0401 06:17:13.418118       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0401 06:17:14.047462       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0401 06:17:14.047635       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0401 06:17:14.063483       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0401 06:17:14.063607       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0401 06:17:14.073467       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0401 06:17:14.074293       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0401 06:17:14.168530       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Apr 01 08:50:04 minikube kubelet[177422]: E0401 08:50:04.027917  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:50:19 minikube kubelet[177422]: E0401 08:50:19.059748  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:50:19 minikube kubelet[177422]: E0401 08:50:19.060115  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:50:34 minikube kubelet[177422]: E0401 08:50:34.027753  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:50:34 minikube kubelet[177422]: E0401 08:50:34.027837  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:50:45 minikube kubelet[177422]: E0401 08:50:45.040180  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:50:45 minikube kubelet[177422]: E0401 08:50:45.042237  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:50:56 minikube kubelet[177422]: E0401 08:50:56.031644  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:50:56 minikube kubelet[177422]: E0401 08:50:56.033986  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:51:08 minikube kubelet[177422]: E0401 08:51:08.031052  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:51:08 minikube kubelet[177422]: E0401 08:51:08.031337  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:51:19 minikube kubelet[177422]: E0401 08:51:19.031366  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:51:19 minikube kubelet[177422]: E0401 08:51:19.031492  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:51:30 minikube kubelet[177422]: E0401 08:51:30.028599  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:51:30 minikube kubelet[177422]: E0401 08:51:30.028691  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:51:44 minikube kubelet[177422]: E0401 08:51:44.033240  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:51:44 minikube kubelet[177422]: E0401 08:51:44.033435  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:51:46 minikube kubelet[177422]: W0401 08:51:46.347041  177422 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Apr 01 08:51:56 minikube kubelet[177422]: E0401 08:51:56.028132  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:51:56 minikube kubelet[177422]: E0401 08:51:56.029146  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:52:10 minikube kubelet[177422]: E0401 08:52:10.044694  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:52:10 minikube kubelet[177422]: E0401 08:52:10.053170  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:52:23 minikube kubelet[177422]: E0401 08:52:23.045873  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:52:23 minikube kubelet[177422]: E0401 08:52:23.045972  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:52:37 minikube kubelet[177422]: E0401 08:52:37.027497  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:52:37 minikube kubelet[177422]: E0401 08:52:37.036475  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:52:50 minikube kubelet[177422]: E0401 08:52:50.030032  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:52:50 minikube kubelet[177422]: E0401 08:52:50.030153  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:53:01 minikube kubelet[177422]: E0401 08:53:01.029435  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:01 minikube kubelet[177422]: E0401 08:53:01.031635  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:53:15 minikube kubelet[177422]: E0401 08:53:15.041187  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:15 minikube kubelet[177422]: E0401 08:53:15.045455  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:53:26 minikube kubelet[177422]: E0401 08:53:26.035224  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:26 minikube kubelet[177422]: E0401 08:53:26.038348  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:53:38 minikube kubelet[177422]: E0401 08:53:38.028615  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:38 minikube kubelet[177422]: E0401 08:53:38.028776  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:53:51 minikube kubelet[177422]: E0401 08:53:51.043699  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52qjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-b4vwd_default(341c525d-a965-4d47-8634-6970f5cd70ac): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:51 minikube kubelet[177422]: E0401 08:53:51.044347  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-b4vwd" podUID="341c525d-a965-4d47-8634-6970f5cd70ac"
Apr 01 08:53:58 minikube kubelet[177422]: I0401 08:53:58.180292  177422 topology_manager.go:215] "Topology Admit Handler" podUID="62a436e2-1386-4812-8bc5-316c8e98ef84" podNamespace="default" podName="fast-api-deployment-6846896678-2rzlg"
Apr 01 08:53:58 minikube kubelet[177422]: I0401 08:53:58.194692  177422 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5vtdf\" (UniqueName: \"kubernetes.io/projected/62a436e2-1386-4812-8bc5-316c8e98ef84-kube-api-access-5vtdf\") pod \"fast-api-deployment-6846896678-2rzlg\" (UID: \"62a436e2-1386-4812-8bc5-316c8e98ef84\") " pod="default/fast-api-deployment-6846896678-2rzlg"
Apr 01 08:53:58 minikube kubelet[177422]: I0401 08:53:58.599626  177422 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-52qjv\" (UniqueName: \"kubernetes.io/projected/341c525d-a965-4d47-8634-6970f5cd70ac-kube-api-access-52qjv\") pod \"341c525d-a965-4d47-8634-6970f5cd70ac\" (UID: \"341c525d-a965-4d47-8634-6970f5cd70ac\") "
Apr 01 08:53:58 minikube kubelet[177422]: I0401 08:53:58.613438  177422 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/341c525d-a965-4d47-8634-6970f5cd70ac-kube-api-access-52qjv" (OuterVolumeSpecName: "kube-api-access-52qjv") pod "341c525d-a965-4d47-8634-6970f5cd70ac" (UID: "341c525d-a965-4d47-8634-6970f5cd70ac"). InnerVolumeSpecName "kube-api-access-52qjv". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 01 08:53:58 minikube kubelet[177422]: I0401 08:53:58.700754  177422 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-52qjv\" (UniqueName: \"kubernetes.io/projected/341c525d-a965-4d47-8634-6970f5cd70ac-kube-api-access-52qjv\") on node \"minikube\" DevicePath \"\""
Apr 01 08:53:58 minikube kubelet[177422]: E0401 08:53:58.921895  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vtdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-2rzlg_default(62a436e2-1386-4812-8bc5-316c8e98ef84): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:58 minikube kubelet[177422]: E0401 08:53:58.921976  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-2rzlg" podUID="62a436e2-1386-4812-8bc5-316c8e98ef84"
Apr 01 08:53:59 minikube kubelet[177422]: E0401 08:53:59.213718  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5vtdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-6846896678-2rzlg_default(62a436e2-1386-4812-8bc5-316c8e98ef84): ErrImageNeverPull: Container image "miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:53:59 minikube kubelet[177422]: E0401 08:53:59.213808  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-6846896678-2rzlg" podUID="62a436e2-1386-4812-8bc5-316c8e98ef84"
Apr 01 08:54:01 minikube kubelet[177422]: I0401 08:54:01.060236  177422 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="341c525d-a965-4d47-8634-6970f5cd70ac" path="/var/lib/kubelet/pods/341c525d-a965-4d47-8634-6970f5cd70ac/volumes"
Apr 01 08:54:02 minikube kubelet[177422]: I0401 08:54:02.645800  177422 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5vtdf\" (UniqueName: \"kubernetes.io/projected/62a436e2-1386-4812-8bc5-316c8e98ef84-kube-api-access-5vtdf\") pod \"62a436e2-1386-4812-8bc5-316c8e98ef84\" (UID: \"62a436e2-1386-4812-8bc5-316c8e98ef84\") "
Apr 01 08:54:02 minikube kubelet[177422]: I0401 08:54:02.651192  177422 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/62a436e2-1386-4812-8bc5-316c8e98ef84-kube-api-access-5vtdf" (OuterVolumeSpecName: "kube-api-access-5vtdf") pod "62a436e2-1386-4812-8bc5-316c8e98ef84" (UID: "62a436e2-1386-4812-8bc5-316c8e98ef84"). InnerVolumeSpecName "kube-api-access-5vtdf". PluginName "kubernetes.io/projected", VolumeGidValue ""
Apr 01 08:54:02 minikube kubelet[177422]: I0401 08:54:02.746481  177422 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-5vtdf\" (UniqueName: \"kubernetes.io/projected/62a436e2-1386-4812-8bc5-316c8e98ef84-kube-api-access-5vtdf\") on node \"minikube\" DevicePath \"\""
Apr 01 08:54:05 minikube kubelet[177422]: I0401 08:54:05.045666  177422 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="62a436e2-1386-4812-8bc5-316c8e98ef84" path="/var/lib/kubelet/pods/62a436e2-1386-4812-8bc5-316c8e98ef84/volumes"
Apr 01 08:55:56 minikube kubelet[177422]: I0401 08:55:56.703841  177422 topology_manager.go:215] "Topology Admit Handler" podUID="5cb3e6d4-95ec-43e6-b70e-ec42b682d71c" podNamespace="default" podName="fast-api-deployment-bdc468646-s9q2x"
Apr 01 08:55:56 minikube kubelet[177422]: I0401 08:55:56.812460  177422 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-b4qmb\" (UniqueName: \"kubernetes.io/projected/5cb3e6d4-95ec-43e6-b70e-ec42b682d71c-kube-api-access-b4qmb\") pod \"fast-api-deployment-bdc468646-s9q2x\" (UID: \"5cb3e6d4-95ec-43e6-b70e-ec42b682d71c\") " pod="default/fast-api-deployment-bdc468646-s9q2x"
Apr 01 08:55:57 minikube kubelet[177422]: E0401 08:55:57.342448  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:jaruda/miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4qmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-bdc468646-s9q2x_default(5cb3e6d4-95ec-43e6-b70e-ec42b682d71c): ErrImageNeverPull: Container image "jaruda/miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:55:57 minikube kubelet[177422]: E0401 08:55:57.342523  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"jaruda/miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-bdc468646-s9q2x" podUID="5cb3e6d4-95ec-43e6-b70e-ec42b682d71c"
Apr 01 08:55:58 minikube kubelet[177422]: E0401 08:55:58.117493  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:jaruda/miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4qmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-bdc468646-s9q2x_default(5cb3e6d4-95ec-43e6-b70e-ec42b682d71c): ErrImageNeverPull: Container image "jaruda/miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:55:58 minikube kubelet[177422]: E0401 08:55:58.119497  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"jaruda/miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-bdc468646-s9q2x" podUID="5cb3e6d4-95ec-43e6-b70e-ec42b682d71c"
Apr 01 08:56:12 minikube kubelet[177422]: E0401 08:56:12.030589  177422 kuberuntime_manager.go:1256] container &Container{Name:fast-api,Image:jaruda/miraen-eng-eval-engine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Requests:ResourceList{cpu: {{500 -3} {<nil>} 500m DecimalSI},memory: {{268435456 0} {<nil>}  BinarySI},},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b4qmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Never,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod fast-api-deployment-bdc468646-s9q2x_default(5cb3e6d4-95ec-43e6-b70e-ec42b682d71c): ErrImageNeverPull: Container image "jaruda/miraen-eng-eval-engine" is not present with pull policy of Never
Apr 01 08:56:12 minikube kubelet[177422]: E0401 08:56:12.030731  177422 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"fast-api\" with ErrImageNeverPull: \"Container image \\\"jaruda/miraen-eng-eval-engine\\\" is not present with pull policy of Never\"" pod="default/fast-api-deployment-bdc468646-s9q2x" podUID="5cb3e6d4-95ec-43e6-b70e-ec42b682d71c"

* 
* ==> storage-provisioner [9ffa6f580ea0] <==
* I0401 06:17:55.918417       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0401 06:17:55.950586       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0401 06:17:55.950704       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0401 06:18:13.459271       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0401 06:18:13.460309       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_d84808d0-19c5-4b3a-8d6e-5e166b2c4707!
I0401 06:18:13.481453       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e3f83578-b340-4432-b14f-74833c2215da", APIVersion:"v1", ResourceVersion:"20577", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_d84808d0-19c5-4b3a-8d6e-5e166b2c4707 became leader
I0401 06:18:13.660800       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_d84808d0-19c5-4b3a-8d6e-5e166b2c4707!

* 
* ==> storage-provisioner [f1ee1a0b6484] <==
* I0401 06:17:34.857977       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0401 06:17:34.879024       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

