* 
* ==> Audit <==
* |---------|-----------------------|----------|------|---------|---------------------|---------------------|
| Command |         Args          | Profile  | User | Version |     Start Time      |      End Time       |
|---------|-----------------------|----------|------|---------|---------------------|---------------------|
| start   |                       | minikube | lee  | v1.32.0 | 29 Mar 24 14:17 KST | 29 Mar 24 14:23 KST |
| service | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:23 KST |                     |
| service | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:34 KST |                     |
| service | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:34 KST |                     |
| service | fastapi-service --url | minikube | lee  | v1.32.0 | 29 Mar 24 16:39 KST |                     |
| start   | --driver=docker       | minikube | lee  | v1.32.0 | 29 Mar 24 16:47 KST | 29 Mar 24 16:50 KST |
| service | fast-api-service      | minikube | lee  | v1.32.0 | 29 Mar 24 17:14 KST |                     |
|---------|-----------------------|----------|------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/03/29 16:47:38
Running on machine: DESKTOP-1T299N9
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0329 16:47:38.906256  292670 out.go:296] Setting OutFile to fd 1 ...
I0329 16:47:38.906922  292670 out.go:348] isatty.IsTerminal(1) = true
I0329 16:47:38.906931  292670 out.go:309] Setting ErrFile to fd 2...
I0329 16:47:38.906941  292670 out.go:348] isatty.IsTerminal(2) = true
I0329 16:47:38.907193  292670 root.go:338] Updating PATH: /home/lee/.minikube/bin
W0329 16:47:38.907391  292670 root.go:314] Error reading config file at /home/lee/.minikube/config/config.json: open /home/lee/.minikube/config/config.json: no such file or directory
I0329 16:47:38.908562  292670 out.go:303] Setting JSON to false
I0329 16:47:38.916196  292670 start.go:128] hostinfo: {"hostname":"DESKTOP-1T299N9","uptime":58326,"bootTime":1711640133,"procs":82,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"089e2fbe-8854-471b-a5fd-c5dea93fbac9"}
I0329 16:47:38.916284  292670 start.go:138] virtualization:  guest
I0329 16:47:38.920785  292670 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04 (amd64)
I0329 16:47:38.930773  292670 notify.go:220] Checking for updates...
I0329 16:47:38.931202  292670 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0329 16:47:38.934616  292670 driver.go:378] Setting default libvirt URI to qemu:///system
I0329 16:47:38.997021  292670 docker.go:122] docker version: linux-25.0.4:Docker Engine - Community
I0329 16:47:38.997638  292670 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0329 16:47:40.164883  292670 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.167179713s)
I0329 16:47:40.166698  292670 info.go:266] docker info: {ID:e3db2e71-b1ea-4230-a673-d2dbcdaa6727 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:31 OomKillDisable:true NGoroutines:56 SystemTime:2024-03-29 16:47:40.130971035 +0900 KST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4035866624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-1T299N9 Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0329 16:47:40.167504  292670 docker.go:295] overlay module found
I0329 16:47:40.171353  292670 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0329 16:47:40.174664  292670 start.go:298] selected driver: docker
I0329 16:47:40.175369  292670 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0329 16:47:40.175659  292670 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0329 16:47:40.177455  292670 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0329 16:47:40.458817  292670 info.go:266] docker info: {ID:e3db2e71-b1ea-4230-a673-d2dbcdaa6727 Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:31 OomKillDisable:true NGoroutines:56 SystemTime:2024-03-29 16:47:40.429005738 +0900 KST LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:4035866624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:DESKTOP-1T299N9 Labels:[] ExperimentalBuild:false ServerVersion:25.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.13.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.7]] Warnings:<nil>}}
I0329 16:47:40.462651  292670 cni.go:84] Creating CNI manager for ""
I0329 16:47:40.462711  292670 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 16:47:40.462744  292670 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0329 16:47:40.467573  292670 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0329 16:47:40.473454  292670 cache.go:121] Beginning downloading kic base image for docker with docker
I0329 16:47:40.477660  292670 out.go:177] üöú  Pulling base image ...
I0329 16:47:40.482396  292670 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0329 16:47:40.482507  292670 preload.go:148] Found local preload: /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0329 16:47:40.482946  292670 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0329 16:47:40.483100  292670 cache.go:56] Caching tarball of preloaded images
I0329 16:47:40.484544  292670 preload.go:174] Found /home/lee/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0329 16:47:40.484582  292670 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0329 16:47:40.484832  292670 profile.go:148] Saving config to /home/lee/.minikube/profiles/minikube/config.json ...
I0329 16:47:40.533188  292670 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0329 16:47:40.533672  292670 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0329 16:47:40.533761  292670 cache.go:194] Successfully downloaded all kic artifacts
I0329 16:47:40.534973  292670 start.go:365] acquiring machines lock for minikube: {Name:mk89aad7abaa74b5ceac4179cf674336d941d2b1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0329 16:47:40.535190  292670 start.go:369] acquired machines lock for "minikube" in 148.3¬µs
I0329 16:47:40.535735  292670 start.go:96] Skipping create...Using existing machine configuration
I0329 16:47:40.536242  292670 fix.go:54] fixHost starting: 
I0329 16:47:40.537201  292670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 16:47:40.589720  292670 fix.go:102] recreateIfNeeded on minikube: state=Running err=<nil>
W0329 16:47:40.589758  292670 fix.go:128] unexpected machine state, will restart: <nil>
I0329 16:47:40.594076  292670 out.go:177] üèÉ  Updating the running docker "minikube" container ...
I0329 16:47:40.597665  292670 machine.go:88] provisioning docker machine ...
I0329 16:47:40.598667  292670 ubuntu.go:169] provisioning hostname "minikube"
I0329 16:47:40.598921  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:40.663814  292670 main.go:141] libmachine: Using SSH client type: native
I0329 16:47:40.665047  292670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 16:47:40.665068  292670 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0329 16:47:41.072452  292670 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0329 16:47:41.073662  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:41.236223  292670 main.go:141] libmachine: Using SSH client type: native
I0329 16:47:41.237423  292670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 16:47:41.237466  292670 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0329 16:47:42.042454  292670 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0329 16:47:42.043368  292670 ubuntu.go:175] set auth options {CertDir:/home/lee/.minikube CaCertPath:/home/lee/.minikube/certs/ca.pem CaPrivateKeyPath:/home/lee/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/lee/.minikube/machines/server.pem ServerKeyPath:/home/lee/.minikube/machines/server-key.pem ClientKeyPath:/home/lee/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/lee/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/lee/.minikube}
I0329 16:47:42.043633  292670 ubuntu.go:177] setting up certificates
I0329 16:47:42.044392  292670 provision.go:83] configureAuth start
I0329 16:47:42.044803  292670 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 16:47:42.413532  292670 provision.go:138] copyHostCerts
I0329 16:47:42.414072  292670 exec_runner.go:144] found /home/lee/.minikube/cert.pem, removing ...
I0329 16:47:42.414144  292670 exec_runner.go:203] rm: /home/lee/.minikube/cert.pem
I0329 16:47:42.414249  292670 exec_runner.go:151] cp: /home/lee/.minikube/certs/cert.pem --> /home/lee/.minikube/cert.pem (1111 bytes)
I0329 16:47:42.414423  292670 exec_runner.go:144] found /home/lee/.minikube/key.pem, removing ...
I0329 16:47:42.414429  292670 exec_runner.go:203] rm: /home/lee/.minikube/key.pem
I0329 16:47:42.414483  292670 exec_runner.go:151] cp: /home/lee/.minikube/certs/key.pem --> /home/lee/.minikube/key.pem (1679 bytes)
I0329 16:47:42.414692  292670 exec_runner.go:144] found /home/lee/.minikube/ca.pem, removing ...
I0329 16:47:42.414698  292670 exec_runner.go:203] rm: /home/lee/.minikube/ca.pem
I0329 16:47:42.414782  292670 exec_runner.go:151] cp: /home/lee/.minikube/certs/ca.pem --> /home/lee/.minikube/ca.pem (1070 bytes)
I0329 16:47:42.415212  292670 provision.go:112] generating server cert: /home/lee/.minikube/machines/server.pem ca-key=/home/lee/.minikube/certs/ca.pem private-key=/home/lee/.minikube/certs/ca-key.pem org=lee.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0329 16:47:43.231392  292670 provision.go:172] copyRemoteCerts
I0329 16:47:43.231503  292670 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0329 16:47:43.231571  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:43.265923  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:47:43.409765  292670 ssh_runner.go:362] scp /home/lee/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0329 16:47:43.492599  292670 ssh_runner.go:362] scp /home/lee/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0329 16:47:43.565578  292670 ssh_runner.go:362] scp /home/lee/.minikube/machines/server.pem --> /etc/docker/server.pem (1192 bytes)
I0329 16:47:43.632210  292670 provision.go:86] duration metric: configureAuth took 1.587361136s
I0329 16:47:43.632253  292670 ubuntu.go:193] setting minikube options for container-runtime
I0329 16:47:43.632550  292670 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0329 16:47:43.632640  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:43.843246  292670 main.go:141] libmachine: Using SSH client type: native
I0329 16:47:43.843954  292670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 16:47:43.843969  292670 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0329 16:47:44.109773  292670 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0329 16:47:44.109903  292670 ubuntu.go:71] root file system type: overlay
I0329 16:47:44.111266  292670 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0329 16:47:44.111875  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:44.252571  292670 main.go:141] libmachine: Using SSH client type: native
I0329 16:47:44.253515  292670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 16:47:44.253684  292670 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0329 16:47:44.559214  292670 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0329 16:47:44.560820  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:44.739897  292670 main.go:141] libmachine: Using SSH client type: native
I0329 16:47:44.740672  292670 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0329 16:47:44.740697  292670 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0329 16:47:45.206213  292670 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0329 16:47:45.206241  292670 machine.go:91] provisioned docker machine in 4.608554391s
I0329 16:47:45.219551  292670 start.go:300] post-start starting for "minikube" (driver="docker")
I0329 16:47:45.234978  292670 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0329 16:47:45.235312  292670 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0329 16:47:45.235663  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:45.290763  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:47:45.492001  292670 ssh_runner.go:195] Run: cat /etc/os-release
I0329 16:47:45.519091  292670 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0329 16:47:45.519291  292670 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0329 16:47:45.519367  292670 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0329 16:47:45.519399  292670 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0329 16:47:45.520868  292670 filesync.go:126] Scanning /home/lee/.minikube/addons for local assets ...
I0329 16:47:45.525175  292670 filesync.go:126] Scanning /home/lee/.minikube/files for local assets ...
I0329 16:47:45.526008  292670 start.go:303] post-start completed in 306.420216ms
I0329 16:47:45.526917  292670 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0329 16:47:45.527237  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:45.647803  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:47:45.927774  292670 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0329 16:47:45.984225  292670 fix.go:56] fixHost completed within 5.447967086s
I0329 16:47:45.984288  292670 start.go:83] releasing machines lock for "minikube", held for 5.449068187s
I0329 16:47:45.985329  292670 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0329 16:47:46.050791  292670 ssh_runner.go:195] Run: cat /version.json
I0329 16:47:46.050878  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:46.050951  292670 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0329 16:47:46.051765  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:47:46.102175  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:47:46.122063  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:47:46.237372  292670 ssh_runner.go:195] Run: systemctl --version
I0329 16:47:47.442503  292670 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.391439146s)
I0329 16:47:47.443335  292670 ssh_runner.go:235] Completed: systemctl --version: (1.205892486s)
I0329 16:47:47.443862  292670 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0329 16:47:47.512404  292670 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0329 16:47:47.849419  292670 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0329 16:47:47.850326  292670 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0329 16:47:47.900100  292670 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0329 16:47:47.900138  292670 start.go:472] detecting cgroup driver to use...
I0329 16:47:47.900221  292670 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0329 16:47:47.901995  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0329 16:47:47.959737  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0329 16:47:48.000386  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0329 16:47:48.042683  292670 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0329 16:47:48.042892  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0329 16:47:48.102334  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0329 16:47:48.146120  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0329 16:47:48.187317  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0329 16:47:48.227410  292670 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0329 16:47:48.270069  292670 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0329 16:47:48.308044  292670 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0329 16:47:48.337227  292670 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0329 16:47:48.369943  292670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 16:47:49.699239  292670 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.329205234s)
I0329 16:47:49.699405  292670 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0329 16:48:05.671005  292670 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (15.971483954s)
I0329 16:48:05.671198  292670 start.go:472] detecting cgroup driver to use...
I0329 16:48:05.671342  292670 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0329 16:48:05.672300  292670 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0329 16:48:05.880962  292670 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0329 16:48:05.881133  292670 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0329 16:48:05.949144  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0329 16:48:05.994399  292670 ssh_runner.go:195] Run: which cri-dockerd
I0329 16:48:06.003147  292670 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0329 16:48:06.069633  292670 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0329 16:48:06.281845  292670 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0329 16:48:07.281169  292670 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0329 16:48:08.234749  292670 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0329 16:48:08.235240  292670 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0329 16:48:08.442198  292670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 16:48:09.086733  292670 ssh_runner.go:195] Run: sudo systemctl restart docker
I0329 16:48:12.269593  292670 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.182806302s)
I0329 16:48:12.269774  292670 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0329 16:48:13.339011  292670 ssh_runner.go:235] Completed: sudo systemctl enable cri-docker.socket: (1.069015368s)
I0329 16:48:13.339821  292670 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0329 16:48:14.255105  292670 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0329 16:48:14.855330  292670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 16:48:15.611215  292670 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0329 16:48:15.677274  292670 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0329 16:48:16.169409  292670 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0329 16:48:16.993741  292670 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0329 16:48:16.993994  292670 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0329 16:48:17.012450  292670 start.go:540] Will wait 60s for crictl version
I0329 16:48:17.012579  292670 ssh_runner.go:195] Run: which crictl
I0329 16:48:17.038623  292670 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0329 16:48:18.473751  292670 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.435059855s)
I0329 16:48:18.474300  292670 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0329 16:48:18.474454  292670 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0329 16:48:18.688611  292670 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0329 16:48:19.364259  292670 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0329 16:48:19.367149  292670 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0329 16:48:19.576966  292670 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0329 16:48:19.655233  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0329 16:48:19.787318  292670 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0329 16:48:19.787870  292670 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0329 16:48:20.583614  292670 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0329 16:48:20.587977  292670 docker.go:601] Images already preloaded, skipping extraction
I0329 16:48:20.588913  292670 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0329 16:48:21.448132  292670 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0329 16:48:21.449843  292670 cache_images.go:84] Images are preloaded, skipping loading
I0329 16:48:21.451836  292670 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0329 16:48:34.473644  292670 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (13.021759384s)
I0329 16:48:34.474646  292670 cni.go:84] Creating CNI manager for ""
I0329 16:48:34.474674  292670 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 16:48:34.474989  292670 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0329 16:48:34.475873  292670 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0329 16:48:34.476211  292670 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0329 16:48:34.476439  292670 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0329 16:48:34.476583  292670 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0329 16:48:35.348987  292670 binaries.go:44] Found k8s binaries, skipping transfer
I0329 16:48:35.349361  292670 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0329 16:48:35.664669  292670 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0329 16:48:37.367454  292670 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0329 16:48:39.053064  292670 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0329 16:48:40.446313  292670 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0329 16:48:40.855926  292670 certs.go:56] Setting up /home/lee/.minikube/profiles/minikube for IP: 192.168.49.2
I0329 16:48:40.856540  292670 certs.go:190] acquiring lock for shared ca certs: {Name:mk1cf981783681b0bb5a6ac43e6863efc3a4fc0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 16:48:40.859015  292670 certs.go:199] skipping minikubeCA CA generation: /home/lee/.minikube/ca.key
I0329 16:48:40.863530  292670 certs.go:199] skipping proxyClientCA CA generation: /home/lee/.minikube/proxy-client-ca.key
I0329 16:48:40.869912  292670 certs.go:315] skipping minikube-user signed cert generation: /home/lee/.minikube/profiles/minikube/client.key
I0329 16:48:40.873809  292670 certs.go:315] skipping minikube signed cert generation: /home/lee/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0329 16:48:40.878712  292670 certs.go:315] skipping aggregator signed cert generation: /home/lee/.minikube/profiles/minikube/proxy-client.key
I0329 16:48:40.881711  292670 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/ca-key.pem (1679 bytes)
I0329 16:48:40.882301  292670 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/ca.pem (1070 bytes)
I0329 16:48:40.882565  292670 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/cert.pem (1111 bytes)
I0329 16:48:40.882792  292670 certs.go:437] found cert: /home/lee/.minikube/certs/home/lee/.minikube/certs/key.pem (1679 bytes)
I0329 16:48:40.899463  292670 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0329 16:48:43.343241  292670 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0329 16:48:46.675814  292670 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0329 16:48:48.863721  292670 ssh_runner.go:362] scp /home/lee/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0329 16:48:54.477651  292670 ssh_runner.go:362] scp /home/lee/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0329 16:48:56.455692  292670 ssh_runner.go:362] scp /home/lee/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0329 16:48:57.645387  292670 ssh_runner.go:362] scp /home/lee/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0329 16:48:58.674162  292670 ssh_runner.go:362] scp /home/lee/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0329 16:49:00.275148  292670 ssh_runner.go:362] scp /home/lee/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0329 16:49:01.278501  292670 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0329 16:49:02.081815  292670 ssh_runner.go:195] Run: openssl version
I0329 16:49:03.771887  292670 ssh_runner.go:235] Completed: openssl version: (1.690013857s)
I0329 16:49:03.772100  292670 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0329 16:49:04.351294  292670 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0329 16:49:04.565160  292670 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Mar 29 05:21 /usr/share/ca-certificates/minikubeCA.pem
I0329 16:49:04.565386  292670 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0329 16:49:04.931822  292670 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0329 16:49:05.677223  292670 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0329 16:49:05.837541  292670 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0329 16:49:06.354578  292670 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0329 16:49:07.066169  292670 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0329 16:49:07.330787  292670 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0329 16:49:07.485258  292670 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0329 16:49:07.558861  292670 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0329 16:49:07.947313  292670 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/lee:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0329 16:49:07.948108  292670 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0329 16:49:09.732800  292670 ssh_runner.go:235] Completed: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}: (1.784574977s)
I0329 16:49:09.734597  292670 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0329 16:49:10.175172  292670 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0329 16:49:10.176081  292670 kubeadm.go:636] restartCluster start
I0329 16:49:10.176559  292670 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0329 16:49:10.652816  292670 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0329 16:49:10.653533  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0329 16:49:10.816864  292670 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:32769"
I0329 16:49:10.822273  292670 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0329 16:49:11.259692  292670 api_server.go:166] Checking apiserver status ...
I0329 16:49:11.260038  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:11.756996  292670 ssh_runner.go:195] Run: sudo egrep ^[0-9]+:freezer: /proc/90701/cgroup
I0329 16:49:12.147671  292670 api_server.go:182] apiserver freezer: "7:freezer:/docker/9b2fc22828ae7ee0611287afdf22ba2bb5d769be7dd365d2da6a8ce714349a0e/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/a00ff38a3b954994bd977f2522765d7298018f6064eb17f1924f9f976c9038d2"
I0329 16:49:12.148006  292670 ssh_runner.go:195] Run: sudo cat /sys/fs/cgroup/freezer/docker/9b2fc22828ae7ee0611287afdf22ba2bb5d769be7dd365d2da6a8ce714349a0e/kubepods/burstable/pod55b4bbe24dac3803a7379f9ae169d6ba/a00ff38a3b954994bd977f2522765d7298018f6064eb17f1924f9f976c9038d2/freezer.state
I0329 16:49:12.452368  292670 api_server.go:204] freezer state: "THAWED"
I0329 16:49:12.452468  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:17.455623  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:49:17.465889  292670 retry.go:31] will retry after 265.883894ms: state is "Stopped"
I0329 16:49:17.732394  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:22.734143  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:49:22.734271  292670 retry.go:31] will retry after 375.961952ms: state is "Stopped"
I0329 16:49:23.111494  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:23.850080  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:23.850252  292670 retry.go:31] will retry after 485.540993ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:24.336891  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:24.639973  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:24.640083  292670 retry.go:31] will retry after 493.484105ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:25.134320  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:25.334732  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:25.334848  292670 retry.go:31] will retry after 638.511875ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:25.980491  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:26.232916  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:26.233031  292670 retry.go:31] will retry after 714.625903ms: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:26.948490  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:27.459642  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:27.459692  292670 kubeadm.go:611] needs reconfigure: apiserver error: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:49:27.460142  292670 kubeadm.go:1128] stopping kube-system containers ...
I0329 16:49:27.461672  292670 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0329 16:49:28.983361  292670 ssh_runner.go:235] Completed: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}: (1.51895041s)
I0329 16:49:28.983630  292670 docker.go:469] Stopping containers: [e93b4c9a94e0 bb8576ed3107 84488d1eb5ae 9e21ccd20fbe dfdf3cdd4916 a00ff38a3b95 591dbf9ff270 8f584ab1ea89 7a1057fa20bb 2d7220d79b0e 7809d76f8e69 d41dbfc91eef 71aead4d120a bcc4d0de00b6 facd4ee50579 88882201cd88 dcbe0483c3fb db5b4e75ec09 ec6b6fa3bf9b e9cfcebd9af6 ea9b9aea2cfb b984a30c8107 ab42615bd9c5 c0241f57df65 2e3536aadb30 027818adbb12 b9cbf0f58a88]
I0329 16:49:28.985254  292670 ssh_runner.go:195] Run: docker stop e93b4c9a94e0 bb8576ed3107 84488d1eb5ae 9e21ccd20fbe dfdf3cdd4916 a00ff38a3b95 591dbf9ff270 8f584ab1ea89 7a1057fa20bb 2d7220d79b0e 7809d76f8e69 d41dbfc91eef 71aead4d120a bcc4d0de00b6 facd4ee50579 88882201cd88 dcbe0483c3fb db5b4e75ec09 ec6b6fa3bf9b e9cfcebd9af6 ea9b9aea2cfb b984a30c8107 ab42615bd9c5 c0241f57df65 2e3536aadb30 027818adbb12 b9cbf0f58a88
I0329 16:49:39.145124  292670 ssh_runner.go:235] Completed: docker stop e93b4c9a94e0 bb8576ed3107 84488d1eb5ae 9e21ccd20fbe dfdf3cdd4916 a00ff38a3b95 591dbf9ff270 8f584ab1ea89 7a1057fa20bb 2d7220d79b0e 7809d76f8e69 d41dbfc91eef 71aead4d120a bcc4d0de00b6 facd4ee50579 88882201cd88 dcbe0483c3fb db5b4e75ec09 ec6b6fa3bf9b e9cfcebd9af6 ea9b9aea2cfb b984a30c8107 ab42615bd9c5 c0241f57df65 2e3536aadb30 027818adbb12 b9cbf0f58a88: (10.159769917s)
I0329 16:49:39.145240  292670 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0329 16:49:39.843356  292670 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0329 16:49:40.262819  292670 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Mar 29 05:21 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Mar 29 05:21 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Mar 29 05:22 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Mar 29 05:21 /etc/kubernetes/scheduler.conf

I0329 16:49:40.263180  292670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0329 16:49:40.633126  292670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0329 16:49:40.948068  292670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0329 16:49:41.240135  292670 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0329 16:49:41.240494  292670 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0329 16:49:41.665334  292670 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0329 16:49:42.062178  292670 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0329 16:49:42.062340  292670 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0329 16:49:42.273179  292670 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0329 16:49:43.145190  292670 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0329 16:49:43.145266  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0329 16:49:46.190356  292670 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (3.045037774s)
I0329 16:49:46.190445  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0329 16:49:48.086448  292670 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.895970371s)
I0329 16:49:48.086477  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0329 16:49:48.485927  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0329 16:49:48.601563  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0329 16:49:48.757550  292670 api_server.go:52] waiting for apiserver process to appear ...
I0329 16:49:48.757649  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:48.792763  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:49.327816  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:49.828481  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:50.329705  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:50.829449  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:51.328635  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:51.829172  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:52.327883  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:52.842888  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:53.329126  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:53.828233  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:54.329489  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:54.878077  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:55.454461  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:56.340210  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:49:57.272407  292670 api_server.go:72] duration metric: took 8.514823214s to wait for apiserver process to appear ...
I0329 16:49:57.272466  292670 api_server.go:88] waiting for apiserver healthz status ...
I0329 16:49:57.272536  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:57.319936  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:60394->127.0.0.1:32769: read: connection reset by peer
I0329 16:49:57.320043  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:57.325125  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:60400->127.0.0.1:32769: read: connection reset by peer
I0329 16:49:57.826739  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:57.830978  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:60402->127.0.0.1:32769: read: connection reset by peer
I0329 16:49:58.325856  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:58.330194  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": EOF
I0329 16:49:58.825769  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:49:58.829688  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": read tcp 127.0.0.1:60412->127.0.0.1:32769: read: connection reset by peer
I0329 16:49:59.326979  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:04.328476  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:50:04.328578  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:09.330778  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:50:09.330878  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:14.333209  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:50:14.333307  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:19.334868  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:50:19.334940  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:24.339455  292670 api_server.go:269] stopped: https://127.0.0.1:32769/healthz: Get "https://127.0.0.1:32769/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0329 16:50:24.340675  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:25.041684  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0329 16:50:25.041762  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0329 16:50:25.041825  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:25.356236  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0329 16:50:25.356324  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0329 16:50:25.356385  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:25.579132  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:25.579197  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[-]etcd failed: reason withheld
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[-]autoregister-completion failed: reason withheld
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:25.826406  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:26.134817  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:26.134907  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:26.326649  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:26.434693  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:26.434944  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:26.825974  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:26.978939  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:26.979040  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:27.326671  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:27.357343  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:27.357375  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:27.827065  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:27.977256  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:27.977338  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:28.326765  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:28.401154  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:28.401451  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:28.825882  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:28.872080  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0329 16:50:28.872107  292670 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0329 16:50:29.326754  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:29.376989  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0329 16:50:29.469791  292670 api_server.go:141] control plane version: v1.28.3
I0329 16:50:29.469885  292670 api_server.go:131] duration metric: took 32.197386907s to wait for apiserver health ...
I0329 16:50:29.469920  292670 cni.go:84] Creating CNI manager for ""
I0329 16:50:29.469970  292670 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0329 16:50:29.480192  292670 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0329 16:50:29.498861  292670 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0329 16:50:29.604434  292670 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0329 16:50:29.849720  292670 system_pods.go:43] waiting for kube-system pods to appear ...
I0329 16:50:30.029364  292670 system_pods.go:59] 7 kube-system pods found
I0329 16:50:30.031067  292670 system_pods.go:61] "coredns-5dd5756b68-r7x2l" [cefa8631-fb98-4413-93a4-bfc2b264055e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0329 16:50:30.031130  292670 system_pods.go:61] "etcd-minikube" [dd41b80d-3e65-4a5f-a17b-00a713a9505d] Running
I0329 16:50:30.031204  292670 system_pods.go:61] "kube-apiserver-minikube" [a06337f4-100b-44df-ad92-6f3f7aabb4f0] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0329 16:50:30.031468  292670 system_pods.go:61] "kube-controller-manager-minikube" [c159ab35-d19a-4b52-919d-e406e6664940] Running
I0329 16:50:30.031512  292670 system_pods.go:61] "kube-proxy-2cp4n" [ff9bf080-33af-4fc2-a1b6-a8d898db14ef] Running
I0329 16:50:30.031531  292670 system_pods.go:61] "kube-scheduler-minikube" [5be45852-8295-4133-a8bb-4cec3e936781] Running
I0329 16:50:30.031562  292670 system_pods.go:61] "storage-provisioner" [24bf02fc-5c5a-41bf-aa78-0309d0b93889] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0329 16:50:30.031587  292670 system_pods.go:74] duration metric: took 181.817288ms to wait for pod list to return data ...
I0329 16:50:30.032204  292670 node_conditions.go:102] verifying NodePressure condition ...
I0329 16:50:30.079489  292670 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0329 16:50:30.079994  292670 node_conditions.go:123] node cpu capacity is 4
I0329 16:50:30.080932  292670 node_conditions.go:105] duration metric: took 48.682697ms to run NodePressure ...
I0329 16:50:30.081028  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0329 16:50:32.196878  292670 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (2.115776258s)
I0329 16:50:32.196925  292670 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0329 16:50:32.237189  292670 ops.go:34] apiserver oom_adj: -16
I0329 16:50:32.238794  292670 kubeadm.go:640] restartCluster took 1m22.061112979s
I0329 16:50:32.238831  292670 kubeadm.go:406] StartCluster complete in 1m24.291581874s
I0329 16:50:32.238951  292670 settings.go:142] acquiring lock: {Name:mkac65ce0183564c0160b580651098887c156546 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 16:50:32.239440  292670 settings.go:150] Updating kubeconfig:  /home/lee/.kube/config
I0329 16:50:32.242423  292670 lock.go:35] WriteFile acquiring /home/lee/.kube/config: {Name:mke78f7e985c39ffd6431c856c291714e0dfcbf0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0329 16:50:32.245611  292670 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0329 16:50:32.246174  292670 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0329 16:50:32.247208  292670 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0329 16:50:32.247498  292670 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0329 16:50:32.247665  292670 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0329 16:50:32.248015  292670 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0329 16:50:32.248348  292670 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0329 16:50:32.248371  292670 addons.go:240] addon storage-provisioner should already be in state true
I0329 16:50:32.250533  292670 host.go:66] Checking if "minikube" exists ...
I0329 16:50:32.251815  292670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 16:50:32.252338  292670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 16:50:32.267292  292670 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0329 16:50:32.267392  292670 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0329 16:50:32.283038  292670 out.go:177] üîé  Verifying Kubernetes components...
I0329 16:50:32.303556  292670 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0329 16:50:32.345225  292670 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0329 16:50:32.345247  292670 addons.go:240] addon default-storageclass should already be in state true
I0329 16:50:32.345296  292670 host.go:66] Checking if "minikube" exists ...
I0329 16:50:32.346730  292670 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0329 16:50:32.357158  292670 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0329 16:50:32.366114  292670 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0329 16:50:32.366140  292670 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0329 16:50:32.366280  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:50:32.410147  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:50:32.420458  292670 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0329 16:50:32.420477  292670 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0329 16:50:32.420585  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0329 16:50:32.461569  292670 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/lee/.minikube/machines/minikube/id_rsa Username:docker}
I0329 16:50:32.581476  292670 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0329 16:50:32.600688  292670 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0329 16:50:32.600890  292670 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0329 16:50:32.634724  292670 api_server.go:52] waiting for apiserver process to appear ...
I0329 16:50:32.634832  292670 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0329 16:50:32.664709  292670 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0329 16:50:36.937746  292670 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (4.302845111s)
I0329 16:50:36.937751  292670 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.356238107s)
I0329 16:50:36.937779  292670 api_server.go:72] duration metric: took 4.670321286s to wait for apiserver process to appear ...
I0329 16:50:36.937789  292670 api_server.go:88] waiting for apiserver healthz status ...
I0329 16:50:36.937813  292670 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0329 16:50:36.937936  292670 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (4.273191713s)
I0329 16:50:36.950721  292670 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0329 16:50:36.955205  292670 api_server.go:141] control plane version: v1.28.3
I0329 16:50:36.955239  292670 api_server.go:131] duration metric: took 17.432999ms to wait for apiserver health ...
I0329 16:50:36.955255  292670 system_pods.go:43] waiting for kube-system pods to appear ...
I0329 16:50:36.962707  292670 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0329 16:50:36.971811  292670 addons.go:502] enable addons completed in 4.725420183s: enabled=[storage-provisioner default-storageclass]
I0329 16:50:36.972367  292670 system_pods.go:59] 7 kube-system pods found
I0329 16:50:36.972394  292670 system_pods.go:61] "coredns-5dd5756b68-r7x2l" [cefa8631-fb98-4413-93a4-bfc2b264055e] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0329 16:50:36.972403  292670 system_pods.go:61] "etcd-minikube" [dd41b80d-3e65-4a5f-a17b-00a713a9505d] Running
I0329 16:50:36.972413  292670 system_pods.go:61] "kube-apiserver-minikube" [a06337f4-100b-44df-ad92-6f3f7aabb4f0] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0329 16:50:36.972421  292670 system_pods.go:61] "kube-controller-manager-minikube" [c159ab35-d19a-4b52-919d-e406e6664940] Running
I0329 16:50:36.972427  292670 system_pods.go:61] "kube-proxy-2cp4n" [ff9bf080-33af-4fc2-a1b6-a8d898db14ef] Running
I0329 16:50:36.972433  292670 system_pods.go:61] "kube-scheduler-minikube" [5be45852-8295-4133-a8bb-4cec3e936781] Running
I0329 16:50:36.972440  292670 system_pods.go:61] "storage-provisioner" [24bf02fc-5c5a-41bf-aa78-0309d0b93889] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0329 16:50:36.972449  292670 system_pods.go:74] duration metric: took 17.186699ms to wait for pod list to return data ...
I0329 16:50:36.972462  292670 kubeadm.go:581] duration metric: took 4.705006884s to wait for : map[apiserver:true system_pods:true] ...
I0329 16:50:36.972479  292670 node_conditions.go:102] verifying NodePressure condition ...
I0329 16:50:36.978757  292670 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0329 16:50:36.978773  292670 node_conditions.go:123] node cpu capacity is 4
I0329 16:50:36.978784  292670 node_conditions.go:105] duration metric: took 6.3003ms to run NodePressure ...
I0329 16:50:36.978800  292670 start.go:228] waiting for startup goroutines ...
I0329 16:50:36.978810  292670 start.go:233] waiting for cluster config update ...
I0329 16:50:36.978822  292670 start.go:242] writing updated cluster config ...
I0329 16:50:36.979214  292670 ssh_runner.go:195] Run: rm -f paused
I0329 16:50:37.226766  292670 start.go:600] kubectl: 1.29.3, cluster: 1.28.3 (minor skew: 1)
I0329 16:50:37.230315  292670 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Mar 29 07:48:16 minikube cri-dockerd[90075]: time="2024-03-29T07:48:16Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Mar 29 07:48:16 minikube cri-dockerd[90075]: time="2024-03-29T07:48:16Z" level=info msg="Start cri-dockerd grpc backend"
Mar 29 07:48:16 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Mar 29 07:48:20 minikube cri-dockerd[90075]: time="2024-03-29T07:48:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/71aead4d120ae5503bdbb985148833bbec142c5702113226bf4b4c06ac2dc4fd/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:48:21 minikube cri-dockerd[90075]: time="2024-03-29T07:48:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bcc4d0de00b69d5a6ff070199485682a451e08aa2418672f50bb536b771d8b8e/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:48:23 minikube cri-dockerd[90075]: time="2024-03-29T07:48:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7809d76f8e692c4b018c3ed7601a51351c15197b5243ce89423cdbe80fd741cb/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:48:25 minikube cri-dockerd[90075]: time="2024-03-29T07:48:25Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d41dbfc91eef0094d44792a7d5f092743fcb3f71c32574b8f3bfb18721548d2b/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:48:26 minikube cri-dockerd[90075]: time="2024-03-29T07:48:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2d7220d79b0e996c6672887345102df34ca68062ae8f1e525b65ac54183b4ebc/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:48:26 minikube cri-dockerd[90075]: time="2024-03-29T07:48:26Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7a1057fa20bb9482888380a24d1f933d143a2a44e10197f52103c31b73ffb2fd/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:48:29 minikube cri-dockerd[90075]: time="2024-03-29T07:48:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8f584ab1ea89b82fdd8919b733cde1a59bf261ce46087276c19525999c80782b/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:04 minikube dockerd[89770]: time="2024-03-29T07:49:04.843151376Z" level=info msg="ignoring event" container=84488d1eb5aeae9c055446c938380d59fa80d91ffb9a3a82a8453c2295ffc43e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:11 minikube cri-dockerd[90075]: time="2024-03-29T07:49:11Z" level=error msg="Error response from daemon: No such container: 1f8f6873d7e4f42d8e7798615fb11f8505c1ae3688a1ab1a63f93dfafdc8b2d1 Failed to get stats from container 1f8f6873d7e4f42d8e7798615fb11f8505c1ae3688a1ab1a63f93dfafdc8b2d1"
Mar 29 07:49:33 minikube dockerd[89770]: time="2024-03-29T07:49:33.744646855Z" level=info msg="ignoring event" container=71aead4d120ae5503bdbb985148833bbec142c5702113226bf4b4c06ac2dc4fd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:33 minikube dockerd[89770]: time="2024-03-29T07:49:33.843954294Z" level=info msg="ignoring event" container=bb8576ed31079dbbd1e8d95f9ec83a111b533963a5216b801752f681d31cb14f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:33 minikube dockerd[89770]: time="2024-03-29T07:49:33.862180301Z" level=info msg="ignoring event" container=dfdf3cdd4916cd56f4f5109edd067c40695a939338029db606e4ce33a9af0fe1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:33 minikube dockerd[89770]: time="2024-03-29T07:49:33.936709231Z" level=info msg="ignoring event" container=2d7220d79b0e996c6672887345102df34ca68062ae8f1e525b65ac54183b4ebc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:34 minikube dockerd[89770]: time="2024-03-29T07:49:34.850433991Z" level=info msg="ignoring event" container=7a1057fa20bb9482888380a24d1f933d143a2a44e10197f52103c31b73ffb2fd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:34 minikube dockerd[89770]: time="2024-03-29T07:49:34.870885300Z" level=info msg="ignoring event" container=7809d76f8e692c4b018c3ed7601a51351c15197b5243ce89423cdbe80fd741cb module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:34 minikube dockerd[89770]: time="2024-03-29T07:49:34.874231801Z" level=info msg="ignoring event" container=9e21ccd20fbe54e454ce32297bb539766bdec2a7a35ec40cd12c2843b2b77ea2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:34 minikube dockerd[89770]: time="2024-03-29T07:49:34.941656827Z" level=info msg="ignoring event" container=d41dbfc91eef0094d44792a7d5f092743fcb3f71c32574b8f3bfb18721548d2b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:35 minikube dockerd[89770]: time="2024-03-29T07:49:35.556469670Z" level=info msg="ignoring event" container=591dbf9ff270f3ddcc22f6435078b53f284f630f4a2b137bc65b304319604361 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:35 minikube dockerd[89770]: time="2024-03-29T07:49:35.753605248Z" level=info msg="ignoring event" container=a00ff38a3b954994bd977f2522765d7298018f6064eb17f1924f9f976c9038d2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:35 minikube dockerd[89770]: time="2024-03-29T07:49:35.773921456Z" level=info msg="ignoring event" container=bcc4d0de00b69d5a6ff070199485682a451e08aa2418672f50bb536b771d8b8e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:35 minikube dockerd[89770]: time="2024-03-29T07:49:35.945925424Z" level=error msg="stream copy error: reading from a closed fifo"
Mar 29 07:49:35 minikube dockerd[89770]: time="2024-03-29T07:49:35.953204627Z" level=error msg="stream copy error: reading from a closed fifo"
Mar 29 07:49:36 minikube dockerd[89770]: time="2024-03-29T07:49:36.067830272Z" level=info msg="ignoring event" container=8f584ab1ea89b82fdd8919b733cde1a59bf261ce46087276c19525999c80782b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:37 minikube cri-dockerd[90075]: time="2024-03-29T07:49:37Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-r7x2l_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8f584ab1ea89b82fdd8919b733cde1a59bf261ce46087276c19525999c80782b\""
Mar 29 07:49:38 minikube dockerd[89770]: time="2024-03-29T07:49:38.836141465Z" level=info msg="ignoring event" container=e93b4c9a94e0a0dd731b6dd67335d5a5af48240a1b760ef2706108bc916ae7cd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 29 07:49:39 minikube cri-dockerd[90075]: W0329 07:49:39.673156   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:49:40 minikube cri-dockerd[90075]: time="2024-03-29T07:49:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/49b0ef87776a7aff8736a5b473dacf7f536a2e6ea6435ce2f394f49dbadf96b8/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:40 minikube cri-dockerd[90075]: W0329 07:49:40.935473   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:49:44 minikube cri-dockerd[90075]: time="2024-03-29T07:49:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e36476df788d3c19d9e47d68d6e71533321e84750a74f534dea5a88f21772948/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:44 minikube cri-dockerd[90075]: W0329 07:49:44.243652   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:49:44 minikube cri-dockerd[90075]: time="2024-03-29T07:49:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b3c42db1eb733af2e5f0b1ce3759a9110f88c3ec1cb30facbfb95e7f08566278/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:44 minikube cri-dockerd[90075]: W0329 07:49:44.349265   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:49:44 minikube cri-dockerd[90075]: time="2024-03-29T07:49:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb1bd314ef8b920bf81f277746258d3c8c02e76a4bfc15dd195b80ebc581f646/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:44 minikube cri-dockerd[90075]: W0329 07:49:44.800509   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:49:44 minikube cri-dockerd[90075]: time="2024-03-29T07:49:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d2a12eafc417e0070833a7d55ff5bb1a733ef267d14fbd55848d454ef8a49f8f/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:44 minikube cri-dockerd[90075]: W0329 07:49:44.801273   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:49:44 minikube cri-dockerd[90075]: time="2024-03-29T07:49:44Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0fcf8d096e2df828d59943f35278561c51ac95eb3a39f2837dd5027b52d34610/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 07:49:44 minikube cri-dockerd[90075]: W0329 07:49:44.964949   90075 logging.go:59] [core] [Server #1] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
Mar 29 07:50:26 minikube cri-dockerd[90075]: time="2024-03-29T07:50:26Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 29 07:50:52 minikube cri-dockerd[90075]: time="2024-03-29T07:50:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a2f696d0fe1aeefa2c3f0482d28c61eaf19d89d68aa6ea6cf002f162f065e46/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Mar 29 08:13:56 minikube cri-dockerd[90075]: time="2024-03-29T08:13:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3ab087ec15d3ae3235ffe49ed6ca6c14476677802b9ea1194594905cc41aa68c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 08:13:56 minikube cri-dockerd[90075]: time="2024-03-29T08:13:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/19f484677efb7e888f6989fbbd7ad01e633b96a3f57a92c008e6d9ef914d09b1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 29 08:14:10 minikube cri-dockerd[90075]: time="2024-03-29T08:14:10Z" level=info msg="Pulling image jaruda/fast-api:latest: 5f899db30843: Downloading [==================>                                ]  24.21MB/64.14MB"
Mar 29 08:14:20 minikube cri-dockerd[90075]: time="2024-03-29T08:14:20Z" level=info msg="Pulling image jaruda/fast-api:latest: 5f899db30843: Downloading [=====================================>             ]  47.92MB/64.14MB"
Mar 29 08:14:30 minikube cri-dockerd[90075]: time="2024-03-29T08:14:30Z" level=info msg="Pulling image jaruda/fast-api:latest: 4fb767c6b146: Downloading [================================>                  ]  19.88MB/30.27MB"
Mar 29 08:14:40 minikube cri-dockerd[90075]: time="2024-03-29T08:14:40Z" level=info msg="Pulling image jaruda/fast-api:latest: 71215d55680c: Extracting [============================>                      ]  27.79MB/49.55MB"
Mar 29 08:14:50 minikube cri-dockerd[90075]: time="2024-03-29T08:14:50Z" level=info msg="Pulling image jaruda/fast-api:latest: 71215d55680c: Extracting [=================================================> ]  49.28MB/49.55MB"
Mar 29 08:15:00 minikube cri-dockerd[90075]: time="2024-03-29T08:15:00Z" level=info msg="Pulling image jaruda/fast-api:latest: 5f899db30843: Extracting [>                                                  ]  557.1kB/64.14MB"
Mar 29 08:15:10 minikube cri-dockerd[90075]: time="2024-03-29T08:15:10Z" level=info msg="Pulling image jaruda/fast-api:latest: 5f899db30843: Extracting [====================>                              ]  26.18MB/64.14MB"
Mar 29 08:15:20 minikube cri-dockerd[90075]: time="2024-03-29T08:15:20Z" level=info msg="Pulling image jaruda/fast-api:latest: 5f899db30843: Extracting [==============================================>    ]   59.6MB/64.14MB"
Mar 29 08:15:30 minikube cri-dockerd[90075]: time="2024-03-29T08:15:30Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [====>                                              ]  20.05MB/211.1MB"
Mar 29 08:15:40 minikube cri-dockerd[90075]: time="2024-03-29T08:15:40Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [======>                                            ]   27.3MB/211.1MB"
Mar 29 08:15:50 minikube cri-dockerd[90075]: time="2024-03-29T08:15:50Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [==========>                                        ]  43.45MB/211.1MB"
Mar 29 08:16:00 minikube cri-dockerd[90075]: time="2024-03-29T08:16:00Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [=================>                                 ]  72.42MB/211.1MB"
Mar 29 08:16:10 minikube cri-dockerd[90075]: time="2024-03-29T08:16:10Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [=======================>                           ]  100.8MB/211.1MB"
Mar 29 08:16:20 minikube cri-dockerd[90075]: time="2024-03-29T08:16:20Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [==============================>                    ]  128.1MB/211.1MB"
Mar 29 08:16:30 minikube cri-dockerd[90075]: time="2024-03-29T08:16:30Z" level=info msg="Pulling image jaruda/fast-api:latest: 567db630df8d: Extracting [=====================================>             ]  156.5MB/211.1MB"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
f021a50142ded       ead0a4a53df89       25 minutes ago      Running             coredns                   2                   2a2f696d0fe1a       coredns-5dd5756b68-r7x2l
09601e9f57853       bfc896cf80fba       25 minutes ago      Running             kube-proxy                2                   49b0ef87776a7       kube-proxy-2cp4n
269cf61452bc7       6e38f40d628db       25 minutes ago      Running             storage-provisioner       3                   b3c42db1eb733       storage-provisioner
89e8595f51739       6d1b4fd1b182d       26 minutes ago      Running             kube-scheduler            2                   d2a12eafc417e       kube-scheduler-minikube
42b7664aa6abc       10baa1ca17068       26 minutes ago      Running             kube-controller-manager   2                   0fcf8d096e2df       kube-controller-manager-minikube
505c322db9dde       5374347291230       26 minutes ago      Running             kube-apiserver            2                   e36476df788d3       kube-apiserver-minikube
cf72a24667ea2       73deb9a3f7025       26 minutes ago      Running             etcd                      2                   eb1bd314ef8b9       etcd-minikube
fdbd66941e77e       6e38f40d628db       27 minutes ago      Created             storage-provisioner       2                   2d7220d79b0e9       storage-provisioner
e93b4c9a94e0a       ead0a4a53df89       28 minutes ago      Exited              coredns                   1                   8f584ab1ea89b       coredns-5dd5756b68-r7x2l
bb8576ed31079       10baa1ca17068       28 minutes ago      Exited              kube-controller-manager   1                   7a1057fa20bb9       kube-controller-manager-minikube
9e21ccd20fbe5       6d1b4fd1b182d       28 minutes ago      Exited              kube-scheduler            1                   d41dbfc91eef0       kube-scheduler-minikube
dfdf3cdd4916c       bfc896cf80fba       28 minutes ago      Exited              kube-proxy                1                   7809d76f8e692       kube-proxy-2cp4n
a00ff38a3b954       5374347291230       28 minutes ago      Exited              kube-apiserver            1                   bcc4d0de00b69       kube-apiserver-minikube
591dbf9ff270f       73deb9a3f7025       28 minutes ago      Exited              etcd                      1                   71aead4d120ae       etcd-minikube

* 
* ==> coredns [e93b4c9a94e0] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:48857 - 62639 "HINFO IN 3856668988306134369.7389560904202161483. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.092604263s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> coredns [f021a50142de] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:46061 - 35567 "HINFO IN 8639309081188834871.5820809575361484037. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.0151583s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_03_29T14_22_32_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 29 Mar 2024 05:22:21 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 29 Mar 2024 08:16:34 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 29 Mar 2024 08:15:46 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 29 Mar 2024 08:15:46 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 29 Mar 2024 08:15:46 +0000   Fri, 29 Mar 2024 05:22:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 29 Mar 2024 08:15:46 +0000   Fri, 29 Mar 2024 05:22:35 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941276Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3941276Ki
  pods:               110
System Info:
  Machine ID:                 e790007a52ab45f88d0859d1935684b8
  System UUID:                e790007a52ab45f88d0859d1935684b8
  Boot ID:                    f67e2ab8-e954-494c-8c47-1f9b00e8dbae
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     fast-api-deployment-84c77b9745-plcdr    500m (12%!)(MISSING)    500m (12%!)(MISSING)  256Mi (6%!)(MISSING)       256Mi (6%!)(MISSING)     2m45s
  default                     fast-api-deployment-84c77b9745-wt9xs    500m (12%!)(MISSING)    500m (12%!)(MISSING)  256Mi (6%!)(MISSING)       256Mi (6%!)(MISSING)     2m45s
  kube-system                 coredns-5dd5756b68-r7x2l                100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     173m
  kube-system                 etcd-minikube                           100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         174m
  kube-system                 kube-apiserver-minikube                 250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         174m
  kube-system                 kube-controller-manager-minikube        200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         174m
  kube-system                 kube-proxy-2cp4n                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         173m
  kube-system                 kube-scheduler-minikube                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         174m
  kube-system                 storage-provisioner                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         173m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                1750m (43%!)(MISSING)  1 (25%!)(MISSING)
  memory             682Mi (17%!)(MISSING)  682Mi (17%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)       0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                 From             Message
  ----    ------                   ----                ----             -------
  Normal  Starting                 27m                 kube-proxy       
  Normal  Starting                 25m                 kube-proxy       
  Normal  NodeNotReady             28m (x2 over 174m)  kubelet          Node minikube status is now: NodeNotReady
  Normal  Starting                 26m                 kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  26m (x8 over 26m)   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    26m (x8 over 26m)   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     26m (x7 over 26m)   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  26m                 kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           25m                 node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Mar28 15:35] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.000000] PCI: Fatal: No config space access function found
[  +0.020887] PCI: System does not support PCI
[  +0.039109] kvm: no hardware support
[  +0.000005] kvm: no hardware support
[  +0.822368] FS-Cache: Duplicate cookie detected
[  +0.000580] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.001113] FS-Cache: O-cookie d=00000000d2239c5b{9P.session} n=00000000a8ae233a
[  +0.002107] FS-Cache: O-key=[10] '34323934393337333835'
[  +0.002764] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000930] FS-Cache: N-cookie d=00000000d2239c5b{9P.session} n=000000003b585744
[  +0.003254] FS-Cache: N-key=[10] '34323934393337333835'
[  +0.007823] FS-Cache: Duplicate cookie detected
[  +0.000653] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000643] FS-Cache: O-cookie d=00000000d2239c5b{9P.session} n=00000000017dd655
[  +0.000712] FS-Cache: O-key=[10] '34323934393337333837'
[  +0.000579] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000670] FS-Cache: N-cookie d=00000000d2239c5b{9P.session} n=000000007723fc8b
[  +0.000938] FS-Cache: N-key=[10] '34323934393337333837'
[  +1.266886] Failed to connect to bus: No such file or directory
[  +0.208744] misc dxg: dxgk: dxgglobal_acquire_channel_lock: Failed to acquire global channel lock
[  +0.049081] Failed to connect to bus: No such file or directory
[Mar29 05:23] hrtimer: interrupt took 10824198 ns

* 
* ==> etcd [591dbf9ff270] <==
* {"level":"info","ts":"2024-03-29T07:49:26.076284Z","caller":"traceutil/trace.go:171","msg":"trace[363269601] range","detail":"{range_begin:/registry/clusterroles/system:public-info-viewer; range_end:; response_count:1; response_revision:7442; }","duration":"135.023619ms","start":"2024-03-29T07:49:25.941167Z","end":"2024-03-29T07:49:26.076191Z","steps":["trace[363269601] 'agreement among raft nodes before linearized reading'  (duration: 117.563516ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:26.34759Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"161.133722ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/admin\" ","response":"range_response_count:1 size:3548"}
{"level":"info","ts":"2024-03-29T07:49:26.347712Z","caller":"traceutil/trace.go:171","msg":"trace[673344294] range","detail":"{range_begin:/registry/clusterroles/admin; range_end:; response_count:1; response_revision:7442; }","duration":"161.263522ms","start":"2024-03-29T07:49:26.186418Z","end":"2024-03-29T07:49:26.347681Z","steps":["trace[673344294] 'range keys from in-memory index tree'  (duration: 160.740922ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:27.872567Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"175.461924ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147339888298 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bef9537f\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bef9537f\" value_size:688 lease:8128028147339888252 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-03-29T07:49:27.87334Z","caller":"traceutil/trace.go:171","msg":"trace[556275859] linearizableReadLoop","detail":"{readStateIndex:9246; appliedIndex:9245; }","duration":"215.24063ms","start":"2024-03-29T07:49:27.65797Z","end":"2024-03-29T07:49:27.87321Z","steps":["trace[556275859] 'read index received'  (duration: 18.144403ms)","trace[556275859] 'applied index is now lower than readState.Index'  (duration: 197.087927ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T07:49:27.874627Z","caller":"traceutil/trace.go:171","msg":"trace[836555936] transaction","detail":"{read_only:false; response_revision:7448; number_of_response:1; }","duration":"223.093231ms","start":"2024-03-29T07:49:27.651313Z","end":"2024-03-29T07:49:27.874407Z","steps":["trace[836555936] 'process raft request'  (duration: 24.958304ms)","trace[836555936] 'compare'  (duration: 173.523224ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:49:27.93084Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"181.710925ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/coredns-5dd5756b68-r7x2l\" ","response":"range_response_count:1 size:4851"}
{"level":"info","ts":"2024-03-29T07:49:27.931069Z","caller":"traceutil/trace.go:171","msg":"trace[1756188256] range","detail":"{range_begin:/registry/pods/kube-system/coredns-5dd5756b68-r7x2l; range_end:; response_count:1; response_revision:7448; }","duration":"181.972525ms","start":"2024-03-29T07:49:27.749038Z","end":"2024-03-29T07:49:27.93101Z","steps":["trace[1756188256] 'agreement among raft nodes before linearized reading'  (duration: 181.244525ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:27.930831Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"272.897138ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:heapster\" ","response":"range_response_count:1 size:638"}
{"level":"info","ts":"2024-03-29T07:49:27.931453Z","caller":"traceutil/trace.go:171","msg":"trace[1010965086] range","detail":"{range_begin:/registry/clusterroles/system:heapster; range_end:; response_count:1; response_revision:7448; }","duration":"273.547938ms","start":"2024-03-29T07:49:27.657851Z","end":"2024-03-29T07:49:27.931399Z","steps":["trace[1010965086] 'agreement among raft nodes before linearized reading'  (duration: 272.492638ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:28.074176Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"120.759216ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:node\" ","response":"range_response_count:1 size:1486"}
{"level":"info","ts":"2024-03-29T07:49:28.07491Z","caller":"traceutil/trace.go:171","msg":"trace[2068050957] range","detail":"{range_begin:/registry/clusterroles/system:node; range_end:; response_count:1; response_revision:7448; }","duration":"121.599417ms","start":"2024-03-29T07:49:27.95317Z","end":"2024-03-29T07:49:28.074769Z","steps":["trace[2068050957] 'range keys from in-memory index tree'  (duration: 119.916816ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:49:29.075779Z","caller":"traceutil/trace.go:171","msg":"trace[1640679768] transaction","detail":"{read_only:false; response_revision:7453; number_of_response:1; }","duration":"126.642318ms","start":"2024-03-29T07:49:28.94905Z","end":"2024-03-29T07:49:29.075692Z","steps":["trace[1640679768] 'process raft request'  (duration: 83.047112ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:30.366523Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"113.361916ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:clusterrole-aggregation-controller\" ","response":"range_response_count:1 size:700"}
{"level":"info","ts":"2024-03-29T07:49:30.366845Z","caller":"traceutil/trace.go:171","msg":"trace[1202986705] range","detail":"{range_begin:/registry/clusterroles/system:controller:clusterrole-aggregation-controller; range_end:; response_count:1; response_revision:7460; }","duration":"113.650416ms","start":"2024-03-29T07:49:30.253073Z","end":"2024-03-29T07:49:30.366723Z","steps":["trace[1202986705] 'range keys from in-memory index tree'  (duration: 113.005815ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:30.658121Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"215.825129ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:cronjob-controller\" ","response":"range_response_count:1 size:837"}
{"level":"info","ts":"2024-03-29T07:49:30.658537Z","caller":"traceutil/trace.go:171","msg":"trace[891805145] range","detail":"{range_begin:/registry/clusterroles/system:controller:cronjob-controller; range_end:; response_count:1; response_revision:7461; }","duration":"216.254829ms","start":"2024-03-29T07:49:30.442204Z","end":"2024-03-29T07:49:30.658459Z","steps":["trace[891805145] 'agreement among raft nodes before linearized reading'  (duration: 23.862403ms)","trace[891805145] 'range keys from in-memory index tree'  (duration: 191.795726ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T07:49:31.032266Z","caller":"traceutil/trace.go:171","msg":"trace[380295661] transaction","detail":"{read_only:false; response_revision:7463; number_of_response:1; }","duration":"154.319721ms","start":"2024-03-29T07:49:30.877857Z","end":"2024-03-29T07:49:31.032176Z","steps":["trace[380295661] 'process raft request'  (duration: 153.846021ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:31.266947Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"130.442518ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147339888360 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/etcd-minikube.17c12cb2def80d98\" mod_revision:7440 > success:<request_put:<key:\"/registry/events/kube-system/etcd-minikube.17c12cb2def80d98\" value_size:726 lease:8128028147339888252 >> failure:<request_range:<key:\"/registry/events/kube-system/etcd-minikube.17c12cb2def80d98\" > >>","response":"size:16"}
{"level":"info","ts":"2024-03-29T07:49:31.267551Z","caller":"traceutil/trace.go:171","msg":"trace[1498967735] linearizableReadLoop","detail":"{readStateIndex:9262; appliedIndex:9261; }","duration":"332.168246ms","start":"2024-03-29T07:49:30.935326Z","end":"2024-03-29T07:49:31.267494Z","steps":["trace[1498967735] 'read index received'  (duration: 100.670314ms)","trace[1498967735] 'applied index is now lower than readState.Index'  (duration: 231.490432ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T07:49:31.267627Z","caller":"traceutil/trace.go:171","msg":"trace[178220712] transaction","detail":"{read_only:false; response_revision:7464; number_of_response:1; }","duration":"384.892454ms","start":"2024-03-29T07:49:30.882657Z","end":"2024-03-29T07:49:31.267549Z","steps":["trace[178220712] 'process raft request'  (duration: 247.944635ms)","trace[178220712] 'compare'  (duration: 129.294418ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:49:31.268136Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T07:49:30.882364Z","time spent":"385.563354ms","remote":"127.0.0.1:56418","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":803,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/etcd-minikube.17c12cb2def80d98\" mod_revision:7440 > success:<request_put:<key:\"/registry/events/kube-system/etcd-minikube.17c12cb2def80d98\" value_size:726 lease:8128028147339888252 >> failure:<request_range:<key:\"/registry/events/kube-system/etcd-minikube.17c12cb2def80d98\" > >"}
{"level":"warn","ts":"2024-03-29T07:49:31.268344Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"333.078246ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:endpoint-controller\" ","response":"range_response_count:1 size:756"}
{"level":"info","ts":"2024-03-29T07:49:31.26851Z","caller":"traceutil/trace.go:171","msg":"trace[1115848822] range","detail":"{range_begin:/registry/clusterroles/system:controller:endpoint-controller; range_end:; response_count:1; response_revision:7464; }","duration":"333.248046ms","start":"2024-03-29T07:49:30.935208Z","end":"2024-03-29T07:49:31.268456Z","steps":["trace[1115848822] 'agreement among raft nodes before linearized reading'  (duration: 332.803046ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:31.268659Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T07:49:30.935153Z","time spent":"333.462046ms","remote":"127.0.0.1:41568","response type":"/etcdserverpb.KV/Range","request count":0,"request size":62,"response count":1,"response size":780,"request content":"key:\"/registry/clusterroles/system:controller:endpoint-controller\" "}
{"level":"info","ts":"2024-03-29T07:49:31.544572Z","caller":"traceutil/trace.go:171","msg":"trace[995698224] linearizableReadLoop","detail":"{readStateIndex:9263; appliedIndex:9262; }","duration":"192.457827ms","start":"2024-03-29T07:49:31.352086Z","end":"2024-03-29T07:49:31.544544Z","steps":["trace[995698224] 'read index received'  (duration: 99.864314ms)","trace[995698224] 'applied index is now lower than readState.Index'  (duration: 92.591813ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:49:31.54847Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"196.462327ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:endpointslice-controller\" ","response":"range_response_count:1 size:793"}
{"level":"info","ts":"2024-03-29T07:49:31.548652Z","caller":"traceutil/trace.go:171","msg":"trace[1810178722] range","detail":"{range_begin:/registry/clusterroles/system:controller:endpointslice-controller; range_end:; response_count:1; response_revision:7464; }","duration":"196.664627ms","start":"2024-03-29T07:49:31.351961Z","end":"2024-03-29T07:49:31.548626Z","steps":["trace[1810178722] 'agreement among raft nodes before linearized reading'  (duration: 192.943627ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:31.63338Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"268.797737ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-controller-manager-minikube.17c12cb2e4e8d717\" ","response":"range_response_count:1 size:864"}
{"level":"info","ts":"2024-03-29T07:49:31.633738Z","caller":"traceutil/trace.go:171","msg":"trace[2003998895] range","detail":"{range_begin:/registry/events/kube-system/kube-controller-manager-minikube.17c12cb2e4e8d717; range_end:; response_count:1; response_revision:7465; }","duration":"269.170637ms","start":"2024-03-29T07:49:31.364508Z","end":"2024-03-29T07:49:31.633679Z","steps":["trace[2003998895] 'agreement among raft nodes before linearized reading'  (duration: 268.393137ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:31.634393Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"270.640437ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:49:31.63455Z","caller":"traceutil/trace.go:171","msg":"trace[508116552] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7465; }","duration":"270.821037ms","start":"2024-03-29T07:49:31.363687Z","end":"2024-03-29T07:49:31.634508Z","steps":["trace[508116552] 'agreement among raft nodes before linearized reading'  (duration: 270.543337ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:49:31.730655Z","caller":"traceutil/trace.go:171","msg":"trace[1842551022] transaction","detail":"{read_only:false; response_revision:7466; number_of_response:1; }","duration":"182.790626ms","start":"2024-03-29T07:49:31.547763Z","end":"2024-03-29T07:49:31.730553Z","steps":["trace[1842551022] 'process raft request'  (duration: 116.427116ms)","trace[1842551022] 'compare'  (duration: 14.971302ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:49:31.731127Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.291225ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:endpointslicemirroring-controller\" ","response":"range_response_count:1 size:843"}
{"level":"info","ts":"2024-03-29T07:49:31.731381Z","caller":"traceutil/trace.go:171","msg":"trace[867323346] range","detail":"{range_begin:/registry/clusterroles/system:controller:endpointslicemirroring-controller; range_end:; response_count:1; response_revision:7466; }","duration":"177.547325ms","start":"2024-03-29T07:49:31.553753Z","end":"2024-03-29T07:49:31.731301Z","steps":["trace[867323346] 'agreement among raft nodes before linearized reading'  (duration: 177.002625ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:31.965834Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.776015ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:ephemeral-volume-controller\" ","response":"range_response_count:1 size:761"}
{"level":"info","ts":"2024-03-29T07:49:31.966221Z","caller":"traceutil/trace.go:171","msg":"trace[1392709217] range","detail":"{range_begin:/registry/clusterroles/system:controller:ephemeral-volume-controller; range_end:; response_count:1; response_revision:7467; }","duration":"112.326415ms","start":"2024-03-29T07:49:31.853814Z","end":"2024-03-29T07:49:31.96614Z","steps":["trace[1392709217] 'range keys from in-memory index tree'  (duration: 111.267015ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:31.968164Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.271015ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bf4f1f90\" ","response":"range_response_count:1 size:831"}
{"level":"info","ts":"2024-03-29T07:49:31.968525Z","caller":"traceutil/trace.go:171","msg":"trace[205802525] range","detail":"{range_begin:/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bf4f1f90; range_end:; response_count:1; response_revision:7467; }","duration":"109.507215ms","start":"2024-03-29T07:49:31.858806Z","end":"2024-03-29T07:49:31.968313Z","steps":["trace[205802525] 'range keys from in-memory index tree'  (duration: 108.628715ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:49:32.187456Z","caller":"traceutil/trace.go:171","msg":"trace[1186316372] linearizableReadLoop","detail":"{readStateIndex:9267; appliedIndex:9266; }","duration":"111.942516ms","start":"2024-03-29T07:49:32.075478Z","end":"2024-03-29T07:49:32.18742Z","steps":["trace[1186316372] 'read index received'  (duration: 111.287916ms)","trace[1186316372] 'applied index is now lower than readState.Index'  (duration: 652.5¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:49:32.187788Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.369416ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:horizontal-pod-autoscaler\" ","response":"range_response_count:1 size:914"}
{"level":"info","ts":"2024-03-29T07:49:32.187869Z","caller":"traceutil/trace.go:171","msg":"trace[1409147796] range","detail":"{range_begin:/registry/clusterroles/system:controller:horizontal-pod-autoscaler; range_end:; response_count:1; response_revision:7468; }","duration":"112.469716ms","start":"2024-03-29T07:49:32.075375Z","end":"2024-03-29T07:49:32.187845Z","steps":["trace[1409147796] 'agreement among raft nodes before linearized reading'  (duration: 112.243916ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:49:32.188243Z","caller":"traceutil/trace.go:171","msg":"trace[1331631557] transaction","detail":"{read_only:false; response_revision:7468; number_of_response:1; }","duration":"124.450017ms","start":"2024-03-29T07:49:32.063762Z","end":"2024-03-29T07:49:32.188212Z","steps":["trace[1331631557] 'process raft request'  (duration: 123.092417ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:32.669795Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.377216ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147339888393 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bf4f1f90\" mod_revision:7470 > success:<request_put:<key:\"/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bf4f1f90\" value_size:729 lease:8128028147339888252 >> failure:<request_range:<key:\"/registry/events/kube-system/kube-apiserver-minikube.17c12cb3bf4f1f90\" > >>","response":"size:16"}
{"level":"info","ts":"2024-03-29T07:49:32.670128Z","caller":"traceutil/trace.go:171","msg":"trace[1012956075] linearizableReadLoop","detail":"{readStateIndex:9270; appliedIndex:9269; }","duration":"134.729318ms","start":"2024-03-29T07:49:32.535339Z","end":"2024-03-29T07:49:32.670069Z","steps":["trace[1012956075] 'read index received'  (duration: 17.452402ms)","trace[1012956075] 'applied index is now lower than readState.Index'  (duration: 117.266316ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T07:49:32.670483Z","caller":"traceutil/trace.go:171","msg":"trace[1993878894] transaction","detail":"{read_only:false; response_revision:7471; number_of_response:1; }","duration":"136.648018ms","start":"2024-03-29T07:49:32.533779Z","end":"2024-03-29T07:49:32.670427Z","steps":["trace[1993878894] 'process raft request'  (duration: 19.197702ms)","trace[1993878894] 'compare'  (duration: 115.367216ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T07:49:32.67163Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.357919ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:service-account-controller\" ","response":"range_response_count:1 size:677"}
{"level":"info","ts":"2024-03-29T07:49:32.671829Z","caller":"traceutil/trace.go:171","msg":"trace[219404206] range","detail":"{range_begin:/registry/clusterroles/system:controller:service-account-controller; range_end:; response_count:1; response_revision:7471; }","duration":"136.579819ms","start":"2024-03-29T07:49:32.535199Z","end":"2024-03-29T07:49:32.671778Z","steps":["trace[219404206] 'agreement among raft nodes before linearized reading'  (duration: 136.023119ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T07:49:32.765588Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.965317ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T07:49:32.765901Z","caller":"traceutil/trace.go:171","msg":"trace[1023542267] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:7471; }","duration":"214.00843ms","start":"2024-03-29T07:49:32.551805Z","end":"2024-03-29T07:49:32.765814Z","steps":["trace[1023542267] 'agreement among raft nodes before linearized reading'  (duration: 124.856817ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T07:49:34.461548Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-03-29T07:49:34.461892Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-03-29T07:49:34.462272Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-03-29T07:49:34.462512Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-03-29T07:49:34.472905Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-03-29T07:49:34.482338Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-03-29T07:49:34.530612Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-03-29T07:49:34.586366Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-03-29T07:49:34.587197Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-03-29T07:49:34.587345Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [cf72a24667ea] <==
* {"level":"warn","ts":"2024-03-29T08:06:00.572203Z","caller":"etcdserver/v3_server.go:897","msg":"waiting for ReadIndex response took too long, retrying","sent-request-id":8128028147361376421,"retry-timeout":"500ms"}
{"level":"warn","ts":"2024-03-29T08:06:00.837753Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.594188991s","expected-duration":"1s"}
{"level":"info","ts":"2024-03-29T08:06:00.838936Z","caller":"traceutil/trace.go:171","msg":"trace[364283134] linearizableReadLoop","detail":"{readStateIndex:10270; appliedIndex:10269; }","duration":"1.267608864s","start":"2024-03-29T08:05:59.571294Z","end":"2024-03-29T08:06:00.838903Z","steps":["trace[364283134] 'read index received'  (duration: 1.266755764s)","trace[364283134] 'applied index is now lower than readState.Index'  (duration: 727.4¬µs)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T08:06:00.839184Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.267911364s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-03-29T08:06:00.83926Z","caller":"traceutil/trace.go:171","msg":"trace[435849873] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:8267; }","duration":"1.267991564s","start":"2024-03-29T08:05:59.57124Z","end":"2024-03-29T08:06:00.839231Z","steps":["trace[435849873] 'agreement among raft nodes before linearized reading'  (duration: 1.267836464s)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:06:00.839329Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:05:59.571176Z","time spent":"1.268137864s","remote":"127.0.0.1:43240","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2024-03-29T08:06:00.839716Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:05:59.243321Z","time spent":"1.596390391s","remote":"127.0.0.1:42912","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-03-29T08:06:00.951084Z","caller":"traceutil/trace.go:171","msg":"trace[1394854232] transaction","detail":"{read_only:false; response_revision:8268; number_of_response:1; }","duration":"103.310608ms","start":"2024-03-29T08:06:00.847736Z","end":"2024-03-29T08:06:00.951047Z","steps":["trace[1394854232] 'process raft request'  (duration: 85.118206ms)","trace[1394854232] 'compare'  (duration: 15.899202ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T08:06:01.030682Z","caller":"traceutil/trace.go:171","msg":"trace[1557658807] transaction","detail":"{read_only:false; response_revision:8269; number_of_response:1; }","duration":"169.331612ms","start":"2024-03-29T08:06:00.861321Z","end":"2024-03-29T08:06:01.030653Z","steps":["trace[1557658807] 'process raft request'  (duration: 168.917012ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:06:39.415459Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"162.823708ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2024-03-29T08:06:39.415559Z","caller":"traceutil/trace.go:171","msg":"trace[975225538] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:8298; }","duration":"162.933708ms","start":"2024-03-29T08:06:39.252601Z","end":"2024-03-29T08:06:39.415535Z","steps":["trace[975225538] 'range keys from in-memory index tree'  (duration: 162.327408ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:08:29.064075Z","caller":"traceutil/trace.go:171","msg":"trace[718453960] transaction","detail":"{read_only:false; response_revision:8382; number_of_response:1; }","duration":"113.0542ms","start":"2024-03-29T08:08:28.950984Z","end":"2024-03-29T08:08:29.064039Z","steps":["trace[718453960] 'process raft request'  (duration: 112.814ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:10:00.160617Z","caller":"traceutil/trace.go:171","msg":"trace[1377572366] transaction","detail":"{read_only:false; response_revision:8454; number_of_response:1; }","duration":"119.857901ms","start":"2024-03-29T08:10:00.040657Z","end":"2024-03-29T08:10:00.160515Z","steps":["trace[1377572366] 'process raft request'  (duration: 119.332901ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:10:08.494313Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8227}
{"level":"info","ts":"2024-03-29T08:10:08.498764Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8227,"took":"3.2803ms","hash":3077418664}
{"level":"info","ts":"2024-03-29T08:10:08.499011Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3077418664,"revision":8227,"compact-revision":7990}
{"level":"info","ts":"2024-03-29T08:10:43.396784Z","caller":"traceutil/trace.go:171","msg":"trace[936369280] transaction","detail":"{read_only:false; response_revision:8489; number_of_response:1; }","duration":"102.430239ms","start":"2024-03-29T08:10:43.294317Z","end":"2024-03-29T08:10:43.396748Z","steps":["trace[936369280] 'process raft request'  (duration: 101.953439ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:11:36.527098Z","caller":"traceutil/trace.go:171","msg":"trace[216060284] transaction","detail":"{read_only:false; response_revision:8532; number_of_response:1; }","duration":"153.703814ms","start":"2024-03-29T08:11:36.373305Z","end":"2024-03-29T08:11:36.527009Z","steps":["trace[216060284] 'process raft request'  (duration: 152.967314ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:11:38.70261Z","caller":"traceutil/trace.go:171","msg":"trace[616735398] transaction","detail":"{read_only:false; response_revision:8533; number_of_response:1; }","duration":"128.734697ms","start":"2024-03-29T08:11:38.573826Z","end":"2024-03-29T08:11:38.70256Z","steps":["trace[616735398] 'process raft request'  (duration: 128.096297ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:14:26.675099Z","caller":"traceutil/trace.go:171","msg":"trace[1469541817] transaction","detail":"{read_only:false; response_revision:8687; number_of_response:1; }","duration":"397.953724ms","start":"2024-03-29T08:14:26.277104Z","end":"2024-03-29T08:14:26.675058Z","steps":["trace[1469541817] 'process raft request'  (duration: 397.605424ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:14:26.675405Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:14:26.277004Z","time spent":"398.203424ms","remote":"127.0.0.1:43240","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:8686 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2024-03-29T08:14:26.675515Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"341.23522ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T08:14:26.675349Z","caller":"traceutil/trace.go:171","msg":"trace[444972581] linearizableReadLoop","detail":"{readStateIndex:10794; appliedIndex:10793; }","duration":"341.02182ms","start":"2024-03-29T08:14:26.334296Z","end":"2024-03-29T08:14:26.675318Z","steps":["trace[444972581] 'read index received'  (duration: 339.87532ms)","trace[444972581] 'applied index is now lower than readState.Index'  (duration: 1.1432ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T08:14:26.675603Z","caller":"traceutil/trace.go:171","msg":"trace[1032824959] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8687; }","duration":"341.33162ms","start":"2024-03-29T08:14:26.334249Z","end":"2024-03-29T08:14:26.67558Z","steps":["trace[1032824959] 'agreement among raft nodes before linearized reading'  (duration: 341.18172ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:14:26.684356Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:14:26.334226Z","time spent":"350.05852ms","remote":"127.0.0.1:42886","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-03-29T08:14:29.586494Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"102.178406ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147361378406 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:8681 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:66 lease:8128028147361378404 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:16"}
{"level":"info","ts":"2024-03-29T08:14:29.589366Z","caller":"traceutil/trace.go:171","msg":"trace[2107865444] transaction","detail":"{read_only:false; response_revision:8689; number_of_response:1; }","duration":"111.707107ms","start":"2024-03-29T08:14:29.477628Z","end":"2024-03-29T08:14:29.589335Z","steps":["trace[2107865444] 'compare'  (duration: 101.591806ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:14:30.634645Z","caller":"traceutil/trace.go:171","msg":"trace[1455754896] transaction","detail":"{read_only:false; response_revision:8691; number_of_response:1; }","duration":"167.82301ms","start":"2024-03-29T08:14:30.46679Z","end":"2024-03-29T08:14:30.634613Z","steps":["trace[1455754896] 'process raft request'  (duration: 164.42461ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:14:52.566907Z","caller":"traceutil/trace.go:171","msg":"trace[1742111497] transaction","detail":"{read_only:false; response_revision:8708; number_of_response:1; }","duration":"129.957813ms","start":"2024-03-29T08:14:52.436849Z","end":"2024-03-29T08:14:52.566807Z","steps":["trace[1742111497] 'process raft request'  (duration: 129.420713ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:15:01.594393Z","caller":"traceutil/trace.go:171","msg":"trace[1952099216] transaction","detail":"{read_only:false; response_revision:8715; number_of_response:1; }","duration":"137.045483ms","start":"2024-03-29T08:15:01.457299Z","end":"2024-03-29T08:15:01.594345Z","steps":["trace[1952099216] 'process raft request'  (duration: 136.613583ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:15:08.966838Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":8461}
{"level":"warn","ts":"2024-03-29T08:15:09.062688Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"275.247189ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147361378559 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:8461 > ","response":"size:5"}
{"level":"info","ts":"2024-03-29T08:15:09.063346Z","caller":"traceutil/trace.go:171","msg":"trace[747377680] compact","detail":"{revision:8461; response_revision:8719; }","duration":"426.284827ms","start":"2024-03-29T08:15:08.636992Z","end":"2024-03-29T08:15:09.063277Z","steps":["trace[747377680] 'process raft request'  (duration: 53.593578ms)","trace[747377680] 'check and update compact revision'  (duration: 274.991489ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T08:15:09.06352Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:15:08.636938Z","time spent":"426.569927ms","remote":"127.0.0.1:42902","response type":"/etcdserverpb.KV/Compact","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"info","ts":"2024-03-29T08:15:09.671501Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":8461,"took":"702.338916ms","hash":565684872}
{"level":"info","ts":"2024-03-29T08:15:09.672478Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":565684872,"revision":8461,"compact-revision":8227}
{"level":"warn","ts":"2024-03-29T08:15:09.854641Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"410.656933ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147361378562 username:\"kube-apiserver-etcd-client\" auth_revision:1 > lease_grant:<ttl:15-second id:70cc8e892fd9f101>","response":"size:41"}
{"level":"info","ts":"2024-03-29T08:15:09.854957Z","caller":"traceutil/trace.go:171","msg":"trace[1668484572] linearizableReadLoop","detail":"{readStateIndex:10836; appliedIndex:10835; }","duration":"417.50653ms","start":"2024-03-29T08:15:09.437384Z","end":"2024-03-29T08:15:09.85489Z","steps":["trace[1668484572] 'read index received'  (duration: 6.354897ms)","trace[1668484572] 'applied index is now lower than readState.Index'  (duration: 411.145233ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T08:15:09.855271Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:15:09.433702Z","time spent":"421.559029ms","remote":"127.0.0.1:42912","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-03-29T08:15:09.856234Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"418.80003ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T08:15:09.856785Z","caller":"traceutil/trace.go:171","msg":"trace[1220468370] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8719; }","duration":"419.38033ms","start":"2024-03-29T08:15:09.437339Z","end":"2024-03-29T08:15:09.856719Z","steps":["trace[1220468370] 'agreement among raft nodes before linearized reading'  (duration: 418.69283ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:15:09.856979Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-03-29T08:15:09.437321Z","time spent":"419.591929ms","remote":"127.0.0.1:42886","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-03-29T08:15:09.871444Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"299.259979ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2024-03-29T08:15:09.871729Z","caller":"traceutil/trace.go:171","msg":"trace[80582916] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:8719; }","duration":"299.572679ms","start":"2024-03-29T08:15:09.572094Z","end":"2024-03-29T08:15:09.871666Z","steps":["trace[80582916] 'agreement among raft nodes before linearized reading'  (duration: 285.673084ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:15:10.066752Z","caller":"traceutil/trace.go:171","msg":"trace[1409885511] linearizableReadLoop","detail":"{readStateIndex:10837; appliedIndex:10836; }","duration":"190.380722ms","start":"2024-03-29T08:15:09.876301Z","end":"2024-03-29T08:15:10.066682Z","steps":["trace[1409885511] 'read index received'  (duration: 170.67883ms)","trace[1409885511] 'applied index is now lower than readState.Index'  (duration: 19.696092ms)"],"step_count":2}
{"level":"info","ts":"2024-03-29T08:15:10.066805Z","caller":"traceutil/trace.go:171","msg":"trace[557061753] transaction","detail":"{read_only:false; response_revision:8720; number_of_response:1; }","duration":"191.658922ms","start":"2024-03-29T08:15:09.875085Z","end":"2024-03-29T08:15:10.066744Z","steps":["trace[557061753] 'process raft request'  (duration: 172.07573ms)","trace[557061753] 'compare'  (duration: 19.096993ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T08:15:10.067209Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"190.967022ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T08:15:10.06736Z","caller":"traceutil/trace.go:171","msg":"trace[1059386968] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8720; }","duration":"191.145622ms","start":"2024-03-29T08:15:09.876175Z","end":"2024-03-29T08:15:10.06732Z","steps":["trace[1059386968] 'agreement among raft nodes before linearized reading'  (duration: 190.732222ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:15:10.330286Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.24576ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128028147361378566 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:8718 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:16"}
{"level":"info","ts":"2024-03-29T08:15:10.330421Z","caller":"traceutil/trace.go:171","msg":"trace[567842209] linearizableReadLoop","detail":"{readStateIndex:10838; appliedIndex:10837; }","duration":"181.839627ms","start":"2024-03-29T08:15:10.148558Z","end":"2024-03-29T08:15:10.330398Z","steps":["trace[567842209] 'read index received'  (duration: 81.302967ms)","trace[567842209] 'applied index is now lower than readState.Index'  (duration: 100.53396ms)"],"step_count":2}
{"level":"warn","ts":"2024-03-29T08:15:10.330546Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"192.536521ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:420"}
{"level":"info","ts":"2024-03-29T08:15:10.330613Z","caller":"traceutil/trace.go:171","msg":"trace[1025712696] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:8721; }","duration":"192.614221ms","start":"2024-03-29T08:15:10.137981Z","end":"2024-03-29T08:15:10.330596Z","steps":["trace[1025712696] 'agreement among raft nodes before linearized reading'  (duration: 192.488421ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:15:10.330735Z","caller":"traceutil/trace.go:171","msg":"trace[708902166] transaction","detail":"{read_only:false; response_revision:8721; number_of_response:1; }","duration":"280.339186ms","start":"2024-03-29T08:15:10.05036Z","end":"2024-03-29T08:15:10.330699Z","steps":["trace[708902166] 'process raft request'  (duration: 179.574327ms)","trace[708902166] 'compare'  (duration: 36.120086ms)","trace[708902166] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 63.808774ms)"],"step_count":3}
{"level":"info","ts":"2024-03-29T08:15:52.231161Z","caller":"traceutil/trace.go:171","msg":"trace[1366417285] transaction","detail":"{read_only:false; response_revision:8755; number_of_response:1; }","duration":"153.208815ms","start":"2024-03-29T08:15:52.077873Z","end":"2024-03-29T08:15:52.231082Z","steps":["trace[1366417285] 'process raft request'  (duration: 152.326815ms)"],"step_count":1}
{"level":"warn","ts":"2024-03-29T08:15:54.583299Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"129.749561ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-03-29T08:15:54.583728Z","caller":"traceutil/trace.go:171","msg":"trace[1190639979] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:8757; }","duration":"130.437361ms","start":"2024-03-29T08:15:54.453216Z","end":"2024-03-29T08:15:54.583653Z","steps":["trace[1190639979] 'range keys from in-memory index tree'  (duration: 127.047565ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:15:56.660279Z","caller":"traceutil/trace.go:171","msg":"trace[979523615] transaction","detail":"{read_only:false; response_revision:8758; number_of_response:1; }","duration":"109.367083ms","start":"2024-03-29T08:15:56.550885Z","end":"2024-03-29T08:15:56.660252Z","steps":["trace[979523615] 'process raft request'  (duration: 109.079783ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:16:23.858776Z","caller":"traceutil/trace.go:171","msg":"trace[1809860565] transaction","detail":"{read_only:false; response_revision:8779; number_of_response:1; }","duration":"137.661765ms","start":"2024-03-29T08:16:23.720961Z","end":"2024-03-29T08:16:23.858623Z","steps":["trace[1809860565] 'process raft request'  (duration: 137.337565ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:16:32.347715Z","caller":"traceutil/trace.go:171","msg":"trace[588000556] transaction","detail":"{read_only:false; response_revision:8785; number_of_response:1; }","duration":"113.956382ms","start":"2024-03-29T08:16:32.233428Z","end":"2024-03-29T08:16:32.347385Z","steps":["trace[588000556] 'process raft request'  (duration: 113.763582ms)"],"step_count":1}
{"level":"info","ts":"2024-03-29T08:16:37.202684Z","caller":"traceutil/trace.go:171","msg":"trace[1566313961] transaction","detail":"{read_only:false; response_revision:8789; number_of_response:1; }","duration":"186.600628ms","start":"2024-03-29T08:16:37.016016Z","end":"2024-03-29T08:16:37.202616Z","steps":["trace[1566313961] 'process raft request'  (duration: 160.304538ms)","trace[1566313961] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 26.11269ms)"],"step_count":2}

* 
* ==> kernel <==
*  08:16:40 up 16:41,  0 users,  load average: 3.88, 2.95, 2.66
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [505c322db9dd] <==
* I0329 07:50:23.488961       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0329 07:50:23.490986       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0329 07:50:23.491893       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0329 07:50:23.492989       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0329 07:50:23.493184       1 aggregator.go:164] waiting for initial CRD sync...
I0329 07:50:23.459717       1 naming_controller.go:291] Starting NamingConditionController
I0329 07:50:23.661288       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0329 07:50:23.669974       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0329 07:50:24.631897       1 trace.go:236] Trace[353605113]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:18891bb9-efbd-4448-a569-c14a77be25f4,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (29-Mar-2024 07:50:23.875) (total time: 755ms):
Trace[353605113]: ---"About to write a response" 755ms (07:50:24.631)
Trace[353605113]: [755.845776ms] [755.845776ms] END
I0329 07:50:24.633901       1 trace.go:236] Trace[2025131820]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:1c777f76-27e0-4b2d-a442-d19cfbed01df,client:192.168.49.2,protocol:HTTP/2.0,resource:csinodes,scope:resource,url:/apis/storage.k8s.io/v1/csinodes/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (29-Mar-2024 07:50:23.846) (total time: 787ms):
Trace[2025131820]: ---"About to write a response" 786ms (07:50:24.633)
Trace[2025131820]: [787.019074ms] [787.019074ms] END
I0329 07:50:24.737726       1 trace.go:236] Trace[1465903650]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:860ed6f6-2dfd-4731-bad6-85ee2b48a56d,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (29-Mar-2024 07:50:24.073) (total time: 606ms):
Trace[1465903650]: ---"About to write a response" 605ms (07:50:24.678)
Trace[1465903650]: [606.157981ms] [606.157981ms] END
I0329 07:50:25.371069       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0329 07:50:25.452759       1 aggregator.go:166] initial CRD sync complete...
I0329 07:50:25.452881       1 autoregister_controller.go:141] Starting autoregister controller
I0329 07:50:25.452921       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0329 07:50:25.463845       1 shared_informer.go:318] Caches are synced for configmaps
I0329 07:50:25.473788       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0329 07:50:25.464306       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0329 07:50:25.543533       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0329 07:50:25.551868       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0329 07:50:25.557461       1 shared_informer.go:318] Caches are synced for node_authorizer
I0329 07:50:25.581838       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0329 07:50:25.632902       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0329 07:50:25.653554       1 cache.go:39] Caches are synced for autoregister controller
I0329 07:50:25.655485       1 trace.go:236] Trace[1749027127]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:2ecdc87b-8eb4-4105-a4a1-30928fe599cd,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (29-Mar-2024 07:50:24.977) (total time: 677ms):
Trace[1749027127]: ["GuaranteedUpdate etcd3" audit-id:2ecdc87b-8eb4-4105-a4a1-30928fe599cd,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 676ms (07:50:24.978)
Trace[1749027127]:  ---"About to Encode" 584ms (07:50:25.562)
Trace[1749027127]:  ---"Txn call completed" 91ms (07:50:25.654)]
Trace[1749027127]: [677.441978ms] [677.441978ms] END
I0329 07:50:25.652590       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0329 07:50:25.752966       1 trace.go:236] Trace[971257819]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0cfdc06f-745c-4cd3-9a57-86033640dfe1,client:192.168.49.2,protocol:HTTP/2.0,resource:nodes,scope:resource,url:/api/v1/nodes,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (29-Mar-2024 07:50:23.856) (total time: 1895ms):
Trace[971257819]: [1.89585654s] [1.89585654s] END
I0329 07:50:25.764805       1 trace.go:236] Trace[472699152]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d7663838-1922-4cf2-8424-96cc03a27b3e,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (29-Mar-2024 07:50:24.273) (total time: 1491ms):
Trace[472699152]: [1.491281252s] [1.491281252s] END
I0329 07:50:31.824639       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0329 07:50:31.876497       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0329 07:50:32.020852       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0329 07:50:32.128065       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0329 07:50:32.161115       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0329 07:50:43.034835       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0329 07:50:43.038352       1 controller.go:624] quota admission added evaluator for: endpoints
I0329 08:06:00.842916       1 trace.go:236] Trace[1368232463]: "Get" accept:application/json, */*,audit-id:e0eb6e66-2f5f-45c5-9ed4-fd6880a6a0fe,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (29-Mar-2024 08:05:59.569) (total time: 1272ms):
Trace[1368232463]: ---"About to write a response" 1272ms (08:06:00.842)
Trace[1368232463]: [1.272834364s] [1.272834364s] END
I0329 08:06:00.952217       1 trace.go:236] Trace[463388558]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (29-Mar-2024 08:05:59.240) (total time: 1711ms):
Trace[463388558]: ---"Transaction prepared" 1599ms (08:06:00.842)
Trace[463388558]: ---"Txn call completed" 109ms (08:06:00.951)
Trace[463388558]: [1.711114497s] [1.711114497s] END
I0329 08:13:54.416390       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0329 08:13:54.567212       1 alloc.go:330] "allocated clusterIPs" service="default/fast-api-service" clusterIPs={"IPv4":"10.102.65.241"}
I0329 08:15:10.069718       1 trace.go:236] Trace[1466354118]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (29-Mar-2024 08:15:09.431) (total time: 638ms):
Trace[1466354118]: ---"Transaction prepared" 438ms (08:15:09.871)
Trace[1466354118]: ---"Txn call completed" 197ms (08:15:10.068)
Trace[1466354118]: [638.009841ms] [638.009841ms] END

* 
* ==> kube-apiserver [a00ff38a3b95] <==
* I0329 07:49:33.136564       1 controller.go:178] quota evaluator worker shutdown
I0329 07:49:33.136682       1 controller.go:178] quota evaluator worker shutdown
I0329 07:49:33.136767       1 controller.go:178] quota evaluator worker shutdown
I0329 07:49:33.136848       1 controller.go:178] quota evaluator worker shutdown
E0329 07:49:33.138858       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:monitoring: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:monitoring": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.143371       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:discovery: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:discovery": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.155746       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:basic-user: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:basic-user": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.161086       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:public-info-viewer: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:public-info-viewer": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.167451       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:node-proxier: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:node-proxier": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.170149       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:kube-controller-manager: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:kube-controller-manager": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.172583       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:kube-dns": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.176362       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:kube-scheduler: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:kube-scheduler": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.234179       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:volume-scheduler: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:volume-scheduler": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.238827       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:node: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:node": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.242750       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:service-account-issuer-discovery: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:service-account-issuer-discovery": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.255452       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:attachdetach-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:attachdetach-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.259613       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:clusterrole-aggregation-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:clusterrole-aggregation-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.267589       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:cronjob-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:cronjob-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.270637       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:daemon-set-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:daemon-set-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.273515       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:deployment-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:deployment-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.277865       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:disruption-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:disruption-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.333344       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:endpoint-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:endpoint-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.344421       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:endpointslice-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:endpointslice-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.351957       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:endpointslicemirroring-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:endpointslicemirroring-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.355503       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:expand-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:expand-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.361406       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:ephemeral-volume-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:ephemeral-volume-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.367445       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:generic-garbage-collector: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:generic-garbage-collector": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.374988       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:horizontal-pod-autoscaler: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:horizontal-pod-autoscaler": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.431272       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:job-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:job-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.434350       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:namespace-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:namespace-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.441783       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:node-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:node-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.446287       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:persistent-volume-binder: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:persistent-volume-binder": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.449936       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:pod-garbage-collector: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:pod-garbage-collector": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.453072       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:replicaset-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:replicaset-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.458725       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:replication-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:replication-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.461957       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:resourcequota-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:resourcequota-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.467714       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:route-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:route-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.472121       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:service-account-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:service-account-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.475677       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:service-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:service-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.531479       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:statefulset-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:statefulset-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.534245       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:ttl-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:ttl-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.536759       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:certificate-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:certificate-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.538302       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:pvc-protection-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:pvc-protection-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.539646       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:pv-protection-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:pv-protection-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.542039       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:ttl-after-finished-controller: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:ttl-after-finished-controller": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.543031       1 storage_rbac.go:264] unable to reconcile clusterrolebinding.rbac.authorization.k8s.io/system:controller:root-ca-cert-publisher: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:root-ca-cert-publisher": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.544924       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/extension-apiserver-authentication-reader in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/extension-apiserver-authentication-reader": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.546889       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:bootstrap-signer": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.548009       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/system:controller:cloud-provider in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:cloud-provider": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.549721       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/system:controller:token-cleaner in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:controller:token-cleaner": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.551396       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/system::leader-locking-kube-controller-manager in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system::leader-locking-kube-controller-manager": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.552515       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/system::leader-locking-kube-scheduler in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system::leader-locking-kube-scheduler": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.553663       1 storage_rbac.go:295] unable to reconcile role.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-public: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/roles/system:controller:bootstrap-signer": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.555168       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-public: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-public/rolebindings/system:controller:bootstrap-signer": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.556593       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system::extension-apiserver-authentication-reader in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::extension-apiserver-authentication-reader": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.558247       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system::leader-locking-kube-controller-manager in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::leader-locking-kube-controller-manager": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.559660       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system::leader-locking-kube-scheduler in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system::leader-locking-kube-scheduler": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.561196       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:bootstrap-signer": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.562826       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:cloud-provider in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:cloud-provider": dial tcp 127.0.0.1:8443: connect: connection refused
E0329 07:49:33.564489       1 storage_rbac.go:329] unable to reconcile rolebinding.rbac.authorization.k8s.io/system:controller:token-cleaner in kube-system: Get "https://localhost:8443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:controller:token-cleaner": dial tcp 127.0.0.1:8443: connect: connection refused

* 
* ==> kube-controller-manager [42b7664aa6ab] <==
* I0329 07:50:41.864756       1 shared_informer.go:318] Caches are synced for crt configmap
I0329 07:50:41.865256       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-legacy-unknown
I0329 07:50:41.868730       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I0329 07:50:41.931226       1 shared_informer.go:318] Caches are synced for PVC protection
I0329 07:50:41.932793       1 shared_informer.go:318] Caches are synced for ReplicationController
I0329 07:50:41.932928       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0329 07:50:41.936434       1 shared_informer.go:318] Caches are synced for expand
I0329 07:50:41.937764       1 shared_informer.go:318] Caches are synced for job
I0329 07:50:41.975941       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-client
I0329 07:50:42.031000       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0329 07:50:42.045265       1 shared_informer.go:318] Caches are synced for certificate-csrsigning-kubelet-serving
I0329 07:50:42.045579       1 shared_informer.go:318] Caches are synced for PV protection
I0329 07:50:42.046167       1 shared_informer.go:318] Caches are synced for deployment
I0329 07:50:42.060386       1 shared_informer.go:318] Caches are synced for certificate-csrapproving
I0329 07:50:42.065396       1 shared_informer.go:318] Caches are synced for cronjob
I0329 07:50:42.143119       1 shared_informer.go:318] Caches are synced for TTL after finished
I0329 07:50:42.143399       1 shared_informer.go:318] Caches are synced for disruption
I0329 07:50:42.143783       1 shared_informer.go:318] Caches are synced for ephemeral
I0329 07:50:42.255882       1 shared_informer.go:318] Caches are synced for GC
I0329 07:50:42.272533       1 shared_informer.go:318] Caches are synced for taint
I0329 07:50:42.275600       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I0329 07:50:42.277676       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0329 07:50:42.278020       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0329 07:50:42.278484       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I0329 07:50:42.279680       1 taint_manager.go:211] "Sending events to api server"
I0329 07:50:42.282542       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0329 07:50:42.343154       1 shared_informer.go:318] Caches are synced for node
I0329 07:50:42.343459       1 range_allocator.go:174] "Sending events to api server"
I0329 07:50:42.343619       1 range_allocator.go:178] "Starting range CIDR allocator"
I0329 07:50:42.343658       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I0329 07:50:42.343718       1 shared_informer.go:318] Caches are synced for cidrallocator
I0329 07:50:42.344009       1 shared_informer.go:318] Caches are synced for persistent volume
I0329 07:50:42.347846       1 shared_informer.go:318] Caches are synced for attach detach
I0329 07:50:42.352800       1 shared_informer.go:318] Caches are synced for HPA
I0329 07:50:42.353486       1 actual_state_of_world.go:547] "Failed to update statusUpdateNeeded field in actual state of world" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0329 07:50:42.361521       1 shared_informer.go:318] Caches are synced for TTL
I0329 07:50:42.362542       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0329 07:50:42.430471       1 shared_informer.go:318] Caches are synced for endpoint
I0329 07:50:42.434928       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I0329 07:50:42.457097       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I0329 07:50:42.543210       1 shared_informer.go:318] Caches are synced for stateful set
I0329 07:50:42.543947       1 shared_informer.go:318] Caches are synced for daemon sets
I0329 07:50:42.560464       1 shared_informer.go:318] Caches are synced for resource quota
I0329 07:50:42.562084       1 shared_informer.go:318] Caches are synced for resource quota
I0329 07:50:42.766300       1 shared_informer.go:318] Caches are synced for garbage collector
I0329 07:50:42.766424       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0329 07:50:42.768983       1 shared_informer.go:318] Caches are synced for garbage collector
I0329 07:50:43.065460       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="1.132095106s"
I0329 07:50:43.066865       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="319.6¬µs"
I0329 07:50:45.845251       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="5.7332ms"
I0329 07:50:57.408622       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="132.1¬µs"
I0329 07:50:57.468838       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="30.265101ms"
I0329 07:50:57.469923       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="188.6¬µs"
I0329 08:13:54.430813       1 event.go:307] "Event occurred" object="default/fast-api-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set fast-api-deployment-84c77b9745 to 2"
I0329 08:13:54.471943       1 event.go:307] "Event occurred" object="default/fast-api-deployment-84c77b9745" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fast-api-deployment-84c77b9745-wt9xs"
I0329 08:13:54.504380       1 event.go:307] "Event occurred" object="default/fast-api-deployment-84c77b9745" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: fast-api-deployment-84c77b9745-plcdr"
I0329 08:13:54.561726       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-84c77b9745" duration="132.503406ms"
I0329 08:13:54.741806       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-84c77b9745" duration="179.507008ms"
I0329 08:13:54.742728       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-84c77b9745" duration="757.7¬µs"
I0329 08:13:54.832169       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/fast-api-deployment-84c77b9745" duration="278.3¬µs"

* 
* ==> kube-controller-manager [bb8576ed3107] <==
* I0329 07:49:10.744754       1 serving.go:348] Generated self-signed cert in-memory
I0329 07:49:16.547460       1 controllermanager.go:189] "Starting" version="v1.28.3"
I0329 07:49:16.547815       1 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 07:49:16.582278       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0329 07:49:16.584321       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0329 07:49:16.593888       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I0329 07:49:16.594909       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"

* 
* ==> kube-proxy [09601e9f5785] <==
* I0329 07:50:50.874021       1 server_others.go:69] "Using iptables proxy"
I0329 07:50:51.054477       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0329 07:50:51.501196       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0329 07:50:51.564635       1 server_others.go:152] "Using iptables Proxier"
I0329 07:50:51.564975       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0329 07:50:51.565081       1 server_others.go:438] "Defaulting to no-op detect-local"
I0329 07:50:51.565499       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0329 07:50:51.568077       1 server.go:846] "Version info" version="v1.28.3"
I0329 07:50:51.568223       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 07:50:51.581071       1 config.go:188] "Starting service config controller"
I0329 07:50:51.582639       1 shared_informer.go:311] Waiting for caches to sync for service config
I0329 07:50:51.583071       1 config.go:97] "Starting endpoint slice config controller"
I0329 07:50:51.583130       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0329 07:50:51.631030       1 config.go:315] "Starting node config controller"
I0329 07:50:51.633328       1 shared_informer.go:311] Waiting for caches to sync for node config
I0329 07:50:51.732398       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0329 07:50:51.733428       1 shared_informer.go:318] Caches are synced for service config
I0329 07:50:51.735127       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-proxy [dfdf3cdd4916] <==
* I0329 07:49:06.331344       1 server_others.go:69] "Using iptables proxy"
E0329 07:49:17.369380       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": net/http: TLS handshake timeout
I0329 07:49:25.873859       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0329 07:49:30.034731       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0329 07:49:30.459973       1 server_others.go:152] "Using iptables Proxier"
I0329 07:49:30.460489       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0329 07:49:30.460581       1 server_others.go:438] "Defaulting to no-op detect-local"
I0329 07:49:30.467350       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0329 07:49:30.648856       1 server.go:846] "Version info" version="v1.28.3"
I0329 07:49:30.649156       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 07:49:31.235183       1 config.go:188] "Starting service config controller"
I0329 07:49:31.239157       1 shared_informer.go:311] Waiting for caches to sync for service config
I0329 07:49:31.241782       1 config.go:97] "Starting endpoint slice config controller"
I0329 07:49:31.241921       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0329 07:49:31.242162       1 config.go:315] "Starting node config controller"
I0329 07:49:31.244039       1 shared_informer.go:311] Waiting for caches to sync for node config
I0329 07:49:31.543606       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0329 07:49:31.560552       1 shared_informer.go:318] Caches are synced for service config
I0329 07:49:31.648184       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [89e8595f5173] <==
* I0329 07:50:07.294340       1 serving.go:348] Generated self-signed cert in-memory
W0329 07:50:25.170944       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0329 07:50:25.171308       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0329 07:50:25.171547       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0329 07:50:25.171782       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0329 07:50:25.734700       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0329 07:50:25.736053       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 07:50:25.835930       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0329 07:50:25.836261       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0329 07:50:25.844396       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0329 07:50:25.844826       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0329 07:50:26.160710       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [9e21ccd20fbe] <==
* I0329 07:48:51.555419       1 serving.go:348] Generated self-signed cert in-memory
W0329 07:49:13.764646       1 authentication.go:368] Error looking up in-cluster authentication configuration: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication": net/http: TLS handshake timeout
W0329 07:49:13.765634       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0329 07:49:13.765946       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0329 07:49:24.168510       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0329 07:49:24.168716       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0329 07:49:24.263348       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0329 07:49:24.269095       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0329 07:49:24.272658       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0329 07:49:24.272831       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0329 07:49:26.836418       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0329 07:49:33.571306       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Mar 29 07:50:37 minikube kubelet[92492]: E0329 07:50:37.973155   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.074175   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.175626   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.276339   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.377713   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.479608   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.580789   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.687205   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.787754   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.888676   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:38 minikube kubelet[92492]: E0329 07:50:38.990170   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.091099   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.191941   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.293007   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.394718   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.495405   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.597202   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.698446   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.799030   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:39 minikube kubelet[92492]: E0329 07:50:39.900742   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.001704   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.103907   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.205490   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.306280   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.407242   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.509262   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.610933   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.711689   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.813155   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:40 minikube kubelet[92492]: E0329 07:50:40.913545   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: E0329 07:50:41.014367   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: E0329 07:50:41.116030   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: E0329 07:50:41.216664   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: E0329 07:50:41.330942   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: E0329 07:50:41.431576   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: E0329 07:50:41.532186   92492 kubelet_node_status.go:458] "Error getting the current node from lister" err="node \"minikube\" not found"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.765114   92492 apiserver.go:52] "Watching apiserver"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.786331   92492 topology_manager.go:215] "Topology Admit Handler" podUID="ff9bf080-33af-4fc2-a1b6-a8d898db14ef" podNamespace="kube-system" podName="kube-proxy-2cp4n"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.787928   92492 topology_manager.go:215] "Topology Admit Handler" podUID="cefa8631-fb98-4413-93a4-bfc2b264055e" podNamespace="kube-system" podName="coredns-5dd5756b68-r7x2l"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.788161   92492 topology_manager.go:215] "Topology Admit Handler" podUID="24bf02fc-5c5a-41bf-aa78-0309d0b93889" podNamespace="kube-system" podName="storage-provisioner"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.862261   92492 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.966771   92492 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/ff9bf080-33af-4fc2-a1b6-a8d898db14ef-lib-modules\") pod \"kube-proxy-2cp4n\" (UID: \"ff9bf080-33af-4fc2-a1b6-a8d898db14ef\") " pod="kube-system/kube-proxy-2cp4n"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.967371   92492 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/ff9bf080-33af-4fc2-a1b6-a8d898db14ef-xtables-lock\") pod \"kube-proxy-2cp4n\" (UID: \"ff9bf080-33af-4fc2-a1b6-a8d898db14ef\") " pod="kube-system/kube-proxy-2cp4n"
Mar 29 07:50:41 minikube kubelet[92492]: I0329 07:50:41.968922   92492 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/24bf02fc-5c5a-41bf-aa78-0309d0b93889-tmp\") pod \"storage-provisioner\" (UID: \"24bf02fc-5c5a-41bf-aa78-0309d0b93889\") " pod="kube-system/storage-provisioner"
Mar 29 07:50:42 minikube kubelet[92492]: I0329 07:50:42.471989   92492 scope.go:117] "RemoveContainer" containerID="fdbd66941e77eb73b8bc7ecc151bc77ef15316e800d84c203e5fbfe5c7354b32"
Mar 29 07:50:42 minikube kubelet[92492]: I0329 07:50:42.751343   92492 scope.go:117] "RemoveContainer" containerID="dfdf3cdd4916cd56f4f5109edd067c40695a939338029db606e4ce33a9af0fe1"
Mar 29 07:50:46 minikube kubelet[92492]: E0329 07:50:46.750283   92492 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.007s"
Mar 29 07:50:52 minikube kubelet[92492]: I0329 07:50:52.456554   92492 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2a2f696d0fe1aeefa2c3f0482d28c61eaf19d89d68aa6ea6cf002f162f065e46"
Mar 29 07:54:52 minikube kubelet[92492]: W0329 07:54:52.109435   92492 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 29 07:59:52 minikube kubelet[92492]: W0329 07:59:52.117994   92492 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 29 08:04:52 minikube kubelet[92492]: W0329 08:04:52.113563   92492 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 29 08:06:56 minikube kubelet[92492]: E0329 08:06:56.834199   92492 kubelet.go:2477] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.091s"
Mar 29 08:09:52 minikube kubelet[92492]: W0329 08:09:52.102837   92492 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Mar 29 08:13:54 minikube kubelet[92492]: I0329 08:13:54.518767   92492 topology_manager.go:215] "Topology Admit Handler" podUID="3984e4c5-3711-4bc5-8b06-13a41e2df77b" podNamespace="default" podName="fast-api-deployment-84c77b9745-wt9xs"
Mar 29 08:13:54 minikube kubelet[92492]: I0329 08:13:54.543046   92492 topology_manager.go:215] "Topology Admit Handler" podUID="67f49a96-f1cf-415c-bb3c-2b8b779c1381" podNamespace="default" podName="fast-api-deployment-84c77b9745-plcdr"
Mar 29 08:13:54 minikube kubelet[92492]: I0329 08:13:54.567016   92492 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zmjq4\" (UniqueName: \"kubernetes.io/projected/3984e4c5-3711-4bc5-8b06-13a41e2df77b-kube-api-access-zmjq4\") pod \"fast-api-deployment-84c77b9745-wt9xs\" (UID: \"3984e4c5-3711-4bc5-8b06-13a41e2df77b\") " pod="default/fast-api-deployment-84c77b9745-wt9xs"
Mar 29 08:13:54 minikube kubelet[92492]: I0329 08:13:54.567423   92492 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2kkh7\" (UniqueName: \"kubernetes.io/projected/67f49a96-f1cf-415c-bb3c-2b8b779c1381-kube-api-access-2kkh7\") pod \"fast-api-deployment-84c77b9745-plcdr\" (UID: \"67f49a96-f1cf-415c-bb3c-2b8b779c1381\") " pod="default/fast-api-deployment-84c77b9745-plcdr"
Mar 29 08:13:56 minikube kubelet[92492]: I0329 08:13:56.638431   92492 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="19f484677efb7e888f6989fbbd7ad01e633b96a3f57a92c008e6d9ef914d09b1"
Mar 29 08:13:56 minikube kubelet[92492]: I0329 08:13:56.663772   92492 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3ab087ec15d3ae3235ffe49ed6ca6c14476677802b9ea1194594905cc41aa68c"
Mar 29 08:14:52 minikube kubelet[92492]: W0329 08:14:52.231366   92492 sysinfo.go:203] Nodes topology is not available, providing CPU topology

* 
* ==> storage-provisioner [269cf61452bc] <==
* I0329 07:50:50.071232       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0329 07:50:50.548301       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0329 07:50:50.549977       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0329 07:51:08.245415       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0329 07:51:08.246770       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"e3f83578-b340-4432-b14f-74833c2215da", APIVersion:"v1", ResourceVersion:"7570", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_26cb3ed8-05c8-4597-b940-17de0c50c7fc became leader
I0329 07:51:08.254339       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_26cb3ed8-05c8-4597-b940-17de0c50c7fc!
I0329 07:51:08.472769       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_26cb3ed8-05c8-4597-b940-17de0c50c7fc!

* 
* ==> storage-provisioner [fdbd66941e77] <==
* 
